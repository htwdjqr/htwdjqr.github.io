<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2018-08-24腾讯云自媒体分享计划]]></title>
    <url>%2F2018%2F08%2F24%2F2018-08-24%E8%85%BE%E8%AE%AF%E4%BA%91%E8%87%AA%E5%AA%92%E4%BD%93%E5%88%86%E4%BA%AB%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[很荣幸收到腾讯云自媒体分享计划的邀请，我的博客将会同步搬运至腾讯云+社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=1xlh1vtnv63p1]]></content>
      <categories>
        <category>工作记录</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次系统密码安全事故以及修改方案]]></title>
    <url>%2F2017%2F11%2F30%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%B3%BB%E7%BB%9F%E5%AF%86%E7%A0%81%E5%AE%89%E5%85%A8%E4%BA%8B%E6%95%85%E4%BB%A5%E5%8F%8A%E4%BF%AE%E6%94%B9%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[1、问题运营人员反馈在晚上十一点多收到系统后台登录的短信验证码，第二天在后台的操作日志中发现自已的账号有被登录过后台系统，但实际上自已并没有登录操作，怀疑账号被他人恶意登录。 2、排查过程系统后台登录需要用户名、密码、手机验证码，三者缺一不可，运维查看Nginx的访问日志，发现登录的接口被大量访问调用。联系之前系统被攻击，导致数据库泄露，而系统用户的密码是用MD5加密，对于简单常用的密码实际上是可以被破解的，果然拿到被恶意登录用户的加密密码，在MD5破解网上证实确实是可以被破解的。 所以整个流程可以猜测为攻击者拿到数据库后，破解了一部分密码较为简单的用户密码，再无限制的调用登录接口，用不同的验证码去尝试登录，由于验证码的长度为4位，所以攻击者最多只需要尝试10000次即可完成暴力破解。 3、解决方案主要是5个方面的措施： 修改验证码长度 增加验证码输入错误次数限制 密码加密加随机盐值处理 RSA加密，前端密码公钥加密，后端私钥解密 采用新规则全库修改用户密码 3.1、修改验证码长度原先状况：验证码的长度为4位，攻击者暴力破解，最多只需要试10的四次方，即10000次即可完成破解。 解决方案：修改验证码的长度为6位，增加暴力破解难度，注意到我们平时收到各个网站的验证码几乎都是6位数。 3.2、增加输入错误次数限制原先状况：验证码输入错误次数无限制，导致攻击者可以无限调用接口尝试登陆，最终被暴力破击。 解决方案：限制输入错误验证码次数。此功能类似于其他网站输入N次错误密码之后就会冻结账户的功能，由于系统后台获取验证码的功能是基于正确输入用户名和密码的前提下，所以我们只需要限制错误输入验证码的次数即可。 此功能利用Redis可以很容易实现，利用redis的String数据结构和超时自动过期机制，每错误一次，则错误值+1，并设置相应的过期时间，在登录的时候判断从key中获取到失败次数是否大于最大失败次数即可。 12345678910111213141516/** * 登录次数错误+1 * * @param userName */private void increaseFailedLoginCounter(String userName) &#123; String key = ERROR_COUNT_KEY + userName; JedisCluster cluster = jedisClusterManager.getJedisCluster(); String v = cluster.get(key); if (org.springframework.util.StringUtils.isEmpty(v)) &#123; cluster.set(key, &quot;1&quot;); &#125; else &#123; cluster.incr(key); &#125; cluster.expire(key, 1800);&#125; 3.3、密码加密加盐值处理原先状态：系统原先使用简单的MD5加密，导致数据库泄露之后，部分简单常见的密码被破解，虽然MD5加密是不可逆的，但是因为有彩虹表的存在，一些简单常用的简单密码是可以暴力破解的。 解决方案：密码加密加盐值处理。数据库用户表增加salt字段存储加密盐值，在添加用户的时候，生成一个随机盐值存入数据库，用户密码加密的时候用密码+盐值进行MD5加密。同样，在登录的时候也使用密码+盐值进行MD5加密之后再和数据库的密码进行对比。12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.kimeng.weipan.utils;import org.apache.commons.codec.digest.DigestUtils;import java.security.SecureRandom;/** * @author: 会跳舞的机器人 * @date: 2017/9/18 15:08 * @description: MD5工具类 */public class MD5Utils &#123; private static final String B64T = &quot;./0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz&quot;; /** * MD5加密 * * @param plaintext 密码 * @param salt 盐值 * @return 密文 */ public static String md5Hex(String plaintext, String salt) &#123; return DigestUtils.md5Hex(plaintext + salt); &#125; /** * 获取64位的随机盐值 */ public static String getRandomSalt() &#123; return getRandomSalt(64); &#125; /** * 获取指定位数的随机盐值 * * @param num 位数 * @return 随机盐值 */ public static String getRandomSalt(final int num) &#123; final StringBuilder saltString = new StringBuilder(); for (int i = 1; i &lt;= num; i++) &#123; saltString.append(B64T.charAt(new SecureRandom().nextInt(B64T.length()))); &#125; return saltString.toString(); &#125;&#125; 3.4、RSA加密，前端密码公钥加密，后端私钥解密原先状况：登录密码明文传输，没有https，可能导致密码在传输的过程中被监听劫持。 解决方案：利用RSA加密，服务端生成一对密钥缓存至Redis，在用户登录的时候先调用服务端的获取公钥接口获取到公钥，然后用公钥加密密码之后，再传到服务端，服务端从Redis中获取到私钥之后进行密码解密。就算数据被监听劫持，没有私钥攻击者也无法解密，保证密码在传输过程中的安全。 RSA非对称加密的相关内容可以点这里RSA非对称加密算法 前端登录function12345678910111213141516171819202122function login() &#123; var publicKey = &quot;&quot;; var userName = $(&quot;#loginname&quot;).val(); // 获取公钥 $.ajax(&#123; type: &quot;GET&quot;, url: &apos;$&#123;pageContext.request.contextPath&#125;/xxxx/getPublicKey?userName=&apos; + userName, cache: false, async: false, dataType: &quot;text&quot;, success: function (data) &#123; publicKey = data &#125;, &#125;); // RSA加密密码 var encrypt = new JSEncrypt(); encrypt.setPublicKey(publicKey); var encryptPwd = encrypt.encrypt($(&quot;#password&quot;).val()); $(&quot;#password&quot;).val(encryptPwd); $(&quot;#loginForm&quot;).submit();&#125; 注意：前端RSA加密需要引入jsencrypt.js库 获取公钥接口 123456789101112131415161718192021222324 /** * 获取RSA公钥 */@RequestMapping(&quot;/getPublicKey&quot;)@ResponseBodypublic String getPublicKey(HttpServletRequest request) &#123; String userName = ServletRequestUtils.getStringParameter(request, &quot;userName&quot;, &quot;&quot;); if (StringUtil.isEmpty(userName)) &#123; return &quot;&quot;; &#125; // RSA生成公钥私钥 Map&lt;String, Object&gt; map = RSAUtil.init(); String publicKey = RSAUtil.getPublicKey(map); String privateKey = RSAUtil.getPrivateKey(map); // 公钥私钥缓存至redis，过期时间为一分钟，如果存在则覆盖 String key = RedisConstants.PREFIX_RSA_LOGIN + userName; JedisCluster jedisCluster = jedisClusterManager.getJedisCluster(); jedisCluster.hset(key, RedisConstants.KEY_PUBLIC_KEY, publicKey); jedisCluster.hset(key, RedisConstants.KEY_PRIVATE_KEY, privateKey); jedisCluster.expire(key, 60); return publicKey;&#125; RSA密码解密12345678910111213141516 /** * RSA密码解密 */private String decodeRSAPwd(String key, String password) throws Exception &#123; String privateKey = jedisClusterManager.getJedisCluster().hget(key, RedisConstants.KEY_PRIVATE_KEY); if (StringUtil.isEmpty(privateKey)) &#123; logger.error(&quot;private key is null for key&#123;&quot; + key + &quot;&#125;&quot;); throw new Exception(&quot;private key is null&quot;); &#125; String pwd = RSAUtil.decryptByPrivateKey(password.getBytes(), privateKey); if (pwd == null) &#123; throw new Exception(&quot;decode password fail&quot;); &#125; logger.info(&quot;decode password success&quot;); return pwd;&#125;]]></content>
      <categories>
        <category>系统设计</category>
        <category>工作记录</category>
        <category>系统安全</category>
      </categories>
      <tags>
        <tag>系统设计</tag>
        <tag>系统安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实时交易数据监控系统的设计与实现]]></title>
    <url>%2F2017%2F11%2F30%2F%E5%AE%9E%E6%97%B6%E4%BA%A4%E6%98%93%E6%95%B0%E6%8D%AE%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[文章分为四个部分 1、主要功能 2、运用的技术 3、系统设计 4、优化与总结 1、主要功能对平台支付网关的交易订单进行实时的统计，包括实时的交易金额与交易订单量、不同支付方式的交易总额、订单量以及占比、当天各个时间段的数据统计折线图，实现效果图如下： 2、运用的技术 Redis：利用Redis的消息发布与订阅功能、以及List、SortedSet、Hash的数据结构特性 WebSocket：负责将实时汇总的交易数据推送至浏览器客户端 3、系统设计实时交易数据监控系统所涉及的工程包括交易服务、监控统计服务、监控应用（Dubbo服务化）。 交易服务在交易成功后向Redis中发布消息并将数据发送至Redis的list队列 监控服务负责Redis消息的订阅并进行统计，统计完成后将实时的统计结果再次发送至Redis 监控应用作为WebSocket的服务端，也负责监听监控服务推送过来的实时统计数据并通过WebSocket将数据推送至客户端。 Redis数据结构图如下： 利用list的lpush、lpop功能进行对数据的存取操作，SortedSet最开始主要是用于排序，将交易时间作为score进行排序，但是因为涉及到一些数据的计算，在高并发以及分布式部署的情况下，利用SortedSet进行数据统计是会存在问题的，文末会提到，hash结构主要是用于对数据进行原子性的计算。 UML时序图如下： 3.1、交易系统——支付服务支付服务在交易成功后，会给Redis发布一条订单记录消息，并向Redis的list列表lpush一条同样的订单记录信息，为了不影响正常的支付业务流程，所以采用的是异步的方式，伪代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Redis消息通道 */ @Value(&quot;#&#123;settings[&apos;redis.trade.channel&apos;]&#125;&quot;) private String redisChannel; /** * 微信支付的订单队列key */ @Value(&quot;#&#123;settings[&apos;redis.trade.wxDetails&apos;]&#125;&quot;) private String redisWxQueue; /** * 支付宝支付的订单队列key */ @Value(&quot;#&#123;settings[&apos;redis.trade.alipayDetails&apos;]&#125;&quot;) private String redisAlipayQueue; /** * 单个线程的线程池 */ protected static ExecutorService executorService = Executors.newSingleThreadExecutor(); /** * 交易成功后需要执行的业务逻辑 * @param paymentRecord */ public void successPayment(final PaymentRecord paymentRecord) &#123; // do otherthing... // 异步发送消息 executorService.submit(new Runnable() &#123; @Override public void run() &#123; try &#123; pushPaymentRecordMonitorVo(paymentRecord); &#125; catch (Exception e) &#123; log.error(&quot;payment send to redis fail,PaymengRecord:&quot; + JsonUtil.toJsonString(paymentRecord)); &#125; &#125; &#125;); &#125; /** * 将交易成功的订单信息插入至Redis队列并发送一条通知通知 * @param paymentRecord * @throws Exception */ private void pushPaymentRecordMonitorVo (PaymentRecord paymentRecord) throws Exception&#123; PaymentRecordMonitorVo paymentRecordMonitorVo = new PaymentRecordMonitorVo(); paymentRecordMonitorVo.setMerchantOrderNo(paymentRecord.getMerchantOrderNo()); paymentRecordMonitorVo.setPayWay(paymentRecord.getPayWayCode() == null ? null : paymentRecord.getPayWayCode().name()); paymentRecordMonitorVo.setTradeTime(paymentRecord.getPaySuccessTime()); paymentRecordMonitorVo.setAmount(paymentRecord.getOrderAmount()); log.info(&quot;订单消息插入Redis队列...&quot;); if (paymentRecord.getPayWayCode() != null &amp;&amp; paymentRecord.getPayWayCode().equals(PayWayEnum.WEIXIN)) &#123; JedisHelper.dataCluster().lpush(redisWxQueue,JsonUtil.toJsonString(paymentRecordMonitorVo)); &#125; else if (paymentRecord.getPayWayCode() != null &amp;&amp; paymentRecord.getPayWayCode().equals(PayWayEnum.ALIPAY)) &#123; JedisHelper.dataCluster().lpush(redisAlipayQueue,JsonUtil.toJsonString(paymentRecordMonitorVo)); &#125; log.info(&quot;订单消息插入Redis队列结束...&quot;); // 发布消息 log.info(&quot;订单消息发布到Redis...&quot;); JedisHelper.dataCluster().publish(redisChannel,JsonUtil.toJsonString(paymentRecordMonitorVo)); log.info(&quot;订单消息发布到Redis结束...&quot;); &#125; 3.2、监控服务3.2.1.主要功能包括： 订阅Redis中交易服务发布过来的订单消息以及获取list列表中的订单数据 根据订单的交易时间，按照每15分钟为一个数据汇总点进行汇总 对每15分钟汇总的SortedSet进行统计后，将结果再发布至Redis的消息中 3.2.2.遇到的坑：在监控服务启动的时候会进行Redis的list列表中数据的统计初始化，并开启Redis消息订阅者的监听。但有三个比较坑的地方就是： （1）因为用的是Redis6个节点组成的一个集群，所以是用JedisCluster，但是JedisCluster在2.8.x版本以上才支持消息的发布与订阅，项目原先用的是2.7.3版本解决方案：把项目Jedis版本改为2.8.1，pom.xml内容如下：12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.8.1&lt;/version&gt; &lt;/dependency&gt; （2）Redis的消息发布与订阅，在订阅方必须要手动的调用subscribe()方法，并将监听者和需要监听的通道作为参数传入才能开启监听，而像ActiveMQ这种消息中间件是不需要显示的调用，只需配置好消息监听者就会自动监听的。 还有一个坑就是subscribe()方法是一个线程阻塞方法，本想在项目启动的时候就调用subscribe()开启消息的订阅，结果发现方法调用后，其他的代码根本没法往下执行。解决方案是：在项目启动的时候调用subscribe()方法开启消息监听，并且新开一个线程去调用subscribe()方法来避免阻塞主线程。 （3）Redis不支持消息的持久化。在订阅者没有启动的时候，消息发布者将消息发出去了，订阅者没有收到，那订阅者重新启动的时候也不会收到之前发的消息了，而像ActiveMQ是支持消息的持久化的。 解决方案：在往Redis发布消息的时候也同样往Redis的list列表中lpush一条同样消息的数据（参照上面交易服务中的代码），消息订阅者接收到消息并进行相应的业务处理后，再将list列表中的数据删除，那在监控服务挂掉的情况下，Redis消息无法正常被监听消费，但是Redis的list列表中还是会存有消息的数据，所以后续我们可以从list列表中取出消息数据再进行相应的业务处理，这样就间接的实现了Redis消息的持久化。 3.2.3.部分代码（1）RedisSubscribeHelper.java：监控服务启动时，进行Redis队列中数据的统计初始化，并开启Redis消息订阅者的监听的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137package com.ylp.core.monitor.redis;import com.ylp.common.tools.utils.JsonUtil;import com.ylp.core.monitor.biz.MonitorBiz;import com.ylp.facade.monitor.utils.JedisHelper;import com.ylp.facade.monitor.utils.MonitorUtils;import com.ylp.facade.monitor.vo.PaymentRecordMonitorVo;import org.apache.commons.lang3.StringUtils;import org.apache.log4j.Logger;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisCluster;import javax.annotation.PostConstruct;import java.io.IOException;import java.util.Date;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * @author: 会跳舞的机器人 * @date: 16/8/25 下午2:24 * @description:作用1:项目启动时,对Redis消息未进行处理的订单数据进行初始化处理; 作用2:Redis消息订阅的启动器,项目启动时新启动一个线程进行消息的订阅 */@Componentpublic class RedisSubscribeHelper &#123; private Logger logger = Logger.getLogger(RedisSubscribeHelper.class); @Autowired private RedisSubscribeListener redisSubscribeListener; @Autowired private MonitorBiz monitorBiz; /** * Redis消息订阅的channel */ @Value(&quot;#&#123;settings[&apos;redis.trade.channel&apos;]&#125;&quot;) private String redisChannel; /** * 微信支付的订单数据key */ @Value(&quot;#&#123;settings[&apos;redis.trade.wxDetails&apos;]&#125;&quot;) private String redisWxQueue; /** * 支付宝支付的订单数据key */ @Value(&quot;#&#123;settings[&apos;redis.trade.alipayDetails&apos;]&#125;&quot;) private String redisAlipayQueue; /** * redis消息订阅的subscribe方法为阻塞方法,所以需要单独启动一个线程进行消息订阅 */ private ExecutorService executorService = Executors.newSingleThreadExecutor(); /** * 构造函数执行后,执行数据初始化与消息订阅 */ @PostConstruct public void doWork() &#123; init(); subscribe(); &#125; /** * 数据的初始化,不能一次性取出所有的数据,因为有可能系统在启动的时候Redis的list会有元素push进来 */ private void init() &#123; long start = System.currentTimeMillis(); // 避免初始化加载时,JedisCluste还没完成初始化,所以需要sleep JedisCluster jedisCluster = JedisHelper.dataCluster(); if (jedisCluster == null) &#123; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; logger.info(&quot;开始从Redis中获取微信订单初始化数据....&quot;); dealData(jedisCluster, redisWxQueue); logger.info(&quot;微信订单记录数据全部处理完毕...&quot;); logger.info(&quot;开始从Redis中获取支付宝订单初始化数据....&quot;); dealData(jedisCluster, redisAlipayQueue); logger.info(&quot;支付宝订单记录数据全部处理完毕...&quot;); logger.info(&quot;初始化数据工作耗时:&quot; + (System.currentTimeMillis() - start) + &quot;ms&quot;); &#125; /** * 根据队列来处理队列中的数据 * * @param queueName */ private void dealData(JedisCluster jedisCluster, String queueName) &#123; long startTime = MonitorUtils.getCurrentDayZeroTime().getTime(); long endTime = MonitorUtils.getDateEndTime().getTime(); Date tradeTime = null; String str = &quot;&quot;; PaymentRecordMonitorVo paymentRecordMonitorVo = null; while (true) &#123; try &#123; str = jedisCluster.lpop(queueName); if (StringUtils.isEmpty(str)) &#123; break; &#125; paymentRecordMonitorVo = JsonUtil.jsonToObject(str, PaymentRecordMonitorVo.class); tradeTime = paymentRecordMonitorVo.getTradeTime(); // 只处理今天的数据 if (tradeTime != null &amp;&amp; tradeTime.getTime() &gt;= startTime &amp;&amp; tradeTime.getTime() &lt;= endTime) &#123; monitorBiz.periodStatistics(paymentRecordMonitorVo); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); break; &#125; &#125; &#125; /** * Redis消息订阅 */ private void subscribe() &#123; logger.info(&quot;启动Redis消息订阅,channel:&quot; + redisChannel); executorService.submit(new Runnable() &#123; @Override public void run() &#123; JedisHelper.coreCluster().subscribe(redisSubscribeListener, redisChannel); &#125; &#125;); &#125;&#125; （2）RedisSubscribeListener.java：Redis消息监听者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.ylp.core.monitor.redis;import com.ylp.common.tools.utils.JsonUtil;import com.ylp.core.monitor.biz.MonitorBiz;import com.ylp.facade.monitor.vo.PaymentRecordMonitorVo;import org.apache.commons.lang3.StringUtils;import org.apache.log4j.Logger;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import redis.clients.jedis.Client;import redis.clients.jedis.JedisPubSub;import java.io.IOException;/** * @author: 会跳舞的机器人 * @date: 16/8/23 下午2:57 * @description: Redis消息队列监听者 */@Component(&quot;redisSubscribeListener&quot;)public class RedisSubscribeListener extends JedisPubSub &#123; private Logger logger = Logger.getLogger(RedisSubscribeListener.class); @Autowired private MonitorBiz monitorBiz; public RedisSubscribeListener() &#123; super(); &#125; @Override public void onMessage(String channel, String message) &#123; logger.info(&quot;Redis received...&quot;); logger.info(&quot;channel:&quot; + channel + &quot;,message:&quot; + message); if (StringUtils.isEmpty(message)) &#123; return; &#125; PaymentRecordMonitorVo paymentRecordMonitorVo = null; try &#123; paymentRecordMonitorVo = JsonUtil.jsonToObject(message, PaymentRecordMonitorVo.class); &#125; catch (IOException e) &#123; logger.error(&quot;message转PaymentRecordMonitorVo异常,message=&quot; + message, e); return; &#125; // 先把该笔交易订单信息解析存入对应的统计时间段内,每隔15分钟汇总一次数据,存入Redis数据汇总集合 long start = System.currentTimeMillis(); monitorBiz.periodStatistics(paymentRecordMonitorVo); logger.info(&quot;monitorBiz.periodStatistics()执行耗时:&quot; + (System.currentTimeMillis() - start) + &quot;ms&quot;); // 再统计当天各个时间段的总数据,即为实时的交易数据 start = System.currentTimeMillis(); monitorBiz.publishRealTimeData(); logger.info(&quot;monitorBiz.publishRealTimeData()执行耗时:&quot; + (System.currentTimeMillis() - start) + &quot;ms&quot;); super.onMessage(channel, message); &#125; &#125; （3）MonitorBiz.java：Redis数据统计业务类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127package com.ylp.core.monitor.biz;import com.ylp.common.tools.utils.DateUtils;import com.ylp.common.tools.utils.JsonUtil;import com.ylp.facade.monitor.utils.JedisHelper;import com.ylp.facade.monitor.utils.MonitorUtils;import com.ylp.facade.monitor.utils.NumberUtil;import com.ylp.facade.monitor.vo.PaymentRecordMonitorVo;import com.ylp.facade.monitor.vo.ScreenDataVo;import com.ylp.facade.monitor.vo.StatisticsVo;import com.ylp.facade.trade.enums.PayWayEnum;import org.apache.log4j.Logger;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component;import org.springframework.util.CollectionUtils;import redis.clients.jedis.Tuple;import java.io.IOException;import java.math.BigDecimal;import java.util.*;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * @author: 会跳舞的机器人 * @date: 16/8/25 下午6:12 * @description: Redis数据统计业务类 */@Component(&quot;monitorBiz&quot;)public class MonitorBiz &#123; private Logger logger = Logger.getLogger(MonitorBiz.class); /** * 实时数据监控的数据队列 */ @Value(&quot;#&#123;settings[&apos;redis.trade.screen.channel&apos;]&#125;&quot;) private String redisTradeScreenChannel; /** * 微信支付的订单详细数据key */ @Value(&quot;#&#123;settings[&apos;redis.trade.wxDetails&apos;]&#125;&quot;) private String redisWxQueue; /** * 支付宝支付的订单详细数据key */ @Value(&quot;#&#123;settings[&apos;redis.trade.alipayDetails&apos;]&#125;&quot;) private String redisAlipayQueue; /** * 微信支付的订单汇总数据 */ @Value(&quot;#&#123;settings[&apos;redis.trade.wxSummary&apos;]&#125;&quot;) private String redisWxStatisticsQueue; /** * 支付宝支付的订单汇总数据 */ @Value(&quot;#&#123;settings[&apos;redis.trade.alipaySummary&apos;]&#125;&quot;) private String redisAlipayStatisticsQueue; /** * 单个线程的线程池 */ private ExecutorService executorService = Executors.newSingleThreadExecutor(); /** * 分时间间隔的数据统计,将时间段内的数据存入Redis有序集合 * * @param paymentRecordMonitorVo 交易订单信息 * @return */ public void periodStatistics(final PaymentRecordMonitorVo paymentRecordMonitorVo) &#123; // 根据支付订单的交易时间获取该笔交易应该划分至哪个统计时间段内 Map&lt;String, Date&gt; dateMap = MonitorUtils.getTimeSpace(paymentRecordMonitorVo.getTradeTime()); Date start = dateMap.get(&quot;previous&quot;); Date end = dateMap.get(&quot;next&quot;); logger.info(&quot;start:&quot; + DateUtils.dateToTime(start) + &quot;,end:&quot; + DateUtils.dateToTime(end)); String payWay = paymentRecordMonitorVo.getPayWay(); String queue = payWay.equals(PayWayEnum.WEIXIN.name()) ? redisWxStatisticsQueue : payWay.equals(PayWayEnum.ALIPAY.name()) ? redisAlipayStatisticsQueue : &quot;&quot;; StatisticsVo result = new StatisticsVo(); result.setPayWay(payWay); result.setPeriodTime(end); // 查询该时间段内,统计集合中的数据 Set&lt;String&gt; wxSet = JedisHelper.dataCluster().zrangeByScore(queue, end.getTime(), end.getTime()); // 如果汇总集合中没有该时间段的数据,那么则新增 if (CollectionUtils.isEmpty(wxSet)) &#123; logger.info(&quot;wxSet is null.... &quot;); result.setTotalOrderCount(1); result.setTotalOrderAmount(paymentRecordMonitorVo.getAmount().doubleValue()); JedisHelper.dataCluster().zadd(queue, end.getTime(), JsonUtil.toJsonString(result)); &#125; else &#123; logger.info(&quot;wxSet is not null and wxSet.size()=&quot; + wxSet.size()); Double amount = 0.0; Integer count = 0; Iterator&lt;String&gt; iterator = wxSet.iterator(); while (iterator.hasNext()) &#123; try &#123; StatisticsVo statisticsVo = JsonUtil.jsonToObject(iterator.next(), StatisticsVo.class); logger.info(&quot;statisticsVo:&quot; + JsonUtil.toJsonString(statisticsVo)); amount = NumberUtil.add(statisticsVo.getTotalOrderAmount().doubleValue(), amount.doubleValue()); count = count + statisticsVo.getTotalOrderCount(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; result.setTotalOrderCount(count + 1); result.setTotalOrderAmount(NumberUtil.add(amount, paymentRecordMonitorVo.getAmount().doubleValue())); // 先移除旧数据 long removeRow = JedisHelper.dataCluster().zremrangeByScore(queue, end.getTime(), end.getTime()); logger.info(&quot;移除旧数据记录数:&quot; + removeRow); // 再插入最新的数据 JedisHelper.dataCluster().zadd(queue, end.getTime(), JsonUtil.toJsonString(result)); &#125; // 异步删除Redis队列中的订单详细信息 final String key = payWay.equals(PayWayEnum.WEIXIN.name()) ? redisWxQueue : payWay.equals(PayWayEnum.ALIPAY.name()) ? redisAlipayQueue : &quot;&quot;; executorService.submit(new Runnable() &#123; @Override public void run() &#123; // 指定元素进行删除 Long row = JedisHelper.dataCluster().lrem(key, 0, JsonUtil.toJsonString(paymentRecordMonitorVo)); logger.info(&quot;删除明细记录数:&quot; + row); &#125; &#125;); logger.info(&quot;result:&quot; + JsonUtil.toJsonString(result)); &#125; /** * 实时数据推送 */ public void publishRealTimeData() &#123; ScreenDataVo screenDataVo = dayStatistics(); // 将屏幕监控的数据放入Redis消息中 JedisHelper.dataCluster().publish(redisTradeScreenChannel, JsonUtil.toJsonString(screenDataVo)); logger.info(&quot;监控实时汇总数据消息发送成功...&quot;); &#125;&#125; 3.3、监控应用系统系统采用前后端分离的模式，后端提供获取初始化数据的API给前端进行调用，数据初始化完成后，每笔交易产生的数据变动都通过WebSocket的方式实时传输到浏览器客户端进行展示。 3.3.1.主要功能： 实现WebSocket的服务端，将实时的交易数据推送至客户端 作为Redis消息的订阅方，获取到消息数据后，将数据通过WebSocket的方式广播出去3.3.2.遇到的坑 （1）在本地测试的时候，WebSocket服务端与客户端能进行正常的通信，部署至服务器后，客户端却无法连接到服务端，后来发现本地IDEA运行的是Tomcat8，服务器上部署的是Tomcat7.x版本，于是把服务器上的Tomcat版本换成8后就能进行正常的通信了。WebSocket所需要的jar包如下：12345678910&lt;dependency&gt; &lt;groupId&gt;javax.websocket&lt;/groupId&gt; &lt;artifactId&gt;javax.websocket-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.glassfish.tyrus.bundles&lt;/groupId&gt; &lt;artifactId&gt;tyrus-standalone-client&lt;/artifactId&gt; &lt;version&gt;1.9&lt;/version&gt;&lt;/dependency&gt; 3.3.3.部分代码（1）WebSocketServer.java：WebSocket服务端123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package com.ylp.webSocket.server;import org.apache.log4j.Logger;import javax.websocket.*;import javax.websocket.server.ServerEndpoint;import java.util.Map;import java.util.Set;import java.util.concurrent.ConcurrentHashMap;/** * @author: 会跳舞的机器人 * @date: 16/8/24 上午11:25 * @description:WebSocket服务端 */@ServerEndpoint(&quot;/tradeSocketServer&quot;)public class WebSocketServer &#123; private Logger logger = Logger.getLogger(this.getClass().getName()); /** * 客户端会话集合 */ private static Map&lt;String, Session&gt; sessionMap = new ConcurrentHashMap&lt;String, Session&gt;(); /** * 连接建立成功需要执行的方法 * * @param session */ @OnOpen public void onOpen(Session session) &#123; logger.info(&quot;onOpen,sessionId:&quot; + session.getId()); sessionMap.put(session.getId(), session); &#125; /** * 收到客户端调用后需要执行的方法 * * @param message * @param session */ @OnMessage public void onMessage(String message, Session session) &#123; logger.info(&quot;received message:&quot; + message); broadcastAll(message); &#125; /** * 广播给所有客户端 * * @param message */ public static void broadcastAll(String message) &#123; Set&lt;Map.Entry&lt;String, Session&gt;&gt; set = sessionMap.entrySet(); for (Map.Entry&lt;String, Session&gt; i : set) &#123; try &#123; i.getValue().getBasicRemote().sendText(message); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 连接关闭时执行的方法 * * @param session * @param closeReason */ @OnClose public void onClose(Session session, CloseReason closeReason) &#123; sessionMap.remove(session.getId()); logger.info(String.format(&quot;Session %s closed because of %s&quot;, session.getId(), closeReason)); &#125; /** * 发生错误时执行的方法 * * @param session * @param throwable */ @OnError public void error(Session session, Throwable throwable) &#123; sessionMap.remove(session.getId()); logger.error(&quot;异常&quot;, throwable); &#125;&#125; （3）RedisSubscribeListener.java：Redis消息监听者，这个工程中又要实现一遍，只不过是收到消息后需要执行的业务逻辑不一样，同样的，RedisSubcribeHelper.java类也要重新实现一遍，后续可以优化12345678910111213141516171819202122232425262728293031323334353637383940package com.ylp.redis;import com.ylp.common.tools.utils.JsonUtil;import com.ylp.facade.monitor.vo.ScreenDataVo;import com.ylp.webSocket.server.WebSocketServer;import org.apache.commons.lang3.StringUtils;import org.apache.log4j.Logger;import org.springframework.stereotype.Component;import redis.clients.jedis.Client;import redis.clients.jedis.JedisPubSub;import java.io.IOException;/** * @author: 会跳舞的机器人 * @date: 16/8/23 下午2:57 * @description: Redis消息队列监听者 */@Component(&quot;redisSubscribeListener&quot;)public class RedisSubscribeListener extends JedisPubSub &#123; private Logger logger = Logger.getLogger(RedisSubscribeListener.class); public RedisSubscribeListener() &#123; super(); &#125; @Override public void onMessage(String channel, String message) &#123; logger.info(&quot;Redis received...&quot;); logger.info(&quot;channel:&quot; + channel + &quot;,message:&quot; + message); if (StringUtils.isEmpty(message)) &#123; return; &#125; try &#123; ScreenDataVo screenDataVo = JsonUtil.jsonToObject(message, ScreenDataVo.class); logger.info(&quot;screenDataVo:&quot; + JsonUtil.toJsonString(screenDataVo)); &#125; catch (IOException e) &#123; logger.error(&quot;message转ScreenDataVo异常,message=&quot; + message, e); e.printStackTrace(); &#125; // 将实时交易数据推送至WebSocketServer WebSocketServer.broadcastAll(message); super.onMessage(channel, message); &#125;&#125; （4）WebSocket的客户端（模拟）123456789101112131415161718192021222324252627282930313233343536&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;title&gt;Index&lt;/title&gt; &lt;script type=&quot;text/javascript&quot;&gt; var ws = null; function startWebSocket() &#123; if (&apos;WebSocket&apos; in window) ws = new WebSocket(&quot;ws://172.xx.xx.xx/tradeSocketServer&quot;); else if (&apos;MozWebSocket&apos; in window) ws = new MozWebSocket(&quot;ws://172.xx.xx.xx/tradeSocketServer&quot;); else alert(&quot;not support&quot;); ws.onmessage = function (evt) &#123; alert(evt.data); &#125;; ws.onclose = function (evt) &#123; alert(&quot;close&quot;); &#125;; ws.onopen = function (evt) &#123; alert(&quot;open&quot;); &#125;; &#125; function sendMsg() &#123; ws.send(document.getElementById(&apos;writeMsg&apos;).value); &#125; &lt;/script&gt;&lt;/head&gt;&lt;body onload=&quot;startWebSocket();&quot;&gt;&lt;input type=&quot;text&quot; id=&quot;writeMsg&quot;&gt;&lt;/input&gt;&lt;input type=&quot;button&quot; value=&quot;send&quot; onclick=&quot;sendMsg()&quot;&gt;&lt;/input&gt;&lt;/body&gt;&lt;/html&gt; 3.3.4.WebSocket服务端通过域名访问配置Nginx1.WebSocket服务端如果要通过域名来访问，需要配置Nginx，附上Nginx相关的配置信息，需要特别配置map选项Nginx版本：Tengine version: Tengine/2.1.1 (nginx/1.6.2)1234567891011121314151617181920212223242526272829map $http_upgrade $connection_upgrade &#123; default upgrade; &apos;&apos; close;&#125;server &#123; listen 80; server_name xxx.xxx.com; location / &#123; root /usr/local/www/monitorsystem; index index.html; try_files $uri $uri/ /index.html =404; &#125; location /monitor/ &#123; proxy_pass http://172.xx.xx.xx:0000/; add_header &apos;Access-Control-Allow-Origin&apos; &apos;*&apos;; add_header &apos;Access-Control-Allow-Credentials&apos; &apos;true&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;POST, GET, OPTIONS,PUT,DELETE&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;*,token&apos;; proxy_set_header Host $http_host; proxy_set_header Cookie $http_cookie; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_read_timeout 1200; client_max_body_size 100m; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; &#125; 2.遇到的坑 在客户端不活动的情况下，默认60秒后会客户端会自动断开与服务端的连接，通过Nginx中的proxy_red_timeout参数可以配置，客户端也可以通过每隔一段时间去请求一次服务端，保持于服务端的连接。 4、总结与优化1、在测试部门进行高并发测试的情况下，periodStatistics()统计方法会出现数据统计错乱的情况，所以针对这个方法要加上同步代码块 2、上述的代码在单台服务器部署的情况下，基本上没啥问题，但是如果部署在多台服务器上（分布式部署），那么很多问题就发生咯，比如说：问题1：Redis消息发布与订阅，消息发布出去了，A服务器中的应用会收到消息然后进行统计，B服务器中的应用也会收到相同的消息然后进行计算，那计算出来的结果就多出一倍的数据了。 问题2：当日实时数据是统计Redis中“每15分钟汇总”数据的SortedSet，即一天被分为96条汇总数据，统计当日实时数据的时候就把96条数据进行累加即可。但是 但是在每一笔订单过来的时候，会先计算出此次统计的最新结果，再把旧的“每15分钟汇总”数据记录给删除，然后再插入此次计算的最新结果（这样做的原因是SortedSet没有更新的方法），代码请看MonitorBiz的periodStatistics()方法，这样就会存在一个问题：A服务器计算出最新统计结构，然后把旧数据删除了，还没插入最新统计数据的时候，B机器去获取当日实时数据，这时候就会缺少那条已删除还未插入的数据，造成数据的错误。 那针对上面两个问题的解决方案是：问题1解决方案：Redis消息发送的时候只发布一条简单的消息通知即可，把交易订单的数据存入Redis的list队列中，如果多个服务器都收到了消息，便去list队列中获取最新的一条记录，取到则进行处理，取不到则直接返回，Redis的list队列是能保证只有一台机器能取到数据的。 问题2解决方案：最开始设计的时候决定用SortedSet这种数据结构是因为它能满足排序的要求，但是从上面的问题看来，它也只是能解决排序的问题而已（前端要求要按时间排好顺序在“获取初始化数据”的接口中将数据发给他），对于并发的读写数据的操作都不能很好的支持，并不能解决上面所说的问题，所以最后决定改用Hash这种数据结构，原因是因为它支持原子性的数据累加操作，在高并发的情况下，能有很好的支持，如hincrby、hincrbyfloat方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 1.每隔15分钟汇总一次数据,存入Redis数据汇总集合 * 2.当日实时数据进行累加操作 * * @param message 交易订单类型 * @return */ public void periodStatistics(String message) &#123; String listStr = &quot;&quot;; if (StringUtils.isNotEmpty(message) &amp;&amp; message.equals(PayWayEnum.WEIXIN.name())) &#123; listStr = JedisHelper.dataCluster().lpop(redisWxQueue); &#125; else if (StringUtils.isNotEmpty(message) &amp;&amp; message.equals(PayWayEnum.ALIPAY.name())) &#123; listStr = JedisHelper.dataCluster().lpop(redisAlipayQueue); &#125; // 为空则说明数据被其他服务器取走了,则不用进行处理 if (StringUtils.isEmpty(listStr)) &#123; return; &#125; PaymentRecordMonitorVo paymentRecordMonitorVo = null; try &#123; paymentRecordMonitorVo = JsonUtil.jsonToObject(listStr, PaymentRecordMonitorVo.class); &#125; catch (IOException e) &#123; e.printStackTrace(); return; &#125; // 每日实时数据的hash结构增量统计 increaseBy(paymentRecordMonitorVo); // 分时间段的汇总统计 periodStatistics(paymentRecordMonitorVo); &#125; /** * 每日实时数据的hash结构增量统计 * * @param paymentRecordMonitorVo */ private void increaseBy(PaymentRecordMonitorVo paymentRecordMonitorVo) &#123; // step1:微信订单数据处理 if (paymentRecordMonitorVo.getPayWay().equals(PayWayEnum.ALIPAY.name())) &#123; JedisHelper.dataCluster().hincrBy(daySummaryKey, dayAlipayCount, 1); JedisHelper.dataCluster().hincrByFloat(daySummaryKey, dayAlipayAmount, paymentRecordMonitorVo.getAmount().doubleValue()); // step2:支付宝订单数据处理 &#125; else if (paymentRecordMonitorVo.getPayWay().equals(PayWayEnum.WEIXIN.name())) &#123; JedisHelper.dataCluster().hincrBy(daySummaryKey, dayWxCount, 1); JedisHelper.dataCluster().hincrByFloat(daySummaryKey, dayWxAmount, paymentRecordMonitorVo.getAmount().doubleValue()); &#125; // step3:总的数据增加 JedisHelper.dataCluster().hincrBy(daySummaryKey, dayTotalCount, 1); JedisHelper.dataCluster().hincrByFloat(daySummaryKey, dayTotalAmount, paymentRecordMonitorVo.getAmount().doubleValue()); &#125; 3、由于系统在设计之初是满足实时交易数据显示的，所以在每一笔订单过来的时候都会进行数据统计，然后利用WebSocket进行实时数据推送到客户端，在进行压力测试的时候，每秒的并发达到上百个，于是浏览器客户端就会发生崩溃的现象，因为每秒都会向浏览器客户端推送上百次解决方案：在服务端进行数据统计后，每隔2秒把最新的数据推送至浏览器客户端。 4、前期开发的时候为了更好的调试与问题的定位，所以写了很多logger.info()这种日志记录操作，在上生产环境的时候要把这些内容去掉或者日志级别改为debug，这种日志记录会设计到磁盘IO读写，在某些时候可能会成为系统性能的瓶颈。 注：上述的代码大部分都已经过优化，优化后的代码就不贴出来了。]]></content>
      <categories>
        <category>系统设计</category>
        <category>工作记录</category>
      </categories>
      <tags>
        <tag>系统设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次线上Java程序导致服务器CPU占用率过高的问题排除过程]]></title>
    <url>%2F2017%2F11%2F30%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%BA%BF%E4%B8%8AJava%E7%A8%8B%E5%BA%8F%E5%AF%BC%E8%87%B4%E6%9C%8D%E5%8A%A1%E5%99%A8CPU%E5%8D%A0%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E6%8E%92%E9%99%A4%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1、故障现象客服同事反馈平台系统运行缓慢，网页卡顿严重，多次重启系统后问题依然存在。 2、问题定位使用top命令查看服务器情况，发现CPU占用率过高。 2.1、定位问题进程使用top命令查看资源占用情况，发现pid为14063的进程占用了大量的CPU资源，CPU占用率高达776.1%，内存占用率也达到了29.8% 12345678910[ylp@ylp-web-01 ~]$ toptop - 14:51:10 up 233 days, 11:40, 7 users, load average: 6.85, 5.62, 3.97Tasks: 192 total, 2 running, 190 sleeping, 0 stopped, 0 zombie%Cpu(s): 97.3 us, 0.3 sy, 0.0 ni, 2.5 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 16268652 total, 5114392 free, 6907028 used, 4247232 buff/cacheKiB Swap: 4063228 total, 3989708 free, 73520 used. 8751512 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 14063 ylp 20 0 9260488 4.627g 11976 S 776.1 29.8 117:41.66 java 2.2、定位问题线程使用ps -mp pid -o THREAD,tid,time命令查看该进程的线程情况，发现该进程的多个线程占用率很高 123456789101112131415161718[ylp@ylp-web-01 ~]$ ps -mp 14063 -o THREAD,tid,timeUSER %CPU PRI SCNT WCHAN USER SYSTEM TID TIMEylp 361 - - - - - - 02:05:58ylp 0.0 19 - futex_ - - 14063 00:00:00ylp 0.0 19 - poll_s - - 14064 00:00:00ylp 44.5 19 - - - - 14065 00:15:30ylp 44.5 19 - - - - 14066 00:15:30ylp 44.4 19 - - - - 14067 00:15:29ylp 44.5 19 - - - - 14068 00:15:30ylp 44.5 19 - - - - 14069 00:15:30ylp 44.5 19 - - - - 14070 00:15:30ylp 44.5 19 - - - - 14071 00:15:30ylp 44.6 19 - - - - 14072 00:15:32ylp 2.2 19 - futex_ - - 14073 00:00:46ylp 0.0 19 - futex_ - - 14074 00:00:00ylp 0.0 19 - futex_ - - 14075 00:00:00ylp 0.0 19 - futex_ - - 14076 00:00:00ylp 0.7 19 - futex_ - - 14077 00:00:15 从输出信息可以看出，14065~14072之间的线程CPU占用率都很高 2.3、查看问题线程堆栈挑选TID为14065的线程，查看该线程的堆栈情况，先将线程id转为16进制，使用printf “%x\n” tid命令进行转换 123[ylp@ylp-web-01 ~]$ printf &quot;%x\n&quot; 1406536f1 再使用jstack命令打印线程堆栈信息，命令格式：jstack pid |grep tid -A 3012345678910111213[ylp@ylp-web-01 ~]$ jstack 14063 |grep 36f1 -A 30&quot;GC task thread#0 (ParallelGC)&quot; prio=10 tid=0x00007fa35001e800 nid=0x36f1 runnable &quot;GC task thread#1 (ParallelGC)&quot; prio=10 tid=0x00007fa350020800 nid=0x36f2 runnable &quot;GC task thread#2 (ParallelGC)&quot; prio=10 tid=0x00007fa350022800 nid=0x36f3 runnable &quot;GC task thread#3 (ParallelGC)&quot; prio=10 tid=0x00007fa350024000 nid=0x36f4 runnable &quot;GC task thread#4 (ParallelGC)&quot; prio=10 tid=0x00007fa350026000 nid=0x36f5 runnable &quot;GC task thread#5 (ParallelGC)&quot; prio=10 tid=0x00007fa350028000 nid=0x36f6 runnable &quot;GC task thread#6 (ParallelGC)&quot; prio=10 tid=0x00007fa350029800 nid=0x36f7 runnable &quot;GC task thread#7 (ParallelGC)&quot; prio=10 tid=0x00007fa35002b800 nid=0x36f8 runnable &quot;VM Periodic Task Thread&quot; prio=10 tid=0x00007fa3500a8800 nid=0x3700 waiting on condition JNI global references: 392 从输出信息可以看出，此线程是JVM的gc线程。此时可以基本确定是内存不足或内存泄露导致gc线程持续运行，导致CPU占用过高。所以接下来我们要找的内存方面的问题 3、内存问题定位3.1、使用jstat -gcutil命令查看进程的内存情况1234567891011121314[ylp@ylp-web-01 ~]$ jstat -gcutil 14063 2000 10 S0 S1 E O P YGC YGCT FGC FGCT GCT 0.00 0.00 100.00 99.99 26.31 42 21.917 218 1484.830 1506.747 0.00 0.00 100.00 99.99 26.31 42 21.917 218 1484.830 1506.747 0.00 0.00 100.00 99.99 26.31 42 21.917 219 1496.567 1518.484 0.00 0.00 100.00 99.99 26.31 42 21.917 219 1496.567 1518.484 0.00 0.00 100.00 99.99 26.31 42 21.917 219 1496.567 1518.484 0.00 0.00 100.00 99.99 26.31 42 21.917 219 1496.567 1518.484 0.00 0.00 100.00 99.99 26.31 42 21.917 219 1496.567 1518.484 0.00 0.00 100.00 99.99 26.31 42 21.917 220 1505.439 1527.355 0.00 0.00 100.00 99.99 26.31 42 21.917 220 1505.439 1527.355 0.00 0.00 100.00 99.99 26.31 42 21.917 220 1505.439 1527.355 从输出信息可以看出，Eden区内存占用100%，Old区内存占用99.99%，Full GC的次数高达220次，并且频繁Full GC，Full GC的持续时间也特别长，平均每次Full GC耗时6.8秒（1505.439/220）。根据这些信息，基本可以确定是程序代码上出现了问题，可能存在不合理创建对象的地方 3.2、分析堆栈使用jstack命令查看进程的堆栈情况 12[ylp@ylp-web-01 ~]$ jstack 14063 &gt;&gt;jstack.out 把jstack.out文件从服务器拿到本地后，用编辑器查找带有项目目录并且线程状态是RUNABLE的相关信息，从图中可以看出ActivityUtil.java类的447行正在使用HashMap.put()方法 3.3、代码定位打开项目工程，找到ActivityUtil类的477行，代码如下： 找到相关同事了解后，这段代码会从数据库中获取配置，并根据数据库中remain的值进行循环，在循环中会一直对HashMap进行put操作。 查询数据库中的配置，发现remain的数量巨大。 至此，问题定位完毕。]]></content>
      <categories>
        <category>工作记录</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>内存溢出</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git合并特定commit到指定分支]]></title>
    <url>%2F2017%2F11%2F30%2Fgit%E5%90%88%E5%B9%B6%E7%89%B9%E5%AE%9Acommit%E5%88%B0%E6%8C%87%E5%AE%9A%E5%88%86%E6%94%AF%2F</url>
    <content type="text"><![CDATA[应用场景： 在A分支上提交了一个commit，B分支也同样需要这个commit的代码，为了避免人工复制代码，可以用git的一些操作替代 1、先找到A分支的commit代号，1a3c79c2b9cabb710b239c291a64cde2fe2b4599，sourcetree工具里面可以看到，在gitlab的提交记录里面也可以看到的 2、执行以下命令： git checkout B git cherry-pick 1a3c79c2b9cabb710b239c291a64cde2fe2b4599 首先要检出B分支的代码，再通过git的cherry-pick命令合并，1a3c79c2b9cabb710b239c291a64cde2fe2b4599为在A分支上commit的代号，合并完之后再确认下代码是否有push到远端，如果没有，则手动push一次即可。]]></content>
      <categories>
        <category>工作记录</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat启动异常Error listenerStart]]></title>
    <url>%2F2017%2F11%2F30%2Ftomcat%E5%90%AF%E5%8A%A8%E5%BC%82%E5%B8%B8Error-listenerStart%2F</url>
    <content type="text"><![CDATA[tomcat启动失败，在catalina.out文件中没有详细的异常信息，只有下面这种提示启动失败的日志。由于没有详细的异常信息，不便于排查错误。 1234九月 12, 2017 10:11:31 上午 org.apache.catalina.core.StandardContext startInternal严重: Error listenerStart九月 12, 2017 10:11:31 上午 org.apache.catalina.core.StandardContext startInternal严重: Context [/weipan01] startup failed due to previous errors 其实在logs文件下还有一种以localhost开头的日志文件，如果在catalina.out中没有打印详细的异常信息，那么在localhost日志文件中一般是会有详细的错误信息的。 另外一种方式就是新建一个logging.properties文件放在WEB-INF/classes目录下，内容如下，通过这种方式把tomcat启动的日志信息输出到error-debug文件中。 12345678910111213handlers = org.apache.juli.FileHandler, java.util.logging.ConsoleHandler ############################################################ # Handler specific properties. # Describes specific configuration info for Handlers. ############################################################ org.apache.juli.FileHandler.level = FINE org.apache.juli.FileHandler.directory = $&#123;catalina.base&#125;/logs org.apache.juli.FileHandler.prefix = error-debug. java.util.logging.ConsoleHandler.level = FINE java.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatter]]></content>
      <categories>
        <category>工作记录</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo无法创建新线程异常解决方案]]></title>
    <url>%2F2017%2F11%2F30%2FDubbo%E6%97%A0%E6%B3%95%E5%88%9B%E5%BB%BA%E6%96%B0%E7%BA%BF%E7%A8%8B%E5%BC%82%E5%B8%B8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[一、问题在测试环境遇到的异常信息，如下： 异常信息：1Caused by: java.lang.OutOfMemoryError: unable to create new native thread 二、问题分析项目的实际配置：1&lt;dubbo:provider timeout=&quot;50000&quot; threadpool=&quot;fixed&quot; threads=&quot;500&quot; accepts=&quot;1000&quot; /&gt; timeout=”5000”：设置远程调用服务的超时时间为5000毫秒 threadpool=”fixed”：线程模型为固定大小的线程池，启动时建立线程，不关闭，一直持有 threads=”500”：线程数为500 accepts=”1000”：限制服务器端的接受的连接的最大值为1000 再看看dubbo官网上的线程模型的内容 结合上面的异常信息，我们可以知道dispatcher的默认配置值为all（AllChannelHandler来处理消息请求），因为测试环境上部署了好几个应用，如果每个应用都占用了500个线程，那Linux机器中默认配置的线程数是不够用的，所以就导致java.lang.OutOfMemoryError: unable to create new native thread 三、问题解决方案结合Linux线程数限制配置来进行调优 1[ylp@test-web-pay-01 ~]$ vi /etc/security/limits.d/20-nproc.conf 123456# Default limit for number of user&apos;s processes to prevent# accidental fork bombs.# See rhbz #432903 for reasoning.* soft nproc 1024root soft nproc unlimited 从配置文件中我们可以知道，root用户的创建线程数是没有限制的，普通用户可以创建的线程数默认是1024，如果一个Linux机器上部署太多的应用，那么就会耗光线程数，导致java.lang.OutOfMemoryError: unable to create new native thread异常可以使用Linux命令查看可以创建最大的进程数 12[ylp@test-web-pay-01 ~]$ ulimit -u63477 那我们一般把值设置为和ulimit相同即可调整时要注意： 尽量不要使用 root 用户来部署应用程序,避免资源耗尽后无法登录操作系统。 普通用户的线程数限制值要看可用物理内存容量来配置，同时还要结合Linux机器上有多少个用户]]></content>
      <categories>
        <category>工作记录</category>
        <category>Dubbo</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud（6）——注册中心高可用集群]]></title>
    <url>%2F2017%2F11%2F30%2FSpring-Cloud%EF%BC%886%EF%BC%89%E2%80%94%E2%80%94%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[在Spring Cloud（1）——服务注册中心这篇文章中，我们已经搭建好一个单机的注册中心。这篇文章要做的就是把单机版的注册中心改造为高可用集群模式。 我们可以创建三个注册中心节点，每个节点进行两两注册，实现完全对等的效果，可以达到集群的最高可用性，任何一个节点挂掉都不会影响服务的注册与发现。 1、服务器准备 主机名 IP eureka-server-peer1 192.168.31.117 eureka-server-peer2 192.168.31.146 eureka-server-peer3 192.168.31.173 修改各个服务器的hosts文件，添加主机名与IP的映射，内容如下： 123192.168.31.117 eureka-server-peer1192.168.31.146 eureka-server-peer2192.168.31.173 eureka-server-peer3 开放所需要的8761、8762、8763端口 修改系统的iptables文件：1# vi /etc/sysconfig/iptables 在192.168.31.117、192.168.31.146、192.168.31.173分别增加12345678## microservice-eureka-server-A INPUT -m state --state NEW -m tcp -p tcp --dport 8761 -j ACCEPT## microservice-eureka-server-A INPUT -m state --state NEW -m tcp -p tcp --dport 8762 -j ACCEPT## microservice-eureka-server-A INPUT -m state --state NEW -m tcp -p tcp --dport 8763 -j ACCEPT 配置重启生效1# service iptables restart 以上是针对Linux部署的，如果只是Windows本地，那同样可以修改hosts文件内容123127.0.0.1 eureka-server-peer1127.0.0.1 eureka-server-peer2127.0.0.1 eureka-server-peer3 Windows的hosts路径为：C:\Windows\System32\drivers\etc 2、修改application.yml文件创建三个注册中心节点，分别为peer1、peer2、peer3，在yml格式文件中，用—分割每个节点的内容，等同于创建了三个文件的效果。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647---spring: application: name: microservice-eureka-server profiles: peer1server: port: 8761eureka: instance: hostname: eureka-server-peer1 client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://eureka-server-peer2:8762/eureka/,http://eureka-server-peer3:8763/eureka/---spring: application: name: microservice-eureka-server profiles: peer2server: port: 8762eureka: instance: hostname: eureka-server-peer2 client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://eureka-server-peer1:8761/eureka/,http://eureka-server-peer3:8763/eureka/---spring: application: name: microservice-eureka-server profiles: peer3server: port: 8763eureka: instance: hostname: eureka-server-peer3 client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://eureka-server-peer1:8761/eureka/,http://eureka-server-peer2:8762/eureka/ 3、将jar包上传至服务器并运行 将jar包上传至目录/apps/microservice-eureka-server 启动：每台机器指定不同的spring.profiles.active启动参数 123java -jar microservice-eureka-server.jar --spring.profiles.active=peer1java -jar microservice-eureka-server.jar --spring.profiles.active=peer2java -jar microservice-eureka-server.jar --spring.profiles.active=peer3 4、客户端使用配置将之前注册中心单节点的配置改为多节点即可，如下：1eureka.client.serviceUrl.defaultZone=http://192.168.31.117:8761/eureka/,http://192.168.31.146:8762/eureka/,http://192.168.31.173:8763/eureka/ 效果图：]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
        <tag>Eureka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud（5）——熔断器]]></title>
    <url>%2F2017%2F11%2F30%2FSpring-Cloud%EF%BC%885%EF%BC%89%E2%80%94%E2%80%94%E7%86%94%E6%96%AD%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1、简介Hystrix熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力。 2、项目实例使用Hystrix与Feign来实现服务熔断，本节的内容基于Spring Cloud（3）——服务消费者进行改造 1、UserFeignClient指定fallback类1234567891011121314151617181920212223242526package com.baibei.consumer.user.service;import com.baibei.common.core.api.ApiResult;import com.baibei.consumer.user.service.hystrix.UserFeignClientHystrix;import org.springframework.cloud.netflix.feign.FeignClient;import org.springframework.web.bind.annotation.*;/** * @author: 会跳舞的机器人 * @date: 2017/3/29 16:54 * @description: 用户服务调用端 */@FeignClient(name = &quot;microservice-provider-user&quot;, fallback = UserFeignClientHystrix.class)public interface UserFeignClient &#123; /** * 根据ID查找用户 * * @param * @return */ @RequestMapping(value = &quot;/user/&#123;id&#125;&quot;, method = RequestMethod.GET) ApiResult findUserById(@PathVariable(&quot;id&quot;) Integer id);&#125; 2、编写fallback类12345678910111213141516171819202122232425package com.baibei.consumer.user.service.hystrix;import com.baibei.common.core.api.ApiResult;import com.baibei.common.core.api.ApiResultGenerator;import com.baibei.consumer.user.service.UserFeignClient;import org.apache.log4j.Logger;import org.springframework.stereotype.Component;import org.springframework.web.bind.annotation.RequestParam;/** * @author: 会跳舞的机器人 * @date: 2017/3/29 16:55 * @description:用户服务的fallback */@Componentpublic class UserFeignClientHystrix implements UserFeignClient &#123; private Logger logger = Logger.getLogger(UserFeignClientHystrix.class); @Override public ApiResult findUserById(@RequestParam(&quot;id&quot;) Integer id) &#123; logger.info(&quot;请求异常，进入fallback方法，接收的参数：id=&quot; + id); return ApiResultGenerator.serviceError(null); &#125;&#125; UserFeignClientHystrix需要实现UserFeignClient服务接口，并实现其方法，返回在服务无法调用的情况下需要返回的容错值。 在服务提供者正常的情况下，服务消费者进行调用能获取到正确的结果，在服务提供者down掉的情况下，消费者再进行调用就会返回fallback方法中返回的容错值。]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
        <tag>Hystrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud（4）——分布式配置中心]]></title>
    <url>%2F2017%2F11%2F30%2FSpring-Cloud%EF%BC%884%EF%BC%89%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%2F</url>
    <content type="text"><![CDATA[一、简介Spring Cloud Config是一个配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion。Spring Cloud Config分为两部分 config-server：配置服务端，负责管理配置信息 config-client：配置客户端，通过调用server端暴露的接口来换取配置信息 二、项目实例创建maven工程microservice-config-server 1、在pom.xml中加入以下依赖123456789101112131415161718192021222324252627282930&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.4.4.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Camden.SR5&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 2、创建ConfigServerApplication.java1234567891011121314151617181920package com.baibei.config.server;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.config.server.EnableConfigServer;/** * @author: 会跳舞的机器人 * @email:2268549298@qq.com * @date: 17/2/19 下午2:35 * @description:分布式配置中心启动主类 */@SpringBootApplication@EnableConfigServerpublic class ConfigServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigServerApplication.class, args); &#125;&#125; 添加@EnableConfigServer注解开启Config Server 3、在resource文件夹下创建application.properties文件1234567891011#config-server对外提供的端口server.port=9001#git repo的url地址spring.cloud.config.server.git.uri=https://git.oschina.net/dreyer/microservice-config-repo.git#指定搜索路径，config-server会自动搜索根目录和指定目录（逗号分隔）下的文件spring.cloud.config.server.git.searchPaths=api,backend#有读取权限的git用户spring.cloud.config.server.git.username=username#git用户密码spring.cloud.config.server.git.password=password 4、服务端验证为了完成服务端的验证，我们需要在git上创建一个项目作为配置仓库，目录结构如下： . ├── api │ ├── business-dev.properties │ ├── business-prod.properties │ └── business-test.properties ├── exchange │ ├── business-dev.properties │ ├── business-prod.properties │ └── business-test.properties ├── component-dev.properties ├── component-prod.properties ├── component-test.properties ├── database-dev.properties ├── database-prod.properties ├── database-test.properties 接下来，我们就可以通过URL来访问到我们配置的内容了。例如我们可以通过123456789101112131415161718**URL与配置文件的映射关系如下：**- /&#123;application&#125;/&#123;profile&#125;[/&#123;label&#125;] 如： http://localhost:9001/database/dev- /&#123;application&#125;-&#123;profile&#125;.yml 如： http://localhost:9001/database-dev.yml- /&#123;label&#125;/&#123;application&#125;-&#123;profile&#125;.yml 如： http://localhost:9001/f-branch/database-dev.yml (f-branch分支下的database-dev.yml文件)- /&#123;application&#125;-&#123;profile&#125;.properties 如： http://localhost:9001/database-dev.properties (**不指定分支默认是MASTER分支**)- /&#123;label&#125;/&#123;application&#125;-&#123;profile&#125;.properties 如： http://localhost:9001/f-branch/database-dev.properties例如database-dev.properties 对应&#123;application&#125;-&#123;profile&#125;.properties， &#123;application&#125; 就是 database, &#123;profile&#125;就是dev。&#123;label&#125;对应git上不同的分支，默认为master。### 5.使用本地配置开发人员在本机进行开发时，可能会用到自已本地的一些配置信息，如果把这些配置提交至git上的话，又会影响到其他同事的开发，那么在这种场景下，开发人员是可以选择使用本地配置，而不是git上的配置信息的，做法也很简单，只需要在config-server的application.properties中添加spring.profiles.active=native，然后把相关的配置文件放在src/main/resource下即可### 6、配置中心客户端以 [Spring Cloud（2）—服务提供者](http://www.jianshu.com/p/ff8db428d365) 中的microservice-provider-user作为配置中心客户端#### 6.1. 在pom.xml中增加Config Server所需要的jar包 org.springframework.cloud spring-cloud-starter-config12#### 6.2. 创建bootstrap.properties来指定Config Server配置，内容如下： spring.cloud.config.uri=http://localhost:9001/spring.cloud.config.profile=devspring.cloud.config.label=masterspring.cloud.config.name=database,component,business1234567内容含义如下：- spring.application.name：对应前配置文件中的&#123;application&#125;部分- spring.cloud.config.profile：对应前配置文件中的&#123;profile&#125;部分- spring.cloud.config.label：对应前配置文件的git分支- spring.cloud.config.uri：配置中心的地址以上工作完成后，在代码中我们就可以使用@Value注解来调用相关的配置信息， @Value(“${datasource.url}”)private String dataSourceUrl;1配置中心完成之后，在microservice-provider-user工程的application.properties中的数据库配置就可以换为 spring.datasource.url=${datasource.url}spring.datasource.username=${datasource.username}spring.datasource.password=${datasource.password}1在配置中心客户端启动的时候，我们也可以看到相关的日志信息输出 2017-02-19 15:17:28.220 INFO 11206 — [ main] c.c.c.ConfigServicePropertySourceLocator : Fetching config from server at: http://localhost:9001/2017-02-19 15:17:29.565 INFO 11206 — [ main] c.c.c.ConfigServicePropertySourceLocator : Located environment: name=database,component,business, profiles=[dev], label=master, version=a9f8f18e682a03f4d7f046c754472f394313fa82, state=null2017-02-19 15:17:29.565 INFO 11206 — [ main] b.c.PropertySourceBootstrapConfiguration : Located property source: CompositePropertySource [name=’configService’, propertySources=[MapPropertySource [name=’configClient’], MapPropertySource [name=’https://git.oschina.net/dreyer/microservice-config-repo.git/database-dev.properties‘]]]2017-02-19 15:17:29.595 INFO 11206 — [ main] c.b.p.user.UserProviderApplication : No active profile set, falling back to default profiles: default``` 附项目目录截图：]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
        <tag>Spring Cloud Config</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud（3）——服务消费者]]></title>
    <url>%2F2017%2F11%2F30%2FSpring-Cloud%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94%E6%9C%8D%E5%8A%A1%E6%B6%88%E8%B4%B9%E8%80%85%2F</url>
    <content type="text"><![CDATA[一、简介Feign是一个声明式的Web Service客户端，它使得编写Web Serivce客户端变得更加简单。我们只需要使用Feign来创建一个接口并用注解来配置它既可完成。它具备可插拔的注解支持，包括Feign注解和JAX-RS注解。Feign也支持可插拔的编码器和解码器。Spring Cloud为Feign增加了对Spring MVC注解的支持，还整合了Ribbon和Eureka来提供均衡负载的HTTP客户端实现。 二、项目实例创建maven工程microservice-consumer-user 1、在pom.xml中添加依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657 &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.baibei.comsumer&lt;/groupId&gt; &lt;artifactId&gt;microservice-consumer-user&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;microservice-consumer-user Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.4.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Camden.SR5&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;finalName&gt;microservice-consumer-user&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2、创建UserConsumerApplication.java12345678910111213141516171819202122232425package com.baibei.consumer.user;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.cloud.netflix.feign.EnableFeignClients;import org.springframework.context.annotation.Bean;import org.springframework.web.client.RestTemplate;/** * @author: 会跳舞的机器人 * @email:2268549298@qq.com * @date: 17/2/17 上午11:45 * @description:用户服务消费者 */@SpringBootApplication@EnableDiscoveryClient@EnableFeignClientspublic class UserConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(UserConsumerApplication.class, args); &#125;&#125; EnableDiscoveryClient注解用来标识开启服务发现功能 EnableFeignClients注解用来开启Feign功能 3、创建application.properties文件1234567891011121314151617spring.application.name=microservice-consumer-userserver.port=8005# 注册中心地址eureka.client.serviceUrl.defaultZone=http://eureka-server-peer1:8761/eureka/,http://eureka-server-peer2:8762/eureka/,http://eureka-server-peer3:8763/eureka/# Indicates whether this client should fetch eureka registry information from eureka server# 客户端是否要从eureka server获取注册信息，默认为trueeureka.client.fetchRegistry=true# Indicates how often(in seconds) to fetch the registry information from the eureka server# 从eureka server获取注册信息的频率，默认为30秒，缩短配置时间可以缓解服务上线时间过长的问题eureka.client.registryFetchIntervalSeconds=10# 自定义负载均衡策略，microservice-provider-user为服务应用名microservice-provider-user.ribbon.NFLoadBalancerRuleClassName=com.netflix.loadbalancer.RandomRule 4、创建UserFeignClient12345678910111213141516171819202122232425package com.baibei.consumer.user.service;import com.baibei.common.core.api.ApiResult;import org.springframework.cloud.netflix.feign.FeignClient;import org.springframework.web.bind.annotation.*;/** * @author: 会跳舞的机器人 * @date: 2017/3/29 16:54 * @description: 用户服务调用端 */@FeignClient(name = &quot;microservice-provider-user&quot;)public interface UserFeignClient &#123; /** * 根据ID查找用户 * * @param * @return */ @RequestMapping(value = &quot;/user/&#123;id&#125;&quot;,method = RequestMethod.GET) ApiResult findUserById(@PathVariable(&quot;id&quot;) Integer id);&#125; FeignClient注解用来绑定对应的服务 5、web层的调用1234567891011121314151617181920212223242526272829303132333435363738package com.baibei.consumer.user.controller;import com.baibei.common.core.api.ApiResult;import com.baibei.consumer.user.service.UserFeignClient;import org.apache.log4j.Logger;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * @author: 会跳舞的机器人 * @email:2268549298@qq.com * @date: 17/2/17 上午11:51 * @description: */@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; private Logger logger = Logger.getLogger(UserController.class); @Autowired private UserFeignClient userFeignClient; /** * 根据ID查找用户信息 * * @param id * @return */ @GetMapping(&quot;/&#123;id&#125;&quot;) public ApiResult getUser(@PathVariable Integer id) &#123; return userFeignClient.findUserById(id); &#125;&#125; 通过注入UserFeignClient即可实现像调用本地方法一样去调用远程服务方法 附录：项目目录截图]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
        <tag>Feign</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud（2）——服务提供者]]></title>
    <url>%2F2017%2F11%2F30%2FSpring-Cloud%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94%E6%9C%8D%E5%8A%A1%E6%8F%90%E4%BE%9B%E8%80%85%2F</url>
    <content type="text"><![CDATA[前言：本文中的注册中心基于Spring Cloud（1）——服务注册中心，请先了解注册中心的相关知识后再阅读本文。 以用户服务为例，创建Maven工程microservice-provider-user 1、pom.xml加入以下依赖12345678910111213141516171819202122232425262728293031&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.4.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Camden.SR5&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 2、创建应用启动类UserProviderApplication.java123456789101112131415161718192021package com.baibei.provider.user;import org.mybatis.spring.annotation.MapperScan;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;/** * @author: 会跳舞的机器人 * @email:2268549298@qq.com * @date: 17/2/17 上午9:55 * @description:用户服务提供者 */@SpringBootApplication@EnableDiscoveryClientpublic class UserProviderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(UserProviderApplication.class, args); &#125;&#125; 开启服务注册功能很简单，只需要在启动类上加@EnableDiscoveryClient注解即可开启 3、在resource文件夹下创建application.properties，内容如下：12345678910111213#应用名称spring.application.name=microservice-provider-user#服务端口server.port=8003#注册中心地址eureka.client.serviceUrl.defaultZone=http://localhost:8761/eureka/#服务续租频率。默认是30秒，意思是服务提供者需要多久发送一次心跳检测至Eureka Server来确保Eureka Server知道提供者还存活着,#如果超过指定时间没有发送,Eureka Server则会从服务提供者列表中将其剔除eureka.instance.lease-renewal-interval-in-seconds=30#服务失效时间。默认是90秒，也就是如果Eureka Server在90秒内没有接收到来自服务提供者的Renew操作，就会把服务提供者剔除eureka.instance.leaseExpirationDurationInSeconds=90 4、编写一个查找用户的服务方法1234567891011121314151617181920212223242526272829303132333435package com.baibei.provider.user.controller;import com.baibei.provider.user.entity.User;import com.baibei.provider.user.service.IUserService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * @author: 会跳舞的机器人 * @email:2268549298@qq.com * @date: 17/2/17 上午10:01 * @description:用户相关rest API */@RestController@RequestMapping(&quot;/user&quot;)public class UserProviderController &#123; @Autowired private IUserService userService; /** * 查找用户 * * @param id * @return */ @GetMapping(value = &quot;/&#123;id&#125;&quot;) public User findById(@PathVariable Integer id) &#123; User user = userService.getUser(id); return user; &#125;&#125; 5、运行验证运行UserProviderApplication的main方法，启动成功后，访问http://localhost:8003/user/5 打开 http://localhost:8761/ Eureka Server注册中心地址，可以看到microservice-provider-user服务已经注册至注册中心 —附项目的完整代码包结构]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud（1）——服务注册中心]]></title>
    <url>%2F2017%2F11%2F30%2FSpring-Cloud%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%2F</url>
    <content type="text"><![CDATA[一、简介Eureka是一个云端服务发现，一个基于 REST 的服务，用于定位服务，以实现云端中间层服务发现和故障转移。用它我们可以实现服务注册与发现功能 二、项目实例1、创建Maven工程microservice-eureka-server，并在pom.xml中加入以下依赖包1234567891011121314151617181920212223242526272829&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.4.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Camden.SR5&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 2、创建应用启动类EurekaServerApplication.java123456789101112131415161718192021package com.baibei.eureka;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;/** * @author: 会跳舞的机器人 * @email:2268549298@qq.com * @date: 17/2/15 下午5:39 * @description:启动主类 */@SpringBootApplication@EnableEurekaServerpublic class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; 使用@EnableEurekaServer注解开启服务注册功能 3、配置application.yml在resource目录下创建application.yml文件，内容如下：12345678910111213spring: application: name: microservice-eureka-serverserver: port: 8761eureka: instance: hostname: eureka-server client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://eureka-server:8761/eureka/ 4、启动运行运行EurekaServerApplication的main方法，启动成功后，访问：http://localhost:8761/ 从图中我们可以看出，还没有任何服务注册至注册中心。 附项目目录截图：]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
        <tag>Eureka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis使用规范]]></title>
    <url>%2F2017%2F11%2F30%2FRedis%E4%BD%BF%E7%94%A8%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis实现用户登录错误次数限制]]></title>
    <url>%2F2017%2F11%2F30%2FRedis%E5%AE%9E%E7%8E%B0%E7%94%A8%E6%88%B7%E7%99%BB%E5%BD%95%E9%94%99%E8%AF%AF%E6%AC%A1%E6%95%B0%E9%99%90%E5%88%B6%2F</url>
    <content type="text"><![CDATA[系统登录的时候经常会有这种场景，如果密码连续N次输入错误，则要等N分钟之后才能重试。实现的方式有多种，比如在内存中维护一个数据结构来存储这些信息，但实现起来比较麻烦而且也存在问题，比如应用重启会导致数据丢失，并且内存的占用也是一个问题。 如果项目中已经有用到redis，那么使用redis来实现此功能是非常简单且有保障的。利用redis的String数据结构和超时自动过期机制，每错误一次，则错误值+1，并设置相应的过期时间，在登录的时候判断从key中获取到失败次数是否大于最大失败次数即可。 12345678910111213141516/** * 登录次数错误+1 * * @param userName */private void increaseFailedLoginCounter(String userName) &#123; String key = ERROR_COUNT_KEY + userName; JedisCluster cluster = jedisClusterManager.getJedisCluster(); String v = cluster.get(key); if (org.springframework.util.StringUtils.isEmpty(v)) &#123; cluster.set(key, &quot;1&quot;); &#125; else &#123; cluster.incr(key); &#125; cluster.expire(key, 1800);&#125;]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis持久化]]></title>
    <url>%2F2017%2F11%2F30%2FRedis%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[一、Redis的持久化Redis 提供了不同级别的持久化方式: RDB持久化方式能够在指定的时间间隔能对你的数据进行快照存储. AOF持久化方式记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据,AOF命令以redis协议追加保存每次写的操作到文件末尾. Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大.如果你只希望你的数据在服务器运行的时候存在,你也可以不使用任何持久化方式.你也可以同时开启两种持久化方式, 在这种情况下, 当redis重启的时候会优先载入AOF文件来恢复原始的数据,因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整. 二、RDB的优缺点1、RDB的优点RDB是一个非常紧凑的文件,它保存了某个时间点得数据集,非常适用于数据集的备份,比如你可以在每个小时报保存一下过去24小时内的数据,同时每天保存过去30天的数据,这样即使出了问题你也可以根据需求恢复到不同版本的数据集.RDB是一个紧凑的单一文件,很方便传送到另一个远端数据中心或者亚马逊的S3（可能加密），非常适用于灾难恢复. RDB在保存RDB文件时父进程唯一需要做的就是fork出一个子进程,接下来的工作全部由子进程来做，父进程不需要再做其他IO操作，所以RDB持久化方式可以最大化redis的性能.与AOF相比,在恢复大的数据集的时候，RDB方式会更快一些. 2、RDB的缺点如果你希望在redis意外停止工作（例如电源中断）的情况下丢失的数据最少的话，那么RDB不适合你.虽然你可以配置不同的save时间点(例如每隔5分钟并且对数据集有100个写的操作),是Redis要完整的保存整个数据集是一个比较繁重的工作,你通常会每隔5分钟或者更久做一次完整的保存,万一在Redis意外宕机,你可能会丢失几分钟的数据. RDB 需要经常fork子进程来保存数据集到硬盘上,当数据集比较大的时候,fork的过程是非常耗时的,可能会导致Redis在一些毫秒级内不能响应客户端的请求. 如果数据集巨大并且CPU性能不是很好的情况下,这种情况会持续1秒,AOF也需要fork,但是你可以调节重写日志文件的频率来提高数据集的耐久度. 三、AOF的优缺点1、AOF 优点使用AOF 会让你的Redis更加耐久: 你可以使用不同的fsync策略：无fsync,每秒fsync,每次写的时候fsync.使用默认的每秒fsync策略. Redis的性能依然很好(fsync是由后台线程进行处理的,主线程会尽力处理客户端请求),一旦出现故障，你最多丢失1秒的数据.AOF文件是一个只进行追加的日志文件,所以不需要写入seek,即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令,你也也可使用redis-check-aof工具修复这些问题. Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的. 因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。 2、AOF 缺点对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。 三、如何选择使用哪种持久化方式？一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。 有很多用户都只使用 AOF 持久化， 但我们并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快， 除此之外， 使用 RDB 还可以避免之前提到的 AOF 程序的 bug 。Note: 因为以上提到的种种原因， 未来我们可能会将 AOF 和 RDB 整合成单个持久化模型。 （这是一个长期计划。） 接下来的几个小节将介绍 RDB 和 AOF 的更多细节。 快照在默认情况下， Redis 将数据库快照保存在名字为 dump.rdb的二进制文件中。你可以对 Redis 进行设置， 让它在“ N 秒内数据集至少有 M 个改动”这一条件被满足时， 自动保存一次数据集。你也可以通过调用 SAVE或者 BGSAVE ， 手动让 Redis 进行数据集保存操作。比如说， 以下设置会让 Redis 在满足“ 60 秒内有至少有 1000 个键被改动”这一条件时， 自动保存一次数据集:save 60 1000这种持久化方式被称为快照 snapshotting 四、工作方式当 Redis 需要保存 dump.rdb 文件时， 服务器执行以下操作:Redis 调用forks. 同时拥有父进程和子进程。子进程将数据集写入到一个临时 RDB 文件中。当子进程完成对新 RDB 文件的写入时，Redis 用新 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。 这种工作方式使得 Redis 可以从写时复制（copy-on-write）机制中获益。只追加操作的文件（Append-only file，AOF）快照功能并不是非常耐久（dura ble）： 如果 Redis 因为某些原因而造成故障停机， 那么服务器将丢失最近写入、且仍未保存到快照中的那些数据。 从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化。你可以在配置文件中打开AOF方式:appendonly yes 从现在开始， 每当 Redis 执行一个改变数据集的命令时（比如 SET）， 这个命令就会被追加到 AOF 文件的末尾。这样的话， 当 Redis 重新启时， 程序就可以通过重新执行 AOF 文件中的命令来达到重建数据集的目的。 五、日志重写因为 AOF 的运作方式是不断地将命令追加到文件的末尾， 所以随着写入命令的不断增加， AOF 文件的体积也会变得越来越大。举个例子， 如果你对一个计数器调用了 100 次 INCR ， 那么仅仅是为了保存这个计数器的当前值， AOF 文件就需要使用 100 条记录（entry）。然而在实际上， 只使用一条 SET 命令已经足以保存计数器的当前值了， 其余 99 条记录实际上都是多余的。 为了处理这种情况， Redis 支持一种有趣的特性： 可以在不打断服务客户端的情况下， 对 AOF 文件进行重建（rebuild）。执行 BGREWRITEAOF 命令， Redis 将生成一个新的 AOF 文件， 这个文件包含重建当前数据集所需的最少命令。Redis 2.2 需要自己手动执行 BGREWRITEAOF 命令； Redis 2.4 则可以自动触发 AOF 重写， 具体信息请查看 2.4 的示例配置文件。 六、AOF有多耐用?你可以配置 Redis 多久才将数据 fsync 到磁盘一次。有三种方式：每次有新命令追加到 AOF 文件时就执行一次 fsync ：非常慢，也非常安全每秒 fsync 一次：足够快（和使用 RDB 持久化差不多），并且在故障时只会丢失 1 秒钟的数据。从不 fsync ：将数据交给操作系统来处理。更快，也更不安全的选择。推荐（并且也是默认）的措施为每秒 fsync 一次， 这种 fsync 策略可以兼顾速度和安全性。 七、如果AOF文件损坏了怎么办？服务器可能在程序正在对 AOF 文件进行写入时停机， 如果停机造成了 AOF 文件出错（corrupt）， 那么 Redis 在重启时会拒绝载入这个 AOF 文件， 从而确保数据的一致性不会被破坏。当发生这种情况时， 可以用以下方法来修复出错的 AOF 文件：为现有的 AOF 文件创建一个备份。 使用 Redis 附带的 redis-check-aof 程序，对原来的 AOF 文件进行修复: $ redis-check-aof –fix（可选）使用 diff -u 对比修复后的 AOF 文件和原始 AOF 文件的备份，查看两个文件之间的不同之处。重启 Redis 服务器，等待服务器载入修复后的 AOF 文件，并进行数据恢复。 工作原理AOF 重写和 RDB 创建快照一样，都巧妙地利用了写时复制机制:Redis 执行 fork() ，现在同时拥有父进程和子进程。子进程开始将新 AOF 文件的内容写入到临时文件。对于所有新执行的写入命令，父进程一边将它们累积到一个内存缓存中，一边将这些改动追加到现有 AOF 文件的末尾,这样样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。当子进程完成重写工作时，它给父进程发送一个信号，父进程在接收到信号之后，将内存缓存中的所有数据追加到新 AOF 文件的末尾。搞定！现在 Redis 原子地用新文件替换旧文件，之后所有命令都会直接追加到新 AOF 文件的末尾。 八、怎样从RDB方式切换为AOF方式在 Redis 2.2 或以上版本，可以在不重启的情况下，从 RDB 切换到 AOF ：为最新的 dump.rdb 文件创建一个备份。将备份放到一个安全的地方。执行以下两条命令:12redis-cli config set appendonly yesredis-cli config set save “” 确保写命令会被正确地追加到 AOF 文件的末尾。执行的第一条命令开启了 AOF 功能： Redis 会阻塞直到初始 AOF 文件创建完成为止， 之后 Redis 会继续处理命令请求， 并开始将写入命令追加到 AOF 文件末尾。执行的第二条命令用于关闭 RDB 功能。 这一步是可选的， 如果你愿意的话， 也可以同时使用 RDB 和 AOF 这两种持久化功能。 重要:别忘了在 redis.conf 中打开 AOF 功能！ 否则的话， 服务器重启之后， 之前通过 CONFIG SET 设置的配置就会被遗忘， 程序会按原来的配置来启动服务器。 九、AOF和RDB之间的相互作用在版本号大于等于 2.4 的 Redis 中， BGSAVE 执行的过程中， 不可以执行 BGREWRITEAOF 。 反过来说， 在 BGREWRITEAOF 执行的过程中， 也不可以执行 BGSAVE。这可以防止两个 Redis 后台进程同时对磁盘进行大量的 I/O 操作。 如果 BGSAVE 正在执行， 并且用户显示地调用 BGREWRITEAOF 命令， 那么服务器将向用户回复一个 OK 状态， 并告知用户， BGREWRITEAOF 已经被预定执行： 一旦 BGSAVE 执行完毕， BGREWRITEAOF 就会正式开始。 当 Redis 启动时， 如果 RDB 持久化和 AOF 持久化都被打开了， 那么程序会优先使用 AOF 文件来恢复数据集， 因为 AOF 文件所保存的数据通常是最完整的。 十、备份redis数据在阅读这个小节前， 请牢记下面这句话: 确保你的数据由完整的备份. 磁盘故障， 节点失效， 诸如此类的问题都可能让你的数据消失不见， 不进行备份是非常危险的。 Redis 对于数据备份是非常友好的， 因为你可以在服务器运行的时候对 RDB 文件进行复制： RDB 文件一旦被创建， 就不会进行任何修改。 当服务器要创建一个新的 RDB 文件时， 它先将文件的内容保存在一个临时文件里面， 当临时文件写入完毕时， 程序才使用 rename(2) 原子地用临时文件替换原来的 RDB 文件。这也就是说， 无论何时， 复制 RDB 文件都是绝对安全的。 创建一个定期任务（cron job）， 每小时将一个 RDB 文件备份到一个文件夹， 并且每天将一个 RDB 文件备份到另一个文件夹。 确保快照的备份都带有相应的日期和时间信息， 每次执行定期任务脚本时， 使用 find 命令来删除过期的快照： 比如说， 你可以保留最近 48 小时内的每小时快照， 还可以保留最近一两个月的每日快照。 至少每天一次， 将 RDB 备份到你的数据中心之外， 或者至少是备份到你运行 Redis 服务器的物理机器之外。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JedisHelper——Redis客户端简单封装]]></title>
    <url>%2F2017%2F11%2F30%2FJedisHelper%E2%80%94%E2%80%94Redis%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%AE%80%E5%8D%95%E5%B0%81%E8%A3%85%2F</url>
    <content type="text"><![CDATA[由于公司在不同的业务系统场景都有用到Redis，为了减少业务之间带来的相互影响，所以部署了多个Redis集群，JedisHelper就提供了获取不同业务集群Redis实例的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165package com.ylp.utils;import com.ylp.common.tools.utils.PropertiesUtil;import org.apache.commons.lang3.StringUtils;import org.apache.log4j.Logger;import redis.clients.jedis.HostAndPort;import redis.clients.jedis.JedisCluster;import redis.clients.jedis.JedisPoolConfig;import java.util.*;import java.util.concurrent.ConcurrentHashMap;/** * @author: 会跳舞的机器人 * @date: 16/8/23 下午3:49 * @description: Jedis客户端 */public class JedisHelper &#123; private static int JEDISPOOL_MAXTOTAL = 300; private static int JEDISPOOL_MAXAIDLE = 60; private static int TIMEOUT = 5000; private static Logger logger = Logger.getLogger(JedisHelper.class); /** * 各个Redis集群的IP端口信息集合 */ private static Map&lt;String, Set&lt;HostAndPort&gt;&gt; map = new HashMap&lt;String, Set&lt;HostAndPort&gt;&gt;(); /** * 其他Redis实例集合,用于后续的扩展 */ private static Map&lt;String, JedisCluster&gt; jedisClusterMap = new ConcurrentHashMap&lt;String, JedisCluster&gt;(); /** * Redis核心业务集群实例 */ private static JedisCluster coreJedis = null; /** * Redis数据集群实例 */ private static JedisCluster dataJedis = null; /** * 禁止实例化 */ private JedisHelper() &#123; &#125; /** * Redis集群配置信息初始化... */ static &#123; // step1:先获取全部的Redis集群配置 List&lt;String&gt; list = PropertiesUtil.getKeySet(&quot;redis.cluster&quot;); for (String str : list) &#123; Set&lt;HostAndPort&gt; nodes = getHostAndPortSetByClusterName(str); map.put(str, nodes); &#125; // step2:分别获取核心集群与数据集群 String coreCluseterValue = PropertiesUtil.getProperty(&quot;redis.coreCluster&quot;); String dataClusterValue = PropertiesUtil.getProperty(&quot;redis.dataCluster&quot;); map.put(&quot;redis.coreCluster&quot;, map.get(coreCluseterValue)); map.put(&quot;redis.dataCluster&quot;, map.get(dataClusterValue)); // step3:初始化核心业务集群实例 Set&lt;HostAndPort&gt; coreNodes = map.get(&quot;redis.coreCluster&quot;); coreJedis = getJedisClusterByNodes(coreNodes); // step4:初始化Redis数据集群实例 Set&lt;HostAndPort&gt; dataNodes = map.get(&quot;redis.dataCluster&quot;); dataJedis = getJedisClusterByNodes(dataNodes); &#125; /** * 获取Redis核心业务集群客户端 * * @return */ public static JedisCluster coreCluster() &#123; return coreJedis; &#125; /** * 获取Redis数据集群客户端 * * @return */ public static JedisCluster dataCluster() &#123; return dataJedis; &#125; /** * 根据指定的clusterName获取JedisCluster实例 * * @param clusterName * @return */ public synchronized static JedisCluster getCluster(String clusterName) &#123; logger.info(&quot;clusterName:&quot; + clusterName); if (StringUtils.isEmpty(clusterName)) &#123; throw new IllegalArgumentException(&quot;clusterName不能为空&quot;); &#125; Set&lt;HostAndPort&gt; hostAndPortSet = map.get(clusterName); if (hostAndPortSet == null) &#123; throw new IllegalArgumentException(&quot;clusterName对应的集群配置信息不存在&quot;); &#125; // 如果已经实例化,则直接返回 if (jedisClusterMap.get(clusterName) != null) &#123; return jedisClusterMap.get(clusterName); &#125; // 根据集群名获取集群IP端口信息 Set&lt;HostAndPort&gt; nodes = map.get(clusterName); JedisCluster jedisCluster = getJedisClusterByNodes(nodes); // 实例化后放入jedisClusterMap jedisClusterMap.put(clusterName, jedisCluster); return jedisCluster; &#125; /** * 根据集群节点nodes以及默认的配置信息获取JedisCluster * * @param nodes * @return */ private static JedisCluster getJedisClusterByNodes(Set&lt;HostAndPort&gt; nodes) &#123; JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(JEDISPOOL_MAXTOTAL); config.setMaxIdle(JEDISPOOL_MAXAIDLE); // 根据集群名获取集群IP端口信息 JedisCluster jedisCluster = new JedisCluster(nodes, TIMEOUT, 100, config); return jedisCluster; &#125; /** * 根据配置文件中的配置信息,获取Set&lt;HostAndPort&gt; * * @param clusterName * @return */ private static Set&lt;HostAndPort&gt; getHostAndPortSetByClusterName(String clusterName) &#123; Set&lt;HostAndPort&gt; nodes = new HashSet&lt;HostAndPort&gt;(); // 获取配置信息 String result = PropertiesUtil.getProperty(clusterName); if (StringUtils.isEmpty(result)) &#123; return nodes; &#125; // 解析 String[] arry = result.split(&quot; &quot;); String host; String port; for (String str : arry) &#123; host = str.substring(0, str.indexOf(&quot;:&quot;)); port = str.substring(str.indexOf(&quot;:&quot;) + 1); HostAndPort hostAndPort = new HostAndPort(host, Integer.valueOf(port)); nodes.add(hostAndPort); &#125; return nodes; &#125;&#125; 配置信息格式：123456# Redis核心业务集群redis.coreCluster=redis.cluster1# Redis数据集群redis.dataCluster=redis.cluster2redis.cluster1=172.16.8.161:7001 172.16.8.162:7002 172.16.8.163:7003 172.16.8.164:7004 172.16.8.165:7005 172.16.8.166:7006redis.cluster2=172.16.8.161:7001 172.16.8.162:7002 172.16.8.163:7003 172.16.8.164:7004 172.16.8.165:7005 172.16.8.166:7006]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis集群的安装]]></title>
    <url>%2F2017%2F11%2F30%2FRedis%E9%9B%86%E7%BE%A4%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[一、基本介绍Redis集群至少需要3个主节点，要保证Redis的高可用性，那每个主节点至少需要一个从节点（如果没有从节点，那集群中的某个主节点挂掉了，那这个节点中的数据也就获取不到了），所以Redis集群就至少需要6个节点，3个主节点，3个从节点。 Redis集群的数据共享 Redis 集群使用数据分片（sharding）而非一致性哈希（consistency hashing）来实现： 一个 Redis 集群包含 16384 个哈希槽（hash slot）， 数据库中的每个键都属于这 16384 个哈希槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。 集群中的每个节点负责处理一部分哈希槽。 举个例子， 一个集群可以有三个哈希槽， 其中： 节点 A 负责处理 0 号至 5500 号哈希槽。 节点 B 负责处理 5501 号至 11000 号哈希槽。 节点 C 负责处理 11001 号至 16384 号哈希槽。 这种将哈希槽分布到不同节点的做法使得用户可以很容易地向集群中添加或者删除节点。 比如说：如果用户将新节点 D 添加到集群中， 那么集群只需要将节点 A 、B 、 C 中的某些槽移动到节点 D 就可以了。 与此类似， 如果用户要从集群中移除节点 A ， 那么集群只需要将节点 A 中的所有哈希槽移动到节点 B 和节点 C ， 然后再移除空白（不包含任何哈希槽）的节点 A 就可以了。因为将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞， 所以无论是添加新节点还是移除已存在节点， 又或者改变某个节点包含的哈希槽数量， 都不会造成集群下线。所以每次往Redis中set值的时候，会根据Redis的计算规则算出这个这个值的哈希值，然后再把数据放到对应处理这个哈希值的节点上，获取数据的时候也是一样的道理。 二、安装1、服务器规划注意：端口的规划请事先确认好是否已经被占用，建议各个服务器对应端口的数值带有一定的规律性，便于安装和维护 2、在对应的服务器中打开对应的端口切换至root用户1# vi /etc/sysconfig/iptables 192.168.31.143中增加：123## redis -A INPUT -m state --state NEW -m tcp -p tcp --dport 7111 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 17111 -j ACCEPT 192.168.31.103中增加：123## redis -A INPUT -m state --state NEW -m tcp -p tcp --dport 7112 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 17112 -j ACCEPT 192.168.31.154中增加：123## redis -A INPUT -m state --state NEW -m tcp -p tcp --dport 7113 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 17113 -j ACCEPT 192.168.31.117中增加：123## redis -A INPUT -m state --state NEW -m tcp -p tcp --dport 7114 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 17114 -j ACCEPT 192.168.31.146中增加：## redis -A INPUT -m state --state NEW -m tcp -p tcp --dport 7115 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 17115 -j ACCEPT192.168.31.173中增加：123## redis -A INPUT -m state --state NEW -m tcp -p tcp --dport 7116 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 17116 -j ACCEPT 在各个服务器增加完端口后，进行端口配置重启1# service iptables restart 下面对Redis进行安装，安装目录：/usr/local/redis3，安装用户：root 3、编译和安装所需要的包# yum install gcc tcl 4、下载或者上传Redis3的最新稳定版本到/usr/local/src12# cd /usr/local/src# wget http://download.redis.io/releases/redis-3.0.3.tar.gz 5、创建安装目录：1# mkdir /usr/local/redis3 6、解压并进入目录12# tar -zxvf redis-3.0.3.tar.gz# cd redis-3.0.3 7、编译安装（使用PREFIX指定安装目录）1# make PREFIX=/usr/local/redis3 install 安装完成后，可以看到/usr/local/redis3目录下有一个bin目录，bin目录里面就是redis的命令脚本，如下：1redis-benchmark redis-check-aof redis-check-dump redis-cli redis-sentinel redis-server 以上步骤在每台机器上都要执行安装 ####8、创建集群配置目录，并拷贝redis.conf配置文件到各个节点配置目录 192.168.31.14312# mkdir -p /usr/local/redis3/cluster/7111# cp /usr/local/src/redis3.0/redis.conf /usr/local/redis3/cluster/7111/redis-7111.conf 192.168.31.10312# mkdir -p /usr/local/redis3/cluster/7112# cp /usr/local/src/redis-3.0.3/redis.conf /usr/local/redis3/cluster/7112/redis-7112.conf 192.168.31.15412# mkdir -p /usr/local/redis3/cluster/7113# cp /usr/local/src/redis-3.0.3/redis.conf /usr/local/redis3/cluster/7113/redis-7113.conf 192.168.31.11712# mkdir -p /usr/local/redis3/cluster/7114# cp /usr/local/src/redis-3.0.3/redis.conf /usr/local/redis3/cluster/7114/redis-7114.conf 192.168.31.14612# mkdir -p /usr/local/redis3/cluster/7115# cp /usr/local/src/redis-3.0.3/redis.conf /usr/local/redis3/cluster/7115/redis-7115.conf 192.168.31.17312# mkdir -p /usr/local/redis3/cluster/7116# cp /usr/local/src/redis-3.0.3/redis.conf /usr/local/redis3/cluster/7116/redis-7116.conf 9、修改6个节点的redis.conf配置文件内容使用以下命令逐个启动这6个Redis节点实例 192.168.31.1431# /usr/local/redis3/bin/redis-server /usr/local/redis3/cluster/7111/redis-7111.conf 192.168.31.1031# /usr/local/redis3/bin/redis-server /usr/local/redis3/cluster/7112/redis-7112.conf 192.168.31.1541# /usr/local/redis3/bin/redis-server /usr/local/redis3/cluster/7113/redis-7113.conf 192.168.31.1171# /usr/local/redis3/bin/redis-server /usr/local/redis3/cluster/7114/redis-7114.conf 192.168.31.1461# /usr/local/redis3/bin/redis-server /usr/local/redis3/cluster/7115/redis-7115.conf 192.168.31.1731# /usr/local/redis3/bin/redis-server /usr/local/redis3/cluster/7116/redis-7116.conf 逐个启动后，可以用ps -ef|grep redis命令来检测是否启动成功1root 3273 1 3 15:18 ? 00:00:00 /usr/local/redis3/bin/redis-server *:7111 [cluster] 接下来准备创建集群 ####10、安装ruby和rubygems（注意：需要ruby的版本在1.8.7及以上）1# yum install ruby rubygems 查看ruby的版本信息12# ruby -vruby 1.8.7 (2013-06-27 patchlevel 374) [x86_64-linux] 11、gem安装redis ruby接口1# gem install redis 出现如下信息：1234Successfully installed redis-3.3.11 gem installedInstalling ri documentation for redis-3.3.1...Installing RDoc documentation for redis-3.3.1... 12、执行Redis集群创建命令（只需要在其中一个节点上执行即可）拷贝集群创建文件：1# cp /usr/local/src/redis-3.0.3/src/redis-trib.rb /usr/local/bin/redis-trib 执行集群创建脚本：1# redis-trib create --replicas 1 192.168.31.117:7114 192.168.31.146:7115 192.168.31.173:7116 192.168.31.143:7111 192.168.31.103:7112 192.168.31.154:7113 集群创建过程说明：给定redis-trib程序的命令是create，这表示我们希望创建一个新的集群，–replicas 1表示每个主节点下面有一个从节点，后面的参数则是redis实例的地址列表，程序使用这些地址所指示的实例来创建新的集群。在第一次执行的时候，输出信息如下：12345678910111213141516171819202122232425262728&gt;&gt;&gt; Creating clusterConnecting to node 192.168.31.117:7114: OKConnecting to node 192.168.31.146:7115: OKConnecting to node 192.168.31.173:7116: OKConnecting to node 192.168.31.143:7111: OKConnecting to node 192.168.31.103:7112: OKConnecting to node 192.168.31.154:7113: OK&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:192.168.31.154:7113192.168.31.173:7116192.168.31.143:7111Adding replica 192.168.31.146:7115 to 192.168.31.154:7113Adding replica 192.168.31.117:7114 to 192.168.31.173:7116Adding replica 192.168.31.103:7112 to 192.168.31.143:7111S: 26a53ca5649a85d75d7f78c17897846fad8548c3 192.168.31.117:7114 replicates 3dc26de1ddb9304dc3451e7044c91b3fd5cb77b9S: 51dfb182af4fdc33c69218fe6f8421c0311f67f0 192.168.31.146:7115 replicates e9b5cd667523705b7f4052dd847a45c9abd4ff2eM: 3dc26de1ddb9304dc3451e7044c91b3fd5cb77b9 192.168.31.173:7116 slots:5461-10922 (5462 slots) masterM: 45d00776095446fcf7e194a53e50311da4e2c87e 192.168.31.143:7111 slots:10923-16383 (5461 slots) masterS: aa1577c0295783245870e114ffdcabd0ee9bfd07 192.168.31.103:7112 replicates 45d00776095446fcf7e194a53e50311da4e2c87eM: e9b5cd667523705b7f4052dd847a45c9abd4ff2e 192.168.31.154:7113 slots:0-5460 (5461 slots) masterCan I set the above configuration? (type &apos;yes&apos; to accept): 从以上信息可以看到master节点为7111、7113、7116，这和我们预先计划的不一致，预先计划的master节点是7111、7112、7113，所以在确认这一步，我们输入no，然后再执行上面的脚本，在试过几次后终于得到了我们预想的设置（好像是随机的，没有规律，这一步要注意核对主从关系是否是你所预想的）12345678910111213141516171819202122232425262728&gt;&gt;&gt; Creating clusterConnecting to node 192.168.31.117:7114: OKConnecting to node 192.168.31.146:7115: OKConnecting to node 192.168.31.173:7116: OKConnecting to node 192.168.31.143:7111: OKConnecting to node 192.168.31.103:7112: OKConnecting to node 192.168.31.154:7113: OK&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:192.168.31.103:7112192.168.31.154:7113192.168.31.143:7111Adding replica 192.168.31.146:7115 to 192.168.31.103:7112Adding replica 192.168.31.173:7116 to 192.168.31.154:7113Adding replica 192.168.31.117:7114 to 192.168.31.143:7111S: 26a53ca5649a85d75d7f78c17897846fad8548c3 192.168.31.117:7114 replicates 45d00776095446fcf7e194a53e50311da4e2c87eS: 51dfb182af4fdc33c69218fe6f8421c0311f67f0 192.168.31.146:7115 replicates aa1577c0295783245870e114ffdcabd0ee9bfd07S: 3dc26de1ddb9304dc3451e7044c91b3fd5cb77b9 192.168.31.173:7116 replicates e9b5cd667523705b7f4052dd847a45c9abd4ff2eM: 45d00776095446fcf7e194a53e50311da4e2c87e 192.168.31.143:7111 slots:10923-16383 (5461 slots) masterM: aa1577c0295783245870e114ffdcabd0ee9bfd07 192.168.31.103:7112 slots:0-5460 (5461 slots) masterM: e9b5cd667523705b7f4052dd847a45c9abd4ff2e 192.168.31.154:7113 slots:5461-10922 (5462 slots) masterCan I set the above configuration? (type &apos;yes&apos; to accept): 输入yes，集群就会将配置应用到各个节点，并join各个节点，让各个节点开始通讯，输出如下信息123456789101112131415161718192021222324&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join.....&gt;&gt;&gt; Performing Cluster Check (using node 192.168.31.117:7114)M: 26a53ca5649a85d75d7f78c17897846fad8548c3 192.168.31.117:7114 slots: (0 slots) master replicates 45d00776095446fcf7e194a53e50311da4e2c87eM: 51dfb182af4fdc33c69218fe6f8421c0311f67f0 192.168.31.146:7115 slots: (0 slots) master replicates aa1577c0295783245870e114ffdcabd0ee9bfd07M: 3dc26de1ddb9304dc3451e7044c91b3fd5cb77b9 192.168.31.173:7116 slots: (0 slots) master replicates e9b5cd667523705b7f4052dd847a45c9abd4ff2eM: 45d00776095446fcf7e194a53e50311da4e2c87e 192.168.31.143:7111 slots:10923-16383 (5461 slots) masterM: aa1577c0295783245870e114ffdcabd0ee9bfd07 192.168.31.103:7112 slots:0-5460 (5461 slots) masterM: e9b5cd667523705b7f4052dd847a45c9abd4ff2e 192.168.31.154:7113 slots:5461-10922 (5462 slots) master[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 可以看出： 192.168.31.117:7114从节点所属的主节点为192.168.31.143:7111 192.168.31.146:7115从节点所属的主节点为192.168.31.103:7112 192.168.31.173:7116从节点所属的主节点为192.168.31.154:7113（通过replicates 后面的值去匹配，相当于外键ID） 192.168.31.103:7112分配的槽点是是0-5460段 192.168.31.154:7113分配的槽点是5461-10922段 192.168.31.143:7111分配的槽点是10923-16383段 集群中的每个节点负责处理一部分哈希槽，最后一行信息表示集群中的16384个槽都有至少一个主节点在处理，集群运行正常 13、查看redis集群的从属状态关系1# /usr/local/redis3/bin/redis-cli -p 7111 cluster nodes 输出：123456789101126a53ca5649a85d75d7f78c17897846fad8548c3 192.168.31.117:7114 slave 45d00776095446fcf7e194a53e50311da4e2c87e 0 1471133132198 4 connected3dc26de1ddb9304dc3451e7044c91b3fd5cb77b9 192.168.31.173:7116 slave e9b5cd667523705b7f4052dd847a45c9abd4ff2e 0 1471133130178 6 connectedaa1577c0295783245870e114ffdcabd0ee9bfd07 192.168.31.103:7112 master - 0 1471133133207 5 connected 0-546051dfb182af4fdc33c69218fe6f8421c0311f67f0 192.168.31.146:7115 slave aa1577c0295783245870e114ffdcabd0ee9bfd07 0 1471133134217 5 connected45d00776095446fcf7e194a53e50311da4e2c87e 192.168.31.143:7111 myself,master - 0 0 4 connected 10923-16383e9b5cd667523705b7f4052dd847a45c9abd4ff2e 192.168.31.154:7113 master - 0 1471133131189 6 connected 5461-10922 14、集群的简单测试随便挑一个节点，启动redis终端1234567# /usr/local/redis3/bin/redis-cli -c -p 7114127.0.0.1:7114&gt; set key1 dreyer-&gt; Redirected to slot [9189] located at 192.168.31.154:7113OK192.168.31.154:7113&gt; get key1&quot;dreyer&quot;192.168.31.154:7113&gt; 可以看到set的key重定向192.168.31.154:711这个节点，因为redis集群会有一个使用公式CRC16(key) % 16384来计算key属于哪个槽 我们切换到192.168.31.154:7113这个节点上来获取刚刚设置key的值，因为设置的key值就落在这个节点上，所以能直接获取到数据/usr/local/redis3/bin/redis-cli -c -p 711312127.0.0.1:7113&gt; get key1&quot;dreyer&quot; 我们再切换到192.168.31.143:7111这个节点上来获取刚刚设置的key值1234# /usr/local/redis3/bin/redis-cli -c -p 7111`127.0.0.1:7111&gt; get key1-&gt; Redirected to slot [9189] located at 192.168.31.154:7113&quot;dreyer&quot; 可以看到这7111这个节点获取数据的时候，会重定向至192.168.31.154:7113节点上获取数据 至此，完成！]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ简介以及应用]]></title>
    <url>%2F2017%2F11%2F30%2FRabbitMQ%E7%AE%80%E4%BB%8B%E4%BB%A5%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、简要介绍 开源AMQP实现，Erlang语言编写，支持多种客户端 分布式、高可用、持久化、可靠、安全 支持多种协议：AMQP、STOMP、MQTT、HTTP 适用于多系统之间的业务解耦的消息中间件 二、基本概念1、exchange：交换器，负责接收消息，转发消息至绑定的队列，有四种类型： direct：完全匹配的路由 topic：模式匹配的路由 fanout：广播模式 headers：键值对匹配路由 Exchange属性： 持久化：如果启用，那么rabbit服务重启之后仍然存在 自动删除：如果启用，那么交换器将会在其绑定的队列都被删除掉之后自动删除掉自身 2、Queue：队列，rabbitmq的内部对象，用于存储消息，其属性类似于Exchange，同样可以设置是否持久化、自动删除等。消费者重Queue中获取消息并消费。多个消费者可以订阅同一个Queue，这时Queue中的消息会被平均分摊给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理。 3、Binding：绑定，根据路由规则绑定交换器与队列 4、Routing：路由键，路由的关键字 三、消息的可靠性 Message acknowledgment：消息确认，在消息确认机制下，收到回执才会删除消息，未收到回执而断开了连接，消息会转发给其他消费者，如果忘记回执，会导致消息堆积，消费者重启后会重复消费这些消息并重复执行业务逻辑。 Message durability：消息持久化，设置消息持久化可以避免绝大部分消息丢失，比如rabbitmq服务重启，但是采用非持久化可以提升队列的处理效率。如果要确保消息的持久化，那么消息对应的Exchange和Queue同样要设置为持久化。 Prefetch count，每次发送给消费者消息的数量，默认为1 另外，如果需要可靠性业务，需要设置持久化和ack机制，如果系统高吞吐，可以设置为非持久化、noack、自动删除机制 四、简单应用模拟这样一个业务场景，用户下单成功后，需要给用户增加积分，同时还需要给用户发送下单成功的消息，这是在电商业务中很常见的一个业务场景。 如果系统是微服务架构，可能用户下单功能在订单服务，给用户增加积分的功能在积分服务，给用户发送通知消息的功能在通知服务，各个服务之间解耦，互不影响。那么要实现上述的业务场景，消息中间件rabbitmq是一个很好的选择。原因如下： 高性能，它的实现语言是天生具备高并发高可用的erlang 语言 支持消息的持久化，即使服务器挂了，也不会丢失消息 消息应答（ack）机制，消费者消费完消息后发送一个消息应答，rabbitmq才会删除消息，确保消息的可靠性 支持高可用集群 灵活的路由 实现思路： 用户下单成功后，rabbitmq发送一条消息至EXCHANGE.ORDER_CREATE交换器，该交换器绑定了两个队列，QUEUE.ORDER_INCREASESCORE、QUEUE.ORDER_NOTIFY，消费者订阅这两个队列分别用来处理增加积分、发送用户通知。如果后续日志系统还需要记录下单的相关日志，那么我们只需要再定义一个队列并将其绑定到EXCHANGE.ORDER_CREATE即可。 pom.xml增加rabbitmq的jar包依赖12345&lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;3.6.5&lt;/version&gt;&lt;/dependency&gt; 下单发rabbitmq消息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.robot.rabbitmq;import com.rabbitmq.client.AMQP;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.UUID;import java.util.concurrent.TimeoutException;/** * @author: 会跳舞的机器人 * @date: 2017/10/13 10:46 * @description: 模拟用户下单之后发送rabbitmq消息 */public class OrderCreator &#123; // 交换器名称 private static final String EXCHANGE = &quot;EXCHANGE.ORDER_CREATE&quot;; // 消息内容 private static String msg = &quot;create order success&quot;; /** * 模拟创建订单后发送mq消息 */ public void createOrder() &#123; System.out.println(&quot;下单成功，开始发送rabbitmq消息&quot;); ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;192.168.12.44&quot;); connectionFactory.setPort(56720); connectionFactory.setUsername(&quot;baibei&quot;); connectionFactory.setPassword(&quot;baibei&quot;); Connection connection; Channel channel; try &#123; connection = connectionFactory.newConnection(); channel = connection.createChannel(); // 持久化 boolean durable = true; // topic类型 String type = &quot;topic&quot;; // 声明交换器，如果交换器不存在则创建之 channel.exchangeDeclare(EXCHANGE, type, durable); String messgeId = UUID.randomUUID().toString(); // deliveryMode&gt;=2表示设置消息持久化 AMQP.BasicProperties props = new AMQP.BasicProperties.Builder().deliveryMode(2).messageId(messgeId).build(); // 发布消息 String routingKey = &quot;order_create&quot;; channel.basicPublish(EXCHANGE, routingKey, props, msg.getBytes(&quot;utf-8&quot;)); connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (TimeoutException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 积分系统订阅消息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package com.robot.rabbitmq;import com.rabbitmq.client.AMQP;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import com.rabbitmq.client.Consumer;import com.rabbitmq.client.Envelope;import com.rabbitmq.client.ShutdownSignalException;import java.io.IOException;import java.util.concurrent.TimeoutException;/** * @author: 会跳舞的机器人 * @date: 2017/10/13 16:02 * @description: rabbitmq消费者，模拟下单成功后给用户增加积分 */public class IncreaseScoreConsumer implements Consumer &#123; private Connection connection; private Channel channel; // 交换器名称 private static final String EXCHANGE = &quot;EXCHANGE.ORDER_CREATE&quot;; // 增加积分队列名称 private static final String QUEUENAME = &quot;QUEUE.ORDER_INCREASESCORE&quot;; public void consume() &#123; // 初始化rabbitmq连接信息 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;192.168.12.44&quot;); connectionFactory.setPort(56720); connectionFactory.setUsername(&quot;baibei&quot;); connectionFactory.setPassword(&quot;baibei&quot;); try &#123; connection = connectionFactory.newConnection(); channel = connection.createChannel(); // 声明交换器 channel.exchangeDeclare(EXCHANGE, &quot;topic&quot;, true); // 声明队列 channel.queueDeclare(QUEUENAME, true, false, false, null); // 交换器与队列绑定并设置routingKey channel.queueBind(QUEUENAME, EXCHANGE, &quot;order_create&quot;); // 消费消息，callback是该类，关闭自动确认消息，在完成业务逻辑后手动确认确认 channel.basicConsume(QUEUENAME, false, this); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (TimeoutException e) &#123; e.printStackTrace(); &#125; &#125; public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; String msg = new String(body, &quot;UTF-8&quot;); System.out.println(&quot;《积分系统》收到订单消息：&quot; + msg + &quot;，给用户增加积分......&quot;); // 手动确认消息 channel.basicAck(envelope.getDeliveryTag(), false); /** * channel.basicReject(envelope.getDeliveryTag(), false);该方法会丢弃掉队列中的这条消息 * channel.basicReject(envelope.getDeliveryTag(), true);该方法会把消息重新放回队列 * 一般系统会设定一个重试次数，如果超过重试次数，则会丢弃消息，反之则会把消息再放入队列 */ &#125; public void handleConsumeOk(String consumerTag) &#123; &#125; public void handleCancelOk(String consumerTag) &#123; &#125; public void handleCancel(String consumerTag) throws IOException &#123; &#125; public void handleShutdownSignal(String consumerTag, ShutdownSignalException sig) &#123; &#125; public void handleRecoverOk(String consumerTag) &#123; &#125;&#125; 通知系统订阅消息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package com.robot.rabbitmq;import com.rabbitmq.client.AMQP;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import com.rabbitmq.client.Consumer;import com.rabbitmq.client.Envelope;import com.rabbitmq.client.ShutdownSignalException;import java.io.IOException;import java.util.concurrent.TimeoutException;/** * @author: 会跳舞的机器人 * @date: 2017/10/13 16:20 * @description: rabbitmq消费者，模拟下单成功后给用户发送通知 */public class NotifyConsumer implements Consumer &#123; private Connection connection; private Channel channel; // 交换器名称 private static final String EXCHANGE = &quot;EXCHANGE.ORDER_CREATE&quot;; // 通知用户下单成功通知队列名称 private static final String QUEUENAME = &quot;QUEUE.ORDER_NOTIFY&quot;; public void consume() &#123; // 初始化rabbitmq连接信息 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;192.168.12.44&quot;); connectionFactory.setPort(56720); connectionFactory.setUsername(&quot;baibei&quot;); connectionFactory.setPassword(&quot;baibei&quot;); try &#123; connection = connectionFactory.newConnection(); channel = connection.createChannel(); // 声明交换器 channel.exchangeDeclare(EXCHANGE, &quot;topic&quot;, true); // 声明队列 channel.queueDeclare(QUEUENAME, true, false, false, null); // 交换器与队列绑定并设置routingKey channel.queueBind(QUEUENAME, EXCHANGE, &quot;order_create&quot;); // 消费消息，callback是该类，关闭自动确认消息，在完成业务逻辑后手动确认确认 channel.basicConsume(QUEUENAME, false, this); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (TimeoutException e) &#123; e.printStackTrace(); &#125; &#125; public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; String msg = new String(body, &quot;UTF-8&quot;); System.out.println(&quot;《通知系统》收到订单消息：&quot; + msg + &quot;，开始给用户发送通知......&quot;); // 手动确认消息 channel.basicAck(envelope.getDeliveryTag(), false); /** * channel.basicReject(envelope.getDeliveryTag(), false);该方法会丢弃掉队列中的这条消息 * channel.basicReject(envelope.getDeliveryTag(), true);该方法会把消息重新放回队列 * 一般系统会设定一个重试次数，如果超过重试次数，则会丢弃消息，反之则会把消息再放入队列 */ &#125; public void handleConsumeOk(String consumerTag) &#123; &#125; public void handleCancelOk(String consumerTag) &#123; &#125; public void handleCancel(String consumerTag) throws IOException &#123; &#125; public void handleShutdownSignal(String consumerTag, ShutdownSignalException sig) &#123; &#125; public void handleRecoverOk(String consumerTag) &#123; &#125;&#125; 测试12345678910111213141516171819202122package com.robot.rabbitmq;/** * @author: 会跳舞的机器人 * @date: 2017/10/13 16:27 * @description: */public class Test &#123; public static void main(String[] args) &#123; IncreaseScoreConsumer increaseScoreConsumer = new IncreaseScoreConsumer(); increaseScoreConsumer.consume(); NotifyConsumer notifyConsumer = new NotifyConsumer(); notifyConsumer.consume(); OrderCreator orderCreator = new OrderCreator(); for (int i = 0; i &lt; 3; i++) &#123; orderCreator.createOrder(); &#125; &#125;&#125; 输出：123456789下单成功，开始发送rabbitmq消息《积分系统》收到订单消息：create order success，给用户增加积分......《通知系统》收到订单消息：create order success，开始给用户发送通知......下单成功，开始发送rabbitmq消息《积分系统》收到订单消息：create order success，给用户增加积分......《通知系统》收到订单消息：create order success，开始给用户发送通知......下单成功，开始发送rabbitmq消息《积分系统》收到订单消息：create order success，给用户增加积分......《通知系统》收到订单消息：create order success，开始给用户发送通知......]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程的艺术第十章——Executor框架]]></title>
    <url>%2F2017%2F11%2F29%2Fjava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E8%89%BA%E6%9C%AF%E7%AC%AC%E5%8D%81%E7%AB%A0%E2%80%94%E2%80%94Executor%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[1、Executor框架1.1、Executor框架的结构Executor主要由3大部分组成。 任务。包含被执行任务需要实现的接口：Runnable接口或Callable接口。 任务的执行。包括任务执行机制的核心接口Executor，以及继承自Executor接口的ExecutorService接口。Executor有两个关键类实现了ExecutorService接口（ThreadPoolExecutor和ScheduledThreadPoolExecutor） 异步计算的结果。包括接口Future和实现Future接口的FutureTask类。 下面是这些类和接口的简介。 Executor是一个接口，它是Executor框架的基础，它将任务的提交和任务的执行分离开来。 ThreadPoolExecutor是线程池的核心实现类，用来执行被提交的任务。 ScheduledThreadPoolExecutor是一个实现类，可以在给定的延迟后运行命令，或者定期执行命令。SchduledThreadPoolExecutor比Timer更灵活，功能更强大。 Future接口和实现Future接口的FutureTask类，代表异步计算的结果 Runnable接口和Callable接口的实现类，都可以被ThreadPoolExecutor或SchduledThreadPoolExecutor执行 1.2、Executor框架的成员Executor框架的主要成员：ThreadPoolExecutor、ScheduledThreadPoolExecutor、Future接口、Runnable接口、Callable接口、Executors。 1.2.1、ThreadPoolExecutorThreadPoolExecutor通常使用工厂类Executors来创建，Executors可以创建3种类型的ThreadPoolExecutor：SingleThreadExecutor、FixThreadPool、CachedThreadPool。 FixThreadPool：创建固定线程数的线程池，构造函数中可以指定线程数量，适用于为了满足资源管理的需求，而需要限制当前线程数量的应用场景，它适用于负载比较重的服务器。FixThreadPool内部使用无界队列LinkedBlockingQueue作为任务队列，队列的容量为Integer.MAX_VALUE，由于是无界队列，所以不会拒绝任务，可能会造成任务无限堆积，从而导致系统资源耗尽的情况。 SingleThreadExecutor：创建单个线程的线程池，可以保证顺序执行任务。与FixThreadPool类似，只是SingleThreadExecutor的线程数固定为1 CachedThreadPool：可以根据需要创建新的线程，CachedThreadPool是大小无界的线程池，适用于执行很多短期异步任务的小程序，或者是负载比较轻的服务器。CachedThreadPool的corePool为空，maximumPoolSize为Integer.MAX_VALUE，keepAliveTime为60L，这意味着线程空闲超过60秒则会进行回收。CachedThreadPool内部使用不存储元素的SynchronousQueue作为任务队列（一个put操作等待着一个take操作），这意味着如果任务的提交速度高于线程的处理速度，那么CachedThreadPool则会不断的创建新的线程，在极端的情况下，会耗尽CPU和内存资源。 1.2.2、SchduledThreadPoolExecutorSchduledThreadPoolExecutor通常使用工厂类Executors来创建，Executors可以创建2中类型的SchduledThreadPoolExecutor，如下： SchduledThreadPoolExecutor。包含若干个线程的SchduledThreadPoolExecutor。 SingleThreadSchduledExecutor。只包含一个线程的SchduledThreadPoolExecutor。 1.2.3、Future接口Future接口和实现Future接口的FutureTask类用来表示异步计算的结果，当我们把Runnable接口或者Callable接口的实现类提交（submit）给ThreadPoolExecutor或者SchduledThreadPoolExecutor时，ThreadPoolExecutor或者SchduledThreadPoolExecutor会向我们返回一个FutureTask对象。下面是对应的API 1.2.4、Runnable和Callable接口Runnable和Callable接口的实现类都可以被hreadPoolExecutor或者SchduledThreadPoolExecutor执行。它们之间的区别是Runnable不会返回结果，而Callable可以返回结果。 除了可以自已创建实现Callable接口的对象外，还可以使用工厂类Executors来把一个Runnable包装成一个Callable。 当我们把一个Callable对象提交给ThreadPoolExecutor或者SchduledThreadPoolExecutor执行时，summit()会向我们返回一个FutureTask对象。我们可以执行FutureTask.get()来等待任务执行完成。当任务完成后FutureTask.get()将会返回任务的结果。 附录：简单的结构脑图 2、ThreadPoolExecutor FixThreadPool：FixThreadPool内部使用无界队列LinkedBlockingQueue作为任务队列，队列的容量为Integer.MAX_VALUE，由于是无界队列，所以不会拒绝任务，可能会造成任务无限堆积，从而导致系统资源耗尽的情况，keepAliveTime为0L，意味着空闲的线程会被立马终止。 SingleThreadExecutor：与FixThreadPool类似，只是SingleThreadExecutor的线程数固定为1 CachedThreadPool：CachedThreadPool的corePool为空，maximumPoolSize为Integer.MAX_VALUE，keepAliveTime为60L，这意味着线程空闲超过60秒则会进行回收。CachedThreadPool内部使用不存储元素的SynchronousQueue作为任务队列（一个put操作等待着一个take操作），这意味着如果任务的提交速度高于线程的处理速度，那么CachedThreadPool则会不断的创建新的线程，在极端的情况下，会耗尽CPU和内存资源。 3、ScheduledThreadPoolExecutorScheduledThreadPoolExecutor主要用来执行需要延迟或者定时执行的任务，功能与Timer类似，不同的是Timer只能单线程允许，ScheduledThreadPoolExecutor可以指定多个线程。 ScheduledThreadPoolExecutor会把要执行的任务放在一个DelayQueue中，ScheduledThreadPoolExecutor会把Runable对象封装成ScheduledFutureTask，ScheduledFutureTask内部包含三个对象 time：任务要被执行的具体时间 sequenceNumber：任务排序编号，如果两个任务的time相同，那么则sequenceNumber较小的会先执行 period：任务执行的周期 ScheduledThreadPoolExecutor执行任务的流程大体是： 从DelayQueue中获取ScheduledFutureTask（time大于当前时间的任务） 执行ScheduledFutureTask 修改ScheduledFutureTask的time为下次要执行的时间 将ScheduledFutureTask再次放入DelayQueue中 简单代码示例：123456789101112131415161718192021222324package main.java.com.robot.demo;import java.util.Date;import java.util.concurrent.ScheduledThreadPoolExecutor;import java.util.concurrent.TimeUnit;/** * @author: 会跳舞的机器人 * @date: 2017/8/24 10:14 * @description: ScheduledThreadPoolExecutor */public class ScheduledThreadPoolExecutorDemo &#123; private static ScheduledThreadPoolExecutor scheduledThreadPoolExecutor = new ScheduledThreadPoolExecutor(4); public static void main(String[] args) &#123; // 初始化3秒后开始执行任务，此后每隔1秒执行一次 scheduledThreadPoolExecutor.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;正在执行任务，当前时间：&quot; + new Date()); &#125; &#125;, 3L, 1L, TimeUnit.SECONDS); &#125;&#125; 4、FutureTaskFuture接口和FutureTask实现代表线程异步执行的结果，可以通过FutureTask.get()方法获取异步返回的结果。 示例：123456789101112131415161718192021222324252627282930313233343536373839404142package main.java.com.robot.demo;import java.util.Date;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.FutureTask;/** * @author: 会跳舞的机器人 * @date: 2017/8/24 11:14 * @description: FutureTask示例 */public class FutureTaskDemo &#123; private static ExecutorService executorService = Executors.newSingleThreadExecutor(); public static void main(String[] args) &#123; FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; System.out.println(&quot;当前时间：&quot; + new Date() + &quot;执行任务......&quot;); Thread.sleep(3000); return &quot;success&quot;; &#125; &#125;); // 提交任务 executorService.submit(futureTask); // 获取返回结果 String result = &quot;&quot;; try &#123; result = futureTask.get(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;当前时间：&quot; + new Date() + &quot;，线程执行结果返回：&quot; + result); // 关闭线程池 executorService.shutdown(); &#125;&#125;]]></content>
      <categories>
        <category>java并发编程</category>
      </categories>
      <tags>
        <tag>java并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程的艺术第九章——java中的线程池]]></title>
    <url>%2F2017%2F11%2F29%2Fjava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E8%89%BA%E6%9C%AF%E7%AC%AC%E4%B9%9D%E7%AB%A0%E2%80%94%E2%80%94java%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[1、线程池的三个好处： 降低资源消耗。可以重复利用已创建的线程，降低创建/销毁线程的开销 提高响应速度。 提高线程的可管理性。统一分配、调优、监控。 2、线程池的处理流程新任务提交至线程池后的处理流程： 1.核心线程池是否已满，如果没满，则创建一个线程执行任务，如果满了，则进入下一个流程 2.判断队列是否已经满了，如果没满，则将任务存储在队列中，如果满了，则进入下一个流程 3.判断线程池是否已经满了，如果没满，则创建线程执行任务，如果满了，则按照策略处理无法执行的任务。 3、线程池的使用3.1、线程池的创建1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) corePoolSize：线程池的基本大小 maximumPoolSize：线程池的最大线程数 keepAliveTime：线程的活动的保持时间。当线程空闲之后，保持存活的时间，如果任务量大，而任务的执行时间又短的话，可以增大线程池的存活时间，提高线程的利用率 TimeUnit：线程活动保持时间的单位 BlockingQueue：用于存放任务的阻塞队列，队列的选择可以参考java中的阻塞队列 ThreadFactory：用于设置创建线程的工厂。 RejectedExecutionHandler：饱和策略。即当线程和队列都已经满了的时候，应该采取什么样的策略来处理新提交的任务，默认策略是AbortPolicy（抛出异常），其他的策略还有：CallerRunsPolicy（只用调用者所在线程来运行任务），DiscardOldestPolicy（丢弃队列里最近的一个任务，并执行当前任务），DiscardPolicy（不处理，丢弃掉） 3.2、向线程池提交任务提交任务有两种方法，分别是execute()和submit()。 execute()方法适用于任务提交之后没有返回值的这种情况，因为没有返回值，所以提交任务之后我们也无法判断任务是否执行成功。 submit方法适用于提交任务之后有返回值的情况，它会返回一个Future类型的对象，可以通过future.get()方法来获取返回值，get()会阻塞线程直到任务完成。 3.3、关闭线程池关闭线程池有两种方法，分别是shutdown()和shutdownNow()，它们的原理都是遍历线程池中所有的线程，分别调用每个线程池的interrupt()方法来中断线程。但是它们存在一定的区别，shutdownNow首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表，而shutdown只是将线程池的状态设置成SHUTDOWN状态，然后中断所有没有正在执行任务的线程。 3.4、合理的配置线程池线程池配置的分析角度 任务的性质：IO密集型还是CPU密集型，cup密集型可以配置小一点的线程数，io密集型可以配置多的线程数 任务的优先级：可以用PriorityBlockingQueue队列来处理 任务处理时间的长短 任务的依赖性：比如说是否依赖数据库连接 线程池的任务队列的选择建议使用有界队列，因为如果任务太多，有界队列可以抛出异常便于我们排查，而无界队列会使队列中的任务越来越多，可能导致撑满内存，导致整个系统的不可用。 3.5、线程池的监控]]></content>
      <categories>
        <category>java并发编程</category>
      </categories>
      <tags>
        <tag>java并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程的艺术笔记第八章——java中的并发工具类]]></title>
    <url>%2F2017%2F11%2F29%2Fjava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E8%89%BA%E6%9C%AF%E7%AC%94%E8%AE%B0%E7%AC%AC%E5%85%AB%E7%AB%A0%E2%80%94%E2%80%94java%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[在JDK的并发包里面提供了几个非常有用的并发工具，CountDownLatch、CyclicBarrier、Semaphore工具类提供了一种并发控制流程的手段，Exchanger工具类则提供了在线程间交换数据的一种手段。 1、等待多线程完成的CountDownLatchCountDownLatch是一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它运行一个或者多个线程一直处于等待状态。CountDownLatch中有两个关键的方法1public void countDown() &#123;&#125; 1public boolean await(long timeout, TimeUnit unit)&#123;&#125; CountDownLatch是一个计数器，在它的构造方法中需要指定一个值，用来设定计数的次数。每调用一次countDown()方法，数值便会减一，CountDownLatch会一直阻塞着调用await()方法的线程直到计数器的值变为0。设想有这样一个功能需要Thread1、Thread2、Thread3、Thread4四条线程分别统计C、D、E、F四个盘的大小，所有线程都统计完毕交给主线程去做汇总，利用CountDownLatch来完成就非常轻松。 在实际的项目开发中，有类似的应用场景用CountDownLatch来实现也非常简单实用。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.dreyer.javadoc.thread;import java.util.Date;import java.util.Random;import java.util.concurrent.*;/** * @description CountDownLatch * @author: 会跳舞的机器人 * @date: 16/5/14 下午11:41 */public class CountDownLatchDemo &#123; /** * */ private static CountDownLatch countDownLatch = new CountDownLatch(4); /** * 线程池 */ private static ExecutorService executor = Executors.newFixedThreadPool(4); /** * 开启的线程数 */ private static int THREAD_COUNT = 4; public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; THREAD_COUNT; i++) &#123; executor.execute(new Runnable() &#123; public void run() &#123; try &#123; // 模拟业务逻辑的耗时 int timer = new Random().nextInt(5); TimeUnit.SECONDS.sleep(timer); System.out.printf(&quot;%s时完成磁盘的统计任务,耗费%d秒.\n&quot;, new Date().toString(), timer); // 业务处理完成之后,计数器减一 countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; // 主线程一直被阻塞,直到countDownLatch的值为0 countDownLatch.await(); System.out.printf(&quot;%s时全部任务都完成,执行合并计算.\n&quot;, new Date().toString()); executor.shutdown(); &#125;&#125; 程序输出：12345Fri Aug 18 15:20:14 CST 2017时完成磁盘的统计任务,耗费0秒.Fri Aug 18 15:20:15 CST 2017时完成磁盘的统计任务,耗费1秒.Fri Aug 18 15:20:16 CST 2017时完成磁盘的统计任务,耗费2秒.Fri Aug 18 15:20:17 CST 2017时完成磁盘的统计任务,耗费3秒.Fri Aug 18 15:20:17 CST 2017时全部任务都完成,执行合并计算. 从输出中我们可以看出“执行合并计算”这句是在所有线程完成统计任务之后才输出的。 2、同步屏障CyclicBarrierCyclicBarrier要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行。 CyclicBarrier初始化的时候，设置一个屏障数。线程调用await()方法的时候，这个线程就会被阻塞，当调用await()的线程数量到达屏障数的时候，主线程就会取消所有被阻塞线程的状态。其构造方法如下：1public CyclicBarrier(int parties)&#123;&#125; 参数parties则为初始化时的屏障数CyclicBarrier还提供一个更高级的构造函数1public CyclicBarrier(int parties, Runnable barrierAction) &#123;&#125; 用于在线程到达屏障时，优先执行barrierAction，方便处理更复杂的业务场景例如，用一个Excel保存了用户所有的银行流水，每个sheet保存一个账户近一年的每笔交易流水，现在需要统计用户的日均交易流水，先用多线程处理每个sheet里的交易流水，都处理完后，得到每个sheet的日均交易流水，最后再用barrierAction用这些线程的计算结果，计算出整个Excel的日均银行流水，代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.dreyer.javadoc.thread;import java.util.Map;import java.util.concurrent.*;/** * @description 银行交易流水服务类 * @author: 会跳舞的机器人 * @date: 16/5/15 上午11:29 */public class BankWaterService implements Runnable &#123; /** * 创建4个屏障,处理完之后,执行当前类的run方法 */ private CyclicBarrier cyclicBarrier = new CyclicBarrier(4, this); /** * 启动4个线程 */ private Executor executor = Executors.newFixedThreadPool(4); /** * 保存每个sheet计算出来的银行交易流水结果 */ private ConcurrentHashMap&lt;String, Integer&gt; sheetBankWaterCount = new ConcurrentHashMap&lt;String, Integer&gt;(); /** * 交易流水统计 */ private void count() &#123; for (int i = 0; i &lt; 4; i++) &#123; executor.execute(new Runnable() &#123; public void run() &#123; // 模拟计算当前sheet的银行交易流水数据的业务处理 sheetBankWaterCount.put(Thread.currentThread().getName(), 1); // 银行交易流水计算完成后,插入一个屏障 try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125; /** * 汇总计算结果 */ public void run() &#123; int result = 0; for (Map.Entry&lt;String, Integer&gt; sheet : sheetBankWaterCount.entrySet()) &#123; result += sheet.getValue(); &#125; // 设置计算结果,并输出 sheetBankWaterCount.put(&quot;result&quot;, result); System.out.println(result); &#125; public static void main(String[] args) &#123; BankWaterService service = new BankWaterService(); service.count(); &#125;&#125; 3、控制并发线程数SemaphoreSemaphore被用于控制特定资源在同一个时间被访问的线程数量，它通过协调各个线程，以保证资源可以被合理的使用。做个比喻，把Semaphore比作是控制流量的红绿灯，比如xx马路要限制流量，只允许同时有一百辆车在马路上行驶，其他的都必须在路口等待，所以前一百辆会看到绿灯，可以开进马路，后面的车会看到红灯，不能开进马路，但是如果前面一百辆车中有5辆已经离开了马路，那后面就允许有5辆车驶入马路，这里例子里说的车就是线程，驶入马路就代表线程正在执行，离开马路就表示线程执行完成，看到红灯就代表线程被阻塞，不能执行。 应用场景：Semaph可以用来做流量限制，特别是公共资源有限的应用场景，比如说数据库连接。假如有一个需求，要读取几万个文件的数据，因为都是IO密集型人物，我们可以启动几十个线程并发的读取，但是如果读取到内存后，还需要储存到数据库，而数据库的连接数只有10个，这时候我们就必须控制只有10个线程同时获取到数据库连接，否则会抛出异常提示无法连接数据库。针对这种情况，我们就可以使用Semaphore来做流量控制。代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.dreyer.javadoc.thread;import java.util.concurrent.*;/** * @description * @author: 会跳舞的机器人 * @date: 16/5/15 上午11:59 */public class SemaphoreDemo &#123; /** * 线程数量 */ private static final int THREAD_COUNT = 30; /** * 线程池 */ private static ExecutorService executor = Executors.newFixedThreadPool(THREAD_COUNT); private static Semaphore semaphore = new Semaphore(10); public static void main(String[] args) &#123; for (int i = 0; i &lt; THREAD_COUNT; i++) &#123; executor.execute(new Runnable() &#123; public void run() &#123; try &#123; // 获取一个&quot;许可证&quot; semaphore.acquire(); // 模拟数据保存 TimeUnit.SECONDS.sleep(2); System.out.println(&quot;save date...&quot;); // 执行完后,归还&quot;许可证&quot; semaphore.release(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; executor.shutdown(); &#125;&#125; 在代码中，虽然有30个线程在执行，但是只运行10个并发的执行。所以我们可以看到在执行的过程中save data…是每10个输出的。 Semaphore的构造方法Semaphore(int permits)接受一个整形的数字，表示可用的许可证数量。Semaphore(10)表示运行10个线程获取许可证，也就是最大的并发数是10。Semaphore的用法也很简单，首先使用Semaphore.acquire()方法获取一个许可证，使用完之后调用release()方法归还许可证。 4、线程间交换数据的ExchangerExchanger（交换者）是一个用于线程间协作的工具类。Exchanger用于进行线程间的数据交换。它提供一个同步点，在这个同步点，两个线程可以交换彼此的数据。这两个线程通过exchange方法交换数据，如果第一个线程先执行exchange()方法，它会一直等待第二个线程也执行exchange方法，当两个线程都到达同步点时，这两个线程就可以交换数据，将本线程生产出来的数据传递给对方。 下面来看一下Exchanger的应用场景。 Exchanger可以用于遗传算法，遗传算法里需要选出两个人作为交配对象，这时候会交换两人的数据，并使用交叉规则得出2个交配结果。Exchanger也可以用于校对工作，比如我们需要将纸制银行流水通过人工的方式录入成电子银行流水，为了避免错误，采用AB岗两人进行录入，录入到Excel之后，系统需要加载这两个Excel，并对两个Excel数据进行校对，看看是否录入一致，代码示例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package main.java.com.robot.demo;import java.util.concurrent.Exchanger;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * @author: 会跳舞的机器人 * @date: 2017/8/18 15:32 * @description: 线程间交换数据的Exchanger示例 */public class ExchangerDemo &#123; private static final Exchanger&lt;String&gt; exchanger = new Exchanger&lt;&gt;(); /** * 线程数 */ private static ExecutorService executorService = Executors.newFixedThreadPool(2); public static void main(String[] args) &#123; executorService.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; String a = &quot;银行流水A&quot;; // A录入的银行流水数据 exchanger.exchange(a); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); executorService.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; String b = &quot;银行流水B&quot;; // B录入的银行流水数据 String a = exchanger.exchange(b); System.out.println(&quot;A和B的数据是否一致：&quot; + a.equals(b) + &quot;；A录入的是&quot; + a + &quot;；B录入的是：&quot; + b); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); executorService.shutdown(); &#125;&#125; 代码输出：1A和B的数据是否一致：false；A录入的是银行流水A；B录入的是：银行流水B 如果把变量a、b的值改为一样的，则会输出1A和B的数据是否一致：true；A录入的是银行流水A；B录入的是：银行流水A]]></content>
      <categories>
        <category>java并发编程</category>
      </categories>
      <tags>
        <tag>java并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程的艺术笔记第六章——java并发容器和框架]]></title>
    <url>%2F2017%2F11%2F29%2Fjava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E8%89%BA%E6%9C%AF%E7%AC%94%E8%AE%B0%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94java%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8%E5%92%8C%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[1、ConcurrentHashMap的实现原理与使用1.1、为什么使用ConcurrentHashMap HashMap非线程安全 HashTable读写都需要加锁，效率低下 ConcurrentHashMap的锁分段技术可以提高并发效率 1.2、ConcurrentHashMap的结构ConcurrentHashMap由Segment数组结构和HashEntry数组结构组成，Segement是一种可重入锁，在ConcurrentHashMap扮演着锁的角色；HashEntry用于存储键值对数据，一个ConcurrentHashMap中包含一个Segment数组，它是数组和链表结构。一个Segment里包含一个HashEntry数组，当对HashEntry数组进行修改操作时必须要获取它对应的Segment锁。 1.3、ConcurrentHashMap的初始化1.4、定位Segment通过散列算法定位Segment，散列冲突 2、ConcurrentLinkedQueue并发编程中实现线程安全的队列有两种方式，一种是阻塞队列，一种是非阻塞队列，非阻塞的实现方式可以通过CAS方式来实现。 ConcurrentLinkedQueue是一个基于链接节点的无界安全队列。它采用先进先出的方式对节点进行排序，当我们添加一个元素的时候，它会添加至队列的队尾，当我们获取元素的时候，它会返回队列头部的元素。 3、java中的阻塞队列阻塞队列支持阻塞的添加/移除元素的方法。支持阻塞的插入的意思是：当队列已满时，队列会阻塞插入队列的线程，直到队列有空位；支持阻塞的移除的意思是：当队列为空时，队列会阻塞移除队列元素的线程，直到队列中有新的元素添加进来。 阻塞队列场用于生产/消费者模式，生产者是向队列中添加元素的线程，消费者是从队列中获取元素的线程，而阻塞队列在其中充当着容器的角色。阻塞队列的插入和移除有四种操作方式，详情请参考文档。 java中有7中阻塞队列，分别是： ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列 LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列。 PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。 DelayQueue：一个支持延时获取元素的无界阻塞队列。 SynchronousQueue：一个不存储元素的阻塞队列。 LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 着重介绍下DelayQueue，它可以运用在以下业务场景： 缓存系统的设置：可以用DelayQueue保存元素的有效期，使用一个线程无限循环查询DelayQueue，一旦能从DelayQueue中获取到元素，则说明缓存有效期到了。 使用DelayQueue保存当天将会执行的任务和时间，一旦从DelayQueue获取到任务，就开始执行，TimeQueue就是使用DelayQueue来实现的。 在公司有这么一个业务场景：订单支付后要给商户发送相应的通知，针对同一条通知记录，如果是第一次发，则需要等待的时间是0分钟，第二次发则需要等待1分钟，第三次发则需要等待3分钟，即发送次数每+1，则需要等待的时长也要相应的增加，那使用DelayQueue就能很好的实现这个功能了。以下是参考实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package main.java.com.robot.demo;import java.util.concurrent.Delayed;import java.util.concurrent.TimeUnit;/** * @author: 会跳舞的机器人 * @date: 2017/8/17 14:43 * @description: 通知任务 */public class NotifyTask implements Delayed, Runnable &#123; /** * 任务名称 */ private String notifyTaskName; /** * 执行时间，单位：毫秒 */ private long executeTime; public NotifyTask(String notifyTaskName, long executeTime) &#123; this.notifyTaskName = notifyTaskName; this.executeTime = executeTime; &#125; /** * 获取延迟时间 * * @param unit * @return 返回当前元素还需要延长多久时间 */ @Override public long getDelay(TimeUnit unit) &#123; return unit.convert(executeTime - System.currentTimeMillis(), unit.MILLISECONDS); &#125; /** * 用来指定元素的顺序,让延时时间最长的放在队列的末尾 * * @param o * @return */ @Override public int compareTo(Delayed o) &#123; NotifyTask notifyTask = (NotifyTask) o; return executeTime &gt; notifyTask.executeTime ? 1 : (executeTime &lt; notifyTask.executeTime ? -1 : 0); &#125; @Override public void run() &#123; System.out.println(&quot;当前时间毫秒数：&quot; + System.currentTimeMillis() + &quot;，线程：&quot; + this.toString() + &quot;正在执行&quot;); &#125; @Override public String toString() &#123; return &quot;NotifyTask&#123;&quot; + &quot;notifyTaskName=&apos;&quot; + notifyTaskName + &apos;\&apos;&apos; + &quot;, executeTime=&quot; + executeTime + &apos;&#125;&apos;; &#125;&#125; 测试：12345678910111213141516171819202122232425262728293031323334353637package main.java.com.robot.demo;import java.util.Random;import java.util.concurrent.DelayQueue;/** * @author: 会跳舞的机器人 * @date: 2017/8/17 14:52 * @description: 延迟队列DelayQueue测试 */public class NotifyTaskTest &#123; /** * 通知任务存放的延迟队列 */ private static DelayQueue&lt;NotifyTask&gt; tasks = new DelayQueue&lt;&gt;(); public static void main(String[] args) &#123; Random random = new Random(); for (int i = 0; i &lt; 5; i++) &#123; // 随机产生一个秒数 int seconds = random.nextInt(5); NotifyTask notifyTask = new NotifyTask(&quot;任务&quot; + i, System.currentTimeMillis() + (seconds * 1000)); tasks.put(notifyTask); &#125; while (true) &#123; NotifyTask notifyTask = tasks.poll(); if (notifyTask != null) &#123; notifyTask.run(); &#125; // 如果队列中的元素全部被取完,则跳出循环 if (tasks.size() == 0) &#123; break; &#125; &#125; &#125;&#125; 控制台输出：12345当前时间毫秒数：1502953649855，线程：NotifyTask&#123;notifyTaskName=&apos;任务1&apos;, executeTime=1502953649855&#125;正在执行当前时间毫秒数：1502953649855，线程：NotifyTask&#123;notifyTaskName=&apos;任务3&apos;, executeTime=1502953649855&#125;正在执行当前时间毫秒数：1502953650855，线程：NotifyTask&#123;notifyTaskName=&apos;任务0&apos;, executeTime=1502953650855&#125;正在执行当前时间毫秒数：1502953651855，线程：NotifyTask&#123;notifyTaskName=&apos;任务2&apos;, executeTime=1502953651855&#125;正在执行当前时间毫秒数：1502953651855，线程：NotifyTask&#123;notifyTaskName=&apos;任务4&apos;, executeTime=1502953651855&#125;正在执行 从输出中可以看出任务的执行时间都是我们创建任务的时候指定的时间。 4、Fork/Join框架4.1、什么是Fork/Join框架Fork/Join框架是Java 7提供的一个用于并行执行任务的框架，是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架。 我们再通过Fork和Join这两个单词来理解一下Fork/Join框架。Fork就是把一个大任务切分为若干子任务并行的执行，Join就是合并这些子任务的执行结果，最后得到这个大任务的结果。比如计算1+2+…+10000，可以分割成10个子任务，每个子任务分别对1000个数进行求和，最终汇总这10个子任务的结果。 4.2、工作窃取算法工作窃取（work-stealing）算法是指某个线程从其他队列里窃取任务来执行。那么，为什么需要使用工作窃取算法呢？假如我们需要做一个比较大的任务，可以把这个任务分割为若干互不依赖的子任务，为了减少线程间的竞争，把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务，线程和队列一一对应。比如A线程负责处理A队列里的任务。但是，有的线程会先把自己队列里的任务干完，而其他线程对应的队列里还有任务等待处理。干完活的线程与其等着，不如去帮其他线程干活，于是它就去其他线程的队列里窃取一个任务来执行。而在这时它们会访问同一个队列，所以为了减少窃取任务线程和被窃取任务线程之间的竞争，通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，而窃取任务的线程永远从双端队列的尾部拿任务执行。 工作窃取算法的优点：充分利用线程进行并行计算，减少了线程间的竞争。 工作窃取算法的缺点：在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且该算法会消耗了更多的系统资源，比如创建多个线程和多个双端队列。 4.3、使用Fork/Join框架让我们通过一个简单的需求来使用Fork/Join框架，需求是：计算1+2+3+4的结果。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package main.java.com.robot.demo;import java.util.concurrent.ExecutionException;import java.util.concurrent.ForkJoinPool;import java.util.concurrent.Future;import java.util.concurrent.RecursiveTask;/** * @author: 会跳舞的机器人 * @date: 2017/8/17 15:45 * @description: Fork/Join框架简单demo */public class CountTask extends RecursiveTask&lt;Integer&gt; &#123; /** * 阈值 */ private static final int THRESHOLD = 2; private int start; private int end; public CountTask(int start, int end) &#123; this.start = start; this.end = end; &#125; @Override protected Integer compute() &#123; int sum = 0; // 判断任务是否足够小，足够小就直接计算 boolean canCompute = (end - start) &lt;= THRESHOLD; if (canCompute) &#123; for (int i = start; i &lt;= end; i++) &#123; sum += i; &#125; &#125; else &#123; // 如果任务大于阈值，就分解成两个任务执行 int midel = (end - start) / 2; CountTask leftTask = new CountTask(start, midel); CountTask rightTask = new CountTask(midel + 1, end); // 执行子任务 leftTask.fork(); rightTask.fork(); // 等待计算结果，合并子任务 int leftResult = leftTask.join(); int rightResult = rightTask.join(); sum = leftResult + rightResult; &#125; return sum; &#125; public static void main(String[] args) &#123; ForkJoinPool forkJoinPool = new ForkJoinPool(); // 生成一个计算任务，负责计算1+2+3+4 CountTask task = new CountTask(1, 4); // 执行一个任务 Future&lt;Integer&gt; result = forkJoinPool.submit(task); try &#123; System.out.println(result.get()); &#125; catch (InterruptedException e) &#123; &#125; catch (ExecutionException e) &#123; &#125; &#125;&#125; 4.4、Fork/Join框架设计及其实现原理参考书本]]></content>
      <categories>
        <category>java并发编程</category>
      </categories>
      <tags>
        <tag>java并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程的艺术笔记第五章——java中的锁]]></title>
    <url>%2F2017%2F11%2F29%2Fjava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E8%89%BA%E6%9C%AF%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%BA%94%E7%AB%A0%E2%80%94%E2%80%94java%E4%B8%AD%E7%9A%84%E9%94%81%2F</url>
    <content type="text"><![CDATA[1、Lock接口锁是用来控制多个线程访问共享资源的方式，一般来说锁能够防止多个线程同时访问共享资源（有的锁可以允许多个线程访问共享资源，比如说读写锁），在Lock接口出现之前，java程序是靠synchronized关键字实现锁功能的，但是在JKD1.5之后并发包中新增了Lock接口及其实现来实现锁的功能。它提供了synchronized关键字类似的功能，但是Lock需要显示的获取锁、释放锁，而synchronized是通过隐式的方式来实现获取、释放锁。 Lock接口常见的API请自行查找 2、队列同步器略… 3、重入锁重入锁ReentrantLock，顾名思义，就是支持重进入的锁，重入锁支持同一个线程多次的获取锁。除此之外，该锁还支持获取锁时的公平和非公平性。 关于锁的公平性，如果在绝对的时间上，先对锁进行获取请求的操作一定会先被满足，则说明这个锁是公平的，反之则是不公平的。ReentrantLock的构造函数支持构造公平锁与非公平锁，默认为非公平锁，因为非公平锁能减少上下文切换，开销更小。 4、读写锁读写锁支持同一时刻允许多个线程同时访问，但是在写线程访问时，所有的读线程与其他的写线程均被阻塞。读写锁维护了一对锁，读锁与写锁，通过读锁与写锁的分离，读写锁的并发性相比于其他排他锁都有很大的提升。 java并发包提供的读写锁是ReentrantReadWriteLock，它具有公平性选择、锁重入、锁降级三个特性。 读写锁实现简单缓存demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package main.java.com.robot.demo;import java.util.HashMap;import java.util.Map;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantReadWriteLock;/** * @author: 会跳舞的机器人 * @date: 2017/8/16 14:45 * @description: 读写锁实现缓存demo */public class Cache &#123; static Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); static ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); static Lock writeLock = rwl.writeLock(); static Lock readLock = rwl.readLock(); /** * 读缓存 */ public static final Object get(String key) &#123; readLock.lock(); try &#123; return map.get(key); &#125; finally &#123; readLock.unlock(); &#125; &#125; /** * 写入缓存 */ public static final Object put(String key, Object value) &#123; writeLock.lock(); try &#123; return map.put(key, value); &#125; finally &#123; writeLock.unlock(); &#125; &#125; /** * 清空缓存 */ public static final void clear() &#123; writeLock.lock(); try &#123; map.clear(); &#125; finally &#123; writeLock.unlock(); &#125; &#125; /** * 获取缓存大小 */ public static final int size() &#123; writeLock.lock(); try &#123; return map.size(); &#125; finally &#123; writeLock.unlock(); &#125; &#125;&#125; 5、LockSupport工具略… 6、Condition接口利用Condition接口来实现一个简单的有界阻塞队列，当队列为空时，队列的获取操作将会阻塞获取线程，知道有新的元素添加进来，当队列满时，队列的插入操作将会阻塞插入线程，知道队列出现“空位”。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package main.java.com.robot.demo;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * @author: 会跳舞的机器人 * @date: 2017/8/16 15:48 * @description: Condition实现有界阻塞队列demo */public class BoundedQueue &#123; private Object[] items; private int addIndex, removeIndex, count; private Lock lock = new ReentrantLock(); private Condition notEmpty = lock.newCondition(); private Condition notFull = lock.newCondition(); public BoundedQueue(int size) &#123; items = new Object[size]; &#125; /** * 添加一个元素，如果队列已满，则线程进入等待状态，直到有空位为止 */ public void add(Object object) throws InterruptedException &#123; lock.lock(); try &#123; // 队列已满，阻塞线程 while (items.length == count) &#123; System.out.println(&quot;队列已满，等待......&quot;); notFull.await(); &#125; items[addIndex] = object; if (++addIndex == items.length) &#123; addIndex = 0; &#125; ++count; // 唤醒 notEmpty.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; /** * 删除队列的第一个元素，如果队列为空，则线程进入等待状态，直到有新元素添加进来 */ public Object remove() throws InterruptedException &#123; lock.lock(); try &#123; while (count == 0) &#123; System.out.println(&quot;空队列，等待......&quot;); notEmpty.await(); &#125; Object obj = items[removeIndex]; if (++removeIndex == items.length) &#123; removeIndex = 0; &#125; --count; notFull.signal(); return obj; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>java并发编程</category>
      </categories>
      <tags>
        <tag>java并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程的艺术笔记第一章——并发编程的挑战]]></title>
    <url>%2F2017%2F11%2F29%2Fjava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E8%89%BA%E6%9C%AF%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E6%8C%91%E6%88%98%2F</url>
    <content type="text"><![CDATA[1、并发编程的挑战来自哪里？ 1.1、上下文切换 1.2、死锁 1.3、资源限制的挑战 1.1、上下文切换单核处理器也支持多线程执行代码，CPU通过给每个线程分配CPU时间片来实现这个机制。时间片是CPU分配给各个线程的时间，因为时间片非常短，所以CPU通过不停地切换线程执行，让我们感觉多个线程是同时执行的，时间片一般是几十毫秒（ms） CPU通过时间片分配算法来循环执行任务，当前任务执行完一个时间片之后会切换到下一个任务，但是在切换之前会保存上一个任务的状态，以便下次切换回这个任务的时候还可以加载回之前的状态。所以任务从保存到重载的这一过程就是一次上下文切换。 多线程执行任务一定快？答案是不一定，因为创建线程和上下文切换会占用一定的时间。 如何减少上下文切换 无锁并发编程 CAS算法 使用最少线程执行 1.2、死锁线程之间相互持有对方资源而不释放会导致死锁1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package main.java.com.robot.demo;/** * @author: 会跳舞的机器人 * @date: 2017/8/9 16:30 * @description: java死锁案例 */public class DeadLock &#123; private static String firstMonitor = &quot;A&quot;; private static String secondMonitor = &quot;B&quot;; public static void main(String[] args) &#123; Thread thread1 = new Thread(new Runnable() &#123; @Override public void run() &#123; synchronized (firstMonitor) &#123; System.out.println(&quot;thread1 locked firstMonitor&quot;); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (secondMonitor) &#123; System.out.println(&quot;thread1 locked secondMonitor&quot;); &#125; &#125; &#125; &#125;); Thread thread2 = new Thread(new Runnable() &#123; public void run() &#123; synchronized (secondMonitor) &#123; System.out.println(&quot;thread2 locked secondMonitor&quot;); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (firstMonitor) &#123; System.out.println(&quot;thread2 locked firstMonitor&quot;); &#125; &#125; &#125; &#125;); thread1.start(); thread2.start(); &#125;&#125; 1、3资源限制的挑战并不是说线程越多，代码执行速度会更快，还取决于硬件资源。所以根据不同的硬件资源配置合适的并发数显得尤为重要。]]></content>
      <categories>
        <category>java并发编程</category>
      </categories>
      <tags>
        <tag>java并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper典型应用场景一览（转）]]></title>
    <url>%2F2017%2F11%2F28%2FZooKeeper%E5%85%B8%E5%9E%8B%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E4%B8%80%E8%A7%88%EF%BC%88%E8%BD%AC%EF%BC%89%2F</url>
    <content type="text"><![CDATA[ZooKeeper是一个高可用的分布式数据管理与系统协调框架。基于对Paxos算法的实现，使该框架保证了分布式环境中数据的强一致性，也正是基于这样的特性，使得ZooKeeper解决很多分布式问题。网上对ZK的应用场景也有不少介绍，本文将结合作者身边的项目例子，系统地对ZK的应用场景进行一个分门归类的介绍。值得注意的是，ZK并非天生就是为这些应用场景设计的，都是后来众多开发者根据其框架的特性，利用其提供的一系列API接口（或者称为原语集），摸索出来的典型使用方法。因此，也非常欢迎读者分享你在ZK使用上的奇技淫巧。 数据发布与订阅（配置中心）发布与订阅模型，即所谓的配置中心，顾名思义就是发布者将数据发布到ZK节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。例如全局的配置信息，服务式服务框架的服务地址列表等就非常适合使用。 应用中用到的一些配置信息放到ZK上进行集中管理。这类场景通常是这样：应用在启动的时候会主动来获取一次配置，同时，在节点上注册一个Watcher，这样一来，以后每次配置有更新的时候，都会实时通知到订阅的客户端，从来达到获取最新配置信息的目的。 分布式搜索服务中，索引的元信息和服务器集群机器的节点状态存放在ZK的一些指定节点，供各个客户端订阅使用。 分布式日志收集系统。这个系统的核心工作是收集分布在不同机器的日志。收集器通常是按照应用来分配收集任务单元，因此需要在ZK上创建一个以应用名作为path的节点P，并将这个应用的所有机器ip，以子节点的形式注册到节点P上，这样一来就能够实现机器变动的时候，能够实时通知到收集器调整任务分配。 系统中有些信息需要动态获取，并且还会存在人工手动去修改这个信息的发问。通常是暴露出接口，例如JMX接口，来获取一些运行时的信息。引入ZK之后，就不用自己实现一套方案了，只要将这些信息存放到指定的ZK节点上即可。注意：在上面提到的应用场景中，有个默认前提是：数据量很小，但是数据更新可能会比较快的场景。 负载均衡这里说的负载均衡是指软负载均衡。在分布式环境中，为了保证高可用性，通常同一个应用或同一个服务的提供方都会部署多份，达到对等服务。而消费者就须要在这些对等的服务器中选择一个来执行相关的业务逻辑，其中比较典型的是消息中间件中的生产者，消费者负载均衡。消息中间件中发布者和订阅者的负载均衡，linkedin开源的KafkaMQ和阿里开源的metaq都是通过zookeeper来做到生产者、消费者的负载均衡。这里以metaq为例如讲下： 生产者负载均衡metaq发送消息的时候，生产者在发送消息的时候必须选择一台broker上的一个分区来发送消息，因此metaq在运行过程中，会把所有broker和对应的分区信息全部注册到ZK指定节点上，默认的策略是一个依次轮询的过程，生产者在通过ZK获取分区列表之后，会按照brokerId和partition的顺序排列组织成一个有序的分区列表，发送的时候按照从头到尾循环往复的方式选择一个分区来发送消息。 消费负载均衡在消费过程中，一个消费者会消费一个或多个分区中的消息，但是一个分区只会由一个消费者来消费。MetaQ的消费策略是： 每个分区针对同一个group只挂载一个消费者。 如果同一个group的消费者数目大于分区数目，则多出来的消费者将不参与消费。 如果同一个group的消费者数目小于分区数目，则有部分消费者需要额外承担消费任务。在某个消费者故障或者重启等情况下，其他消费者会感知到这一变化（通过 zookeeper watch消费者列表），然后重新进行负载均衡，保证所有的分区都有消费者进行消费。 命名服务(Naming Service)命名服务也是分布式系统中比较常见的一类场景。在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务地址，远程对象等等——这些我们都可以统称他们为名字（Name）。其中较为常见的就是一些分布式服务框架中的服务地址列表。通过调用ZK提供的创建节点的API，能够很容易创建一个全局唯一的path，这个path就可以作为一个名称。 阿里巴巴集团开源的分布式服务框架Dubbo中使用ZooKeeper来作为其命名服务，维护全局的服务地址列表，点击这里查看Dubbo开源项目。在Dubbo实现中：服务提供者在启动的时候，向ZK上的指定节点/dubbo/${serviceName}/providers目录下写入自己的URL地址，这个操作就完成了服务的发布。 服务消费者启动的时候，订阅/dubbo/${serviceName}/providers目录下的提供者URL地址， 并向/dubbo/${serviceName} /consumers目录下写入自己的URL地址。注意，所有向ZK上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。 另外，Dubbo还有针对服务粒度的监控，方法是订阅/dubbo/${serviceName}目录下所有提供者和消费者的信息。 分布式通知/协调ZooKeeper中特有watcher注册与异步通知机制，能够很好的实现分布式环境下不同系统之间的通知与协调，实现对数据变更的实时处理。使用方法通常是不同系统都对ZK上同一个znode进行注册，监听znode的变化（包括znode本身内容及子节点的），其中一个系统update了znode，那么另一个系统能够收到通知，并作出相应处理 另一种心跳检测机制：检测系统和被检测系统之间并不直接关联起来，而是通过zk上某个节点关联，大大减少系统耦合。 另一种系统调度模式：某系统有控制台和推送系统两部分组成，控制台的职责是控制推送系统进行相应的推送工作。管理人员在控制台作的一些操作，实际上是修改了ZK上某些节点的状态，而ZK就把这些变化通知给他们注册Watcher的客户端，即推送系统，于是，作出相应的推送任务。 另一种工作汇报模式：一些类似于任务分发系统，子任务启动后，到zk来注册一个临时节点，并且定时将自己的进度进行汇报（将进度写回这个临时节点），这样任务管理者就能够实时知道任务进度。总之，使用zookeeper来进行分布式通知和协调能够大大降低系统之间的耦合 集群管理与Master选举集群机器监控：这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群机器是否存活。过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报“我还活着”。 这种做法可行，但是存在两个比较明显的问题： 集群中机器有变动的时候，牵连修改的东西比较多。 有一定的延时。 利用ZooKeeper有两个特性，就可以实现另一种集群机器存活性监控系统： 客户端在节点 x 上注册一个Watcher，那么如果 x?的子节点变化了，会通知该客户端。 创建EPHEMERAL类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。例如，监控系统在 /clusterServers 节点上注册一个Watcher，以后每动态加机器，那么就往 /clusterServers 下创建一个 EPHEMERAL类型的节点：/clusterServers/{hostname}. 这样，监控系统就能够实时知道机器的增减情况，至于后续处理就是监控系统的业务了。 Master选举则是zookeeper中最为经典的应用场景了。在分布式环境中，相同的业务应用分布在不同的机器上，有些业务逻辑（例如一些耗时的计算，网络I/O处理），往往只需要让整个集群中的某一台机器进行执行，其余机器可以共享这个结果，这样可以大大减少重复劳动，提高性能，于是这个master选举便是这种场景下的碰到的主要问题。利用ZooKeeper的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性，即：同时有多个客户端请求创建 /currentMaster 节点，最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很轻易的在分布式环境中进行集群选取了。另外，这种场景演化一下，就是动态Master选举。这就要用到EPHEMERAL_SEQUENTIAL类型节点的特性了。上文中提到，所有客户端创建请求，最终只有一个能够创建成功。在这里稍微变化下，就是允许所有请求都能够创建成功，但是得有个创建顺序，于是所有的请求最终在ZK上创建结果的一种可能情况是这样： /currentMaster/{sessionId}-1 ,/currentMaster/{sessionId}-2,/currentMaster/{sessionId}-3 ….. 每次选取序列号最小的那个机器作为Master，如果这个机器挂了，由于他创建的节点会马上小时，那么之后最小的那个机器就是Master了。 在搜索系统中，如果集群中每个机器都生成一份全量索引，不仅耗时，而且不能保证彼此之间索引数据一致。因此让集群中的Master来进行全量索引的生成，然后同步到集群中其它机器。另外，Master选举的容灾措施是，可以随时进行手动指定master，就是说应用在zk在无法获取master信息时，可以通过比如http方式，向一个地方获取master。 在Hbase中，也是使用ZooKeeper来实现动态HMaster的选举。在Hbase实现中，会在ZK上存储一些ROOT表的地址和HMaster的地址，HRegionServer也会把自己以临时节点（Ephemeral）的方式注册到Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的存活状态，同时，一旦HMaster出现问题，会重新选举出一个HMaster来运行，从而避免了HMaster的单点问题 分布式锁分布式锁，这个主要得益于ZooKeeper为我们保证了数据的强一致性。锁服务可以分为两类，一个是保持独占，另一个是控制时序。 所谓保持独占，就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。通常的做法是把zk上的一个znode看作是一把锁，通过create znode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。 控制时序，就是所有视图来获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序了。做法和上面基本类似，只是这里 /distribute_lock 已经预先存在，客户端在它下面创建临时有序节点（这个可以通过节点的属性控制：CreateMode.EPHEMERAL_SEQUENTIAL来指定）。Zk的父节点（/distribute_lock）维持一份sequence,保证子节点创建的时序性，从而也形成了每个客户端的全局时序。 分布式队列队列方面，简单地讲有两种，一种是常规的先进先出队列，另一种是要等到队列成员聚齐之后的才统一按序执行。 对于第一种先进先出队列，和分布式锁服务中的控制时序场景基本原理一致，这里不再赘述。 第二种队列其实是在FIFO队列的基础上作了一个增强。通常可以在 /queue 这个znode下预先建立一个/queue/num 节点，并且赋值为n（或者直接给/queue赋值n），表示队列大小，之后每次有队列成员加入后，就判断下是否已经到达队列大小，决定是否可以开始执行了。这种用法的典型场景是，分布式环境中，一个大任务Task A，需要在很多子任务完成（或条件就绪）情况下才能进行。这个时候，凡是其中一个子任务完成（就绪），那么就去 /taskList 下建立自己的临时时序节点（CreateMode.EPHEMERAL_SEQUENTIAL），当 /taskList 发现自己下面的子节点满足指定个数，就可以进行下一步按序进行处理了。]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper分布式锁应用]]></title>
    <url>%2F2017%2F11%2F28%2FZooKeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、Zookeeper是什么Zookeeper是一个高性能的分布式系统的协调服务。它在一个简单的接口里暴露公共服务：像命名、配置管理、同步、和群组服务，所以你没有必要从头开始实现它们。你可以使用现成的Zookeeper去实现共识、群组管理、领导人选举和业务协议。并且你可以在它的基础之上建立自己特定的需求。 二、Zookeeper分布式锁的实现原理利用临时顺序节点实现Zookeeper分布式锁。 1、首先为一个lock场景，在zookeeper中指定对应的一个根节点，用于记录资源竞争的内容，称之为/lock_node 2、每个lock创建后，会lazy在zookeeper中创建一个node节点，表明对应的资源竞争标识。 (小技巧：node节点为EPHEMERAL_SEQUENTIAL，自增长的临时节点)。比如有两个客户端创建znode，那分别为/lock_node/lock-1、/lock_node/lock-2 3、进行lock操作时，获取对应lock根节点下的所有字节点，也即处于竞争中的资源标识 4、按照Fair竞争的原则，按照对应的自增内容做排序，取出编号最小的一个节点做为lock的owner，判断自己的节点id是否就为owner id，如果是则返回，lock成功。 5、如果自己非owner id，按照排序的结果找到序号比自己前一位的id，关注它锁释放的操作(也就是exist watcher)，形成一个链式的触发过程。 unlock过程 6、将自己id对应的节点删除即可，对应的下一个排队的节点就可以收到Watcher事件，从而被唤醒得到锁后退出 ZooKeeper的几个特性让它非常合适作为分布式锁服务 zookeeper支持watcher机制，这样实现阻塞锁，可以watch锁数据，等到数据被删除，zookeeper会通知客户端去重新竞争锁。 zookeeper的数据可以支持临时节点的概念，即客户端写入的数据是临时数据，在客户端宕机后，临时数据会被删除，这样就实现了锁的异常释放。使用这样的方式，就不需要给锁增加超时自动释放的特性了。 三、Zookeeper分布式锁应用Curator是Netflix公司开源的一个Zookeeper客户端，与Zookeeper提供的原生客户端相比，Curator的抽象层次更高，简化了Zookeeper客户端的开发量。 使用场景1、创建ZooKeeperConnector.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.robot.zookeeper.components;import org.apache.curator.RetryPolicy;import org.apache.curator.framework.CuratorFramework;import org.apache.curator.framework.CuratorFrameworkFactory;import org.apache.curator.retry.ExponentialBackoffRetry;import org.apache.curator.utils.CloseableUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * @author: 会跳舞的机器人 * @date: 2017/6/21 18:07 * @description:ZooKeeper连接器 */public class ZooKeeperConnector &#123; private Logger logger = LoggerFactory.getLogger(this.getClass()); /** * 连接IP端口信息，格式为：10.1.74.75:2281,10.1.74.75:2282,10.1.74.75:2283 */ private String hosts; private CuratorFramework client; private static final int DEFAULT_SESSION_TIMEOUT_MS = 30 * 1000; private static final int DEFAULT_CONNECTION_TIMEOUT_MS = 10 * 1000; private int sessionTimeout = DEFAULT_SESSION_TIMEOUT_MS; private int connectionTimeout = DEFAULT_CONNECTION_TIMEOUT_MS; /** * 连接ZooKeeper */ public void connect() &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); client = CuratorFrameworkFactory.newClient(hosts, sessionTimeout, connectionTimeout, retryPolicy); client.start(); logger.info(&quot;Successfully connected to Zookeeper [&#123;&#125;] &quot;, hosts); &#125; /** * 关闭ZooKeeper的连接 */ public void close() &#123; CloseableUtils.closeQuietly(client); &#125; /** * 重连 * * @return */ public CuratorFramework reConnect() &#123; connect(); return client; &#125; public String getHosts() &#123; return hosts; &#125; public void setHosts(String hosts) &#123; this.hosts = hosts; &#125; public CuratorFramework getClient() &#123; if (client == null) &#123; connect(); &#125; return client; &#125; public void setClient(CuratorFramework client) &#123; this.client = client; &#125; public int getSessionTimeout() &#123; return sessionTimeout; &#125; public void setSessionTimeout(int sessionTimeout) &#123; this.sessionTimeout = sessionTimeout; &#125; public int getConnectionTimeout() &#123; return connectionTimeout; &#125; public void setConnectionTimeout(int connectionTimeout) &#123; this.connectionTimeout = connectionTimeout; &#125;&#125; 2、创建AccountLock.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package com.robot.zookeeper.utils;import com.robot.zookeeper.components.ZooKeeperConnector;import org.apache.curator.framework.recipes.locks.InterProcessMutex;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.concurrent.TimeUnit;/** * @author: 会跳舞的机器人 * @date: 2017/6/22 10:16 * @description:账户分布式锁 */public class AccountLock &#123; private Logger logger = LoggerFactory.getLogger(this.getClass()); /** * 共享锁 */ InterProcessMutex lock; /** * 是否获取到锁 */ boolean wasAcquired = false; /** * 构造器 * * @param path * @param zooKeeperConnector */ public AccountLock(String path, ZooKeeperConnector zooKeeperConnector) &#123; lock = new InterProcessMutex(zooKeeperConnector.getClient(), path); &#125; /** * 尝试加锁并等待 * * @param timeOut 超时时间（秒），-1表示不等待 * @return true表示获取锁成功，false表示获取锁失败 */ public boolean acquire(int timeOut) &#123; try &#123; if (timeOut == -1) &#123; wasAcquired = lock.acquire(-1, null); &#125; else &#123; wasAcquired = lock.acquire(timeOut, TimeUnit.SECONDS); &#125; &#125; catch (Exception e) &#123; logger.error(&quot;Get lock time out error&quot;, e.getMessage()); wasAcquired = false; &#125; return wasAcquired; &#125; /** * 尝试加锁并等待 * * @param timeOut timeOut 超时时间，-1表示不等待 * @param timeUnit 超时时间单位 * @return true表示获取锁成功，false表示获取锁失败 */ public boolean acquire(int timeOut, TimeUnit timeUnit) &#123; try &#123; wasAcquired = lock.acquire(timeOut, timeUnit); &#125; catch (Exception e) &#123; logger.error(&quot;Get lock time out error&quot;, e.getMessage()); wasAcquired = false; &#125; return wasAcquired; &#125; /** * 释放锁 */ public void release() &#123; if (wasAcquired) &#123; try &#123; lock.release(); &#125; catch (Exception e) &#123; logger.error(&quot;release lock error&quot;, e.getMessage()); &#125; &#125; &#125;&#125; 3、测试类1234567891011121314151617181920212223242526272829303132333435363738394041package com.robot.zookeeper;import com.robot.zookeeper.components.ZooKeeperConnector;import com.robot.zookeeper.utils.AccountLock;/** * @author: 会跳舞的机器人 * @date: 2017/6/22 10:34 * @description: */public class Test &#123; public static void main(String[] args) &#123; final ZooKeeperConnector zooKeeperConnector = new ZooKeeperConnector(); zooKeeperConnector.setHosts(&quot;192.168.133.128:2181,192.168.133.129:2182,192.168.133.130:2183&quot;); zooKeeperConnector.connect(); /** * 创建4个线程去获取锁 */ for (int i = 1; i &lt; 5; i++) &#123; Thread thread = new Thread() &#123; @Override public void run() &#123; AccountLock accountLock = new AccountLock(&quot;/ACCOUNT/221890&quot;, zooKeeperConnector); boolean wasAcquired = accountLock.acquire(10); if (wasAcquired) &#123; System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;获取到锁&quot;); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; accountLock.release(); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;释放锁&quot;); &#125; &#125; &#125;; thread.start(); &#125; &#125;&#125; 输出结果：12345678线程Thread-3获取到锁线程Thread-3释放锁线程Thread-4获取到锁线程Thread-4释放锁线程Thread-2获取到锁线程Thread-2释放锁线程Thread-1获取到锁线程Thread-1释放锁 账户加锁的时候，我们针对用户的ID进行加锁，在测试类中，我们创建了4个线程去获取锁，从输出结果可以看出每次只有一个线程能获取到锁，并且在该线程释放锁之后，其他的线程才能获取到锁。 当然，测试类中的ZooKeeperConnector的初始化一般都是通过Spring进行管理123456&lt;beans&gt; &lt;bean id=&quot;zkConnector&quot; class=&quot;com.baibei.component.zk.ZooKeeperConnector&quot; init-method=&quot;connect&quot; lazy-init=&quot;false&quot;&gt; &lt;property name=&quot;hosts&quot; value=&quot;#&#123;environment[&apos;ZOOKEEPER.CONNECTION.HOSTS&apos;]&#125;&quot; /&gt; &lt;/bean&gt;&lt;/beans&gt; Demo中所需要的maven配置如下：123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.10.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.10.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- curator的内嵌包版本存在问题，所以用这个版本来替代--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;18.0&lt;/version&gt; &lt;/dependency&gt;]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper集群搭建]]></title>
    <url>%2F2017%2F11%2F28%2FZookeeper%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[一、zookeeper集群简介Zookeeper集群中只要有过半的节点是正常的情况下,那么整个集群对外就是可用的。正是基于这个特性,要将 ZK 集群的节点数量要为奇数（2n+1），如 3、5、7 个节点)较为合适。 二、环境准备 IP 端口 用户 192.168.31.154 2181、2881、3881 dreyer03 192.168.31.117 2182、2882、3882 dreyer04 192.168.31.146 2183、2883、3883 dreyer05 三、具体安装步骤1、 修改每台机器的/etc/hosts 文件,添加 IP 与主机名映射:1# vi /etc/hosts 增加：123192.168.31.154 dreyer-zk-01192.168.31.117 dreyer-zk-02192.168.31.146 dreyer-zk-03 2、 下载或上传 zookeeper-3.4.6.tar.gz 到/home/dreyer/zookeeper 目录:1$ wget http://apache.fayea.com/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz 3、解压zookeeper安装包，并对照节点号对zookeeper目录进行重命名解压安装包：1$ tar -zxvf zookeeper-3.4.6.tar.gz 将服务器1的zookeeper目录重命名为node-01：1$ mv zookeeper-3.4.6 node-01 将服务器2的zookeeper目录重命名为node-02：1$ mv zookeeper-3.4.6 node-02 将服务器3的zookeeper目录重命名为node-03：1$ mv zookeeper-3.4.6 node-03 4、在各个zookeeper节点目录创建data、logs目录123$ cd /home/dreyer03/zookeeper/node-0X/（X代表节点号1、2、3，以下同解）$ mkdir data$ mkdir logs 5、将zookeeper/node-0X/conf目录下的zoo_sample.cfg文件拷贝一份，并命名为zoo.cfg1$ cp zoo_sample.cfg zoo.cfg 6、修改zoo.cfg配置文件zookeeper/node-01的配置如下：123456789tickTime=2000initLimit=10syncLimit=5dataDir=/home/dreyer03/zookeeper/node-01/datadataLogDir=/home/dreyer03/zookeeper/node-01/logsclientPort=2181server.1=dreyer-zk-01:2881:3881server.2=dreyer-zk-02:2882:3882server.3=dreyer-zk-03:2883:3883 zookeeper/node-02的配置如下：123456789tickTime=2000initLimit=10syncLimit=5dataDir=/home/dreyer04/zookeeper/node-02/datadataLogDir=/home/dreyer04/zookeeper/node-02/logsclientPort=2182server.1=dreyer-zk-01:2881:3881server.2=dreyer-zk-02:2882:3882server.3=dreyer-zk-03:2883:3883 zookeeper/node-03的配置如下：123456789tickTime=2000initLimit=10syncLimit=5dataDir=/home/dreyer05/zookeeper/node-03/datadataLogDir=/home/dreyer05/zookeeper/node-03/logsclientPort=2183server.1=dreyer-zk-01:2881:3881server.2=dreyer-zk-02:2882:3882server.3=dreyer-zk-03:2883:3883 参数说明： tickTime=2000：tickTime 这个时间是作为 Zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔,也就是每各tickTime 时间就会发送一个心跳。 initLimit=10：initLimit 这个配置项是用来配置 Zookeeper 接受客户端(这里所说的客户端不是用户连接 Zookeeper 服务器的客户端,而是Zookeeper 服务器集群中连接到 Leader 的 Follower服务器)初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过10个心跳的时间(也就是 tickTime)长度后Zookeeper 服务器还没有收到客户端的返回信息,那么表明这个客户端连接失败。总的时间长度就是 10*2000=20 秒。 syncLimit=5：syncLimit 这个配置项标识Leader与Follower之间发送消息,请求和应答时间长度,最长不能超过多少个tickTime 的时间长度,总的时间长度就是 5*2000=10 秒。 dataDir=/home/dreyer05/zookeeper/node-03/data：dataDir 顾名思义就是 Zookeeper 保存数据的目录,默认情况下 Zookeeper 将写数据的日志文件也保存在这个目录里。 clientPort=2181：clientPort 这个端口就是客户端(应用程序)连接 Zookeeper服务器的端口,Zookeeper会监听这个端口接受客户端的访问请求。 server.A=B:C:D 123server.1=dreyer-zk-01:2881:3881server.2=dreyer-zk-02:2882:3882server.3=dreyer-zk-03:2883:3883 A 是一个数字,表示这个是第几号服务器;B 是这个服务器的 IP 地址(或者是与 IP 地址做了映射的主机名);C 第一个端口用来集群成员的信息交换,表示这个服务器与集群中的 Leader 服务器交换信息的端口;D 是在 leader 挂掉时专门用来进行选举 leader 所用的端口。 注意:如果是伪集群的配置方式,不同的 Zookeeper 实例通信端口号不能一样,所以要给它们分配不 同的端口号。 7、在dataDir=/home/dreyer03/zookeeper/node-0X/data下创建myid文件编辑myid文件，并在对应的IP机器上输入对应的编号，比如说node-01上，myid文件的内容就是1，node-02上，myid的内容就是2，node-03上，myid的内容就是31$ vi /home/dreyer03/zookeeper/node-01/data/myid #输入1 1$ vi /home/dreyer04/zookeeper/node-02/data/myid #输入2 1$ vi /home/dreyer05/zookeeper/node-03/data/myid #输入3 8、在每台机器的防火墙中打开要用到的端口，218X、288X、388X切换到root用户后：1# vi /etc/sysconfig/iptables 在服务器1号中增加：1234## zookeeper-A INPUT -m state --state NEW -m tcp -p tcp --dport 2181 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 2881 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 3881 -j ACCEPT 在服务器2号中增加：1234## zookeeper-A INPUT -m state --state NEW -m tcp -p tcp --dport 2182 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 2882 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 3882 -j ACCEPT 在服务器3号中增加：1234## zookeeper-A INPUT -m state --state NEW -m tcp -p tcp --dport 2183 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 2883 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 3883 -j ACCEPT 以上，防火墙中增加端口后要记得重启防火墙重启防火墙：1# service iptables restart 查看防火墙状态：1# service iptables status 9、启动并测试zookeeper（用普通用户启动，不要用root用户）1$ /home/dreyer03/zookeeper/node-01/bin/zkServer.sh start 1$ /home/dreyer04/zookeeper/node-02/bin/zkServer.sh start 1$ /home/dreyer05/zookeeper/node-03/bin/zkServer.sh start 注意：zookeeper的启动日志在/bin目录下的zookeeper.out文件在启动第一个节点后，查看日志信息会看到如下异常：123456789101112java.net.ConnectException: Connection refused at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:579) at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:368) at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:402) at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:840) at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:762)2016-07-30 17:13:16,032 [myid:1] - INFO [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@849] - Notification time out: 51200 这是正常的，因为配置文件中配置了此节点是属于集群中的一个节点，zookeeper集群只有在过半的节点是正常的情况下，此节点才会正常，它是一直在检测集群其他两个节点的启动的情况。那在我们启动第二个节点之后，我们会看到原先启动的第一个节点不会在报错，因为这时候已经有过半的节点是正常的了。 10、查看zookeeper的状态1$ /home/dreyer03/zookeeper/node-01/bin/zkServer.sh status 会看到输出信息：123JMX enabled by defaultUsing config: /home/dreyer03/zookeeper/node-01/bin/../conf/zoo.cfgMode: follower follower表示此节点为从节点；leader表示此节点为主节点 11、停止zookeeper进程1$ /home/dreyer04/zookeeper/node-02/bin/zkServer.sh stop 在我们停止主节点之后，我们查看另外另个从节点的状态可以看到，原先的一个从节点会被重新选举为主节点。 12、设置zookeeper开机启动编辑node-01、node-02、node-03节点所在服务器的/etc/rc.local文件，分别加入：1su - dreyer03-c &apos;/home/dreyer03/zookeeper/node-01/bin/zkServer.sh start&apos; 1su - dreyer04-c &apos;/home/dreyer04/zookeeper/node-02/bin/zkServer.sh start&apos; 1su- dreyer05-c &apos;/home/dreyer05/zookeeper/node-03/bin/zkServer.sh start]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
        <tag>ZooKeeper集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx+Keepalived实现高可用Web负载均衡]]></title>
    <url>%2F2017%2F11%2F24%2FNginx-Keepalived%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8Web%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[一、需求场景：通过之前的一篇文章Nginx+Tomcat实现负载均衡我们已经能通过Nginx来实现Tomcat应用的负载均衡，但是单个的Nginx会存在单点隐患，如果Nginx挂掉，那么全部的Tomcat应用都将变得不可用，所以实现Nginx的高可用是必不可少的一步。 二、Keepalived 简要介绍Keepalived 是一种高性能的服务器高可用或双机热备解决方案,Keepalived可以用来防止服务器单点故障的发生,通过配合Nginx可以实现web前端服务的高可用。 Keepalived 以 VRRP 协议为实现基础,用VRRP协议来实现高可用性(HA)。VRRP(VirtualRouterRedundancyProtocol)协议是用于实现路由器冗余的协议，VRRP 协议将两台或多台路由器设备虚拟成一个 设备，对外提供虚拟路由器 IP(一个或多个)，而在路由器组内部,如果实际拥有这个对外 IP 的路由器如果工作正常的话就是 MASTER，或者是通过算法选举产生，MASTER 实现针对虚拟路由器 IP 的各种网络功能,，如 ARP 请求，ICMP,以及数据的转发等；其他设备不拥有该虚拟 IP,状态是 BACKUP，除了接收 MASTER 的 VRRP 状态通告信息外,不执行对外的网络功能。 当主机失效时，BACKUP 将接管原先 MASTER 的网络功能。VRRP 协议使用多播数据来传输 VRRP 数据，VRRP 数据使用特殊的虚拟源 MAC 地址发送数据而不是自身 网卡的 MAC 地址，VRRP 运行时只有 MASTER 路由器定时发送 VRRP 通告信息，表示 MASTER 工作正常以及虚 拟路由器 IP(组)，BACKUP 只接收 VRRP 数据，不发送数据，如果一定时间内没有接收到 MASTER 的通告信 息，各 BACKUP 将宣告自己成为 MASTER，发送通告信息，重新进行 MASTER 选举状态。 Keepalived的作用是检测服务器的状态，如果有一台服务器死机，或工作出现故障，Keepalived将检测到，并将有故障的服务器从系统中剔除，当服务器工作正常后Keepalived自动将服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的服务器。部署两台Nginx机器来做切换，在每一台Nginx机器上安装keepalived，一个为MASTER，一个为BACKUP，再利用keepalived产生一个虚拟IP，这个虚拟IP负责对外提供访问。 三、环境以及方案规划 虚拟IP 真实IP Nginx端口 默认主从 192.168.31.111 192.168.31.146 80 MASTER 192.168.31.111 192.168.31.154 80 BACKUP 四、Nginx的安装（在192.168.31.146和192.168.31.154中分别安装Nginx，安装用户：root） 1、安装编译Nginx所需要的依赖包1# yum install gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl openssl-devel 2、上传Nginx（nginx-1.6.2.tar）到/usr/local/src目录3、编译安装Nginx进入Nginx的上传目录：1# cd /usr/local/src/ 解压安装包：1# tar -zxvf nginx-1.6.2.tar.gz 进入解压后的文件夹：1# cd nginx-1.6.2 指定编译位置：1# ./configure --prefix=/usr/local/nginx 编译：1# make &amp;&amp; make install 4、配置Nginx通过修改/conf/nginx.conf文件来修改Nginx的配置信息1# vi /usr/local/nginx/conf/nginx.conf 5、系统防火墙中打开对应的端口80（默认80）1# vi /etc/sysconfig/iptables 添加：12## nginx-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT 添加后重启防火墙：1# service iptables restart 6、检测Nginx的配置文件是否正确建议每次修改/conf/nginx.conf文件之后，都手动执行该命令以检测此次修改的正确性1# /usr/local/nginx/sbin/nginx -t 出现以下信息，代表成功12nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful 7、启动Nginx1# /usr/local/nginx/sbin/nginx 通过http://ip可以访问nginx的默认页面 查看Nginx进程1# ps -ef|grep nginx 停止Nginx1# /usr/local/nginx/sbin/nginx -s stop 重启Nginx1# /usr/local/nginx/sbin/nginx -s reload 8、设置Nginx开机启动编辑/etc下的rc.local文件1# vi /etc/rc.local 添加：1/usr/local/nginx/sbin/nginx 五、Keepalived的安装1、上传或下载 keepalived(keepalived-1.2.18.tar.gz)到 /usr/local/src 目录2、解压安装进入安装包目录：1# cd /usr/local/src/ 解压：1# tar -zxvf keepalived-1.2.18.tar.gz 进入解压后的目录：1# cd keepalived-1.2.18 指定编译路径：1# ./configure --prefix=/usr/local/keepalived 编译安装：1# make &amp;&amp; make install 3、将keepalived安装成Linux系统服务因为没有使用keepalived的默认路径安装（默认是/usr/local），安装完成后，需要手动复制默认的配置文件到默认路径12# mkdir /etc/keepalived# cp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/ 复制keepalived服务脚本到默认的地址1234# cp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/# cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/# ln -s /usr/local/sbin/keepalived /usr/sbin/ # ln -s /usr/local/keepalived/sbin/keepalived /sbin/ 设置keepalived服务开机启动1# chkconfig keepalived on 4、修改keepalived配置文件1# vi /etc/keepalived/keepalived.conf 配置说明：123456789101112131415161718192021222324252627! Configuration File for keepalivedglobal_defs &#123;## keepalived 自带的邮件提醒需要开启 sendmail 服务。建议用独立的监控或第三方 SMTProuter_id edu-proxy-01 ## 标识本节点的字条串,通常为 hostname &#125;## keepalived 会定时执行脚本并对脚本执行的结果进行分析,动态调整 vrrp_instance 的优先级。如果 脚本执行结果为 0,并且 weight 配置的值大于 0,则优先级相应的增加。如果脚本执行结果非 0,并且 weight 配置的值小于 0,则优先级相应的减少。其他情况,维持原本配置的优先级,即配置文件中 priority 对应 的值。 vrrp_script chk_nginx &#123;script &quot;/etc/keepalived/nginx_check.sh&quot; interval 2 ## 检测时间间隔weight -20 ## 如果条件成立,权重-20&#125;## 检测 nginx 状态的脚本路径## 定义虚拟路由,VI_1 为虚拟路由的标示符,自己定义名称vrrp_instance VI_1 &#123;state MASTER ## 主节点为 MASTER,对应的备份节点为 BACKUPinterface eth1 ## 绑定虚拟 IP 的网络接口,与本机 IP 地址所在的网络接口相同,我的是 eth1 virtual_router_id 51 ## 虚拟路由的 ID 号,两个节点设置必须一样,可选 IP 最后一段使用, 相同的 VRID 为一个组,他将决定多播的 MAC 地址，eth1值的获取可以在机器上执行ifconfig命令得到mcast_src_ip 192.168.1.51 ## 本机 IP 地址priority 100 ## 节点优先级,值范围 0-254,MASTER 要比 BACKUP 高 nopreempt ## 优先级高的设置 nopreempt 解决异常恢复后再次抢占的问题 advert_int 1 ## 组播信息发送间隔,两个节点设置必须一样,默认 1s ## 设置验证信息,两个节点必须一致authentication &#123; auth_type PASSauth_pass 1111 ## 真实生产,按需求对应该过来 &#125;## 将 track_script 块加入 instance 配置块track_script &#123;chk_nginx ## 执行 Nginx 监控的服务&#125;## 虚拟 IP 池, 两个节点设置必须一样virtual_ipaddress &#123;192.168.1.50 ## 虚拟 ip,可以定义多个&#125; &#125; 附录：192.168.31.146（MASTER节点）的keepalived.conf12345678910111213141516171819202122232425262728! Configuration File for keepalivedglobal_defs &#123; router_id dreyer-zk-03&#125;vrrp_script chk_nginx &#123; script &quot;/etc/keepalived/nginx_check.sh&quot; interval 2 weight -20&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 146 mcast_src_ip 192.168.31.146 priority 100 nopreempt advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_nginx &#125; virtual_ipaddress &#123; 192.168.31.111 &#125;&#125; 192.168.31.154（BACKUP节点）的keepalived.conf123456789101112131415161718192021222324252627! Configuration File for keepalivedglobal_defs &#123; router_id dreyer-zk-01&#125;vrrp_script chk_nginx &#123; script &quot;/etc/keepalived/nginx_check.sh&quot; interval 2 weight -20&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 146 mcast_src_ip 192.168.31.154 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_nginx &#125; virtual_ipaddress &#123; 192.168.31.111 &#125;&#125; nginx_check.sh（Nginx状态检测脚本）123456789#!/bin/bashA=`ps -C nginx –no-header |wc -l`if [ $A -eq 0 ];then /usr/local/nginx/sbin/nginx sleep 2 if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then killall keepalived fifi 脚本大意为：检查是否有nginx进程，如果没有，那么就启动nginx，启动后睡眠2秒，再检查是否有nginx的进程，如果没有的话，那么就杀掉keepalived的全部进程（杀掉进程后keepalived才能进行重新选举） 6、启动keepalived1# service keepalived start 出现以下，代表启动成功1Starting keepalived: [ OK ] 7、进行高可用测试7.1、关于虚拟IP在192.168.31.146中执行ip add命令查看123456789101112[root@MiWiFi-R1CM sbin]# ip add1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:34:1c:e2 brd ff:ff:ff:ff:ff:ff inet 192.168.31.146/24 brd 192.168.31.255 scope global eth0 inet 192.168.31.111/32 scope global eth0 inet6 fe80::20c:29ff:fe34:1ce2/64 scope link valid_lft forever preferred_lft forever 在192.168.31.154中执行ip add命名查看123456789101112[root@MiWiFi-R1CM conf]# ip add1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:64:3d:2a brd ff:ff:ff:ff:ff:ff inet 192.168.31.154/24 brd 192.168.31.255 scope global eth0 inet 192.168.31.111/32 scope global eth0 inet6 fe80::20c:29ff:fe64:3d2a/64 scope link valid_lft forever preferred_lft forever 从输出信息中，我们可以知道146和154机器上都有192.168.31.111这个虚拟IP，这个IP是由keepalived虚拟出来的（keepalived.conf中有配置），关掉一台机器的keepalived再执行ip add命令，那么192.168.31.111这个虚拟IP将不存在。我们可以通过192.168.31.111这个虚拟IP来访问146和154这两台机器，其中哪台机器是master，虚拟IP就会映射到哪个地址。 7.2、我们修改两个nginx的首页信息，在首页中加入各自的IP地址1# vi /usr/local/nginx/html/index.html 7.3、高可用切换 我们可以看到由于146被我们设置为master，所以192.168.31.111这个虚拟IP会漂移到146这台机器上，那我们把146的keepalived关掉再看。停止keepalived：1# service keepalived stop 这个时候192.168.31.111这个虚拟IP已经漂移到154这个机器上了。 结论：通过keepalived来实现同一个虚拟IP映射到两台Nginx代理服务器，MASTER节点失效时，BACKUP接管原先MASTER的网络功能，这种方式来实现nginx的高可用性（如上文中的keepalived简要介绍）]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>Keepalived</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx+Tomcat实现负载均衡]]></title>
    <url>%2F2017%2F11%2F24%2FNginx-Tomcat%E5%AE%9E%E7%8E%B0%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[一、环境准备 Tomcat1：192.168.31.103 Tomcat2：192.168.31.117 Nginx：192.168.31.154 在103和117上分别部署相同的Tomcat程序，修改index.jsp页面，把内容改为各自的IP地址。 二、修改配置文件nginx.conf在http节点中增加：123456789101112## user-api upstream user-api &#123; server 192.168.31.103:8080 weight=3; server 192.168.31.117:8080 weight=3; &#125; server &#123; listen 80; server_name 192.168.31.154; location / &#123; proxy_pass http://user-api/; &#125; &#125; upstream为Nginx的负载均衡模块，里面定义了负载应用的列表。 server{}为虚拟主机配置 server_name为虚拟主机的IP或者域名，多个域名之间用空格隔开 listen为监听的端口 location / 表示监听80端口下面的所有请求 proxy_pass配置为：http + upstream名称 以上配置完成后，执行./nginx -t命令检测配置是否有错，再执行./ngin -s reload命令进行更新。 三、配置说明：upstream按照轮询（默认）方式进行负载，每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。虽然这种方式简便、成本低廉。 但缺点是：可靠性低和负载分配不均衡。适用于图片服务器集群和纯静态页面服务器集群。 除此之外，upstream还有其它的分配策略，分别如下： weight（权重）指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。如下所示，10.0.0.88的访问比率要比10.0.0.77的访问比率高一倍。 1234upstream linuxidc&#123; server 10.0.0.77 weight=5; server 10.0.0.88 weight=10; &#125; ip_hash（访问ip）每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 12345upstream favresin&#123; ip_hash; server 10.0.0.10:8080; server 10.0.0.11:8080; &#125; fair（第三方）按后端服务器的响应时间来分配请求，响应时间短的优先分配。与weight分配策略类似。 12345upstream favresin&#123; server 10.0.0.10:8080; server 10.0.0.11:8080; fair; &#125; url_hash（第三方）按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。注意：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法。 123456upstream resinserver&#123; server 10.0.0.10:7777; server 10.0.0.11:8888; hash $request_uri; hash_method crc32; &#125; upstream还可以为每个设备设置状态值，这些状态值的含义分别如下： down 表示单前的server暂时不参与负载. weight 默认为1.weight越大，负载的权重就越大。 max_fails ：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream 模块定义的错误. fail_timeout : max_fails次失败后，暂停的时间。 backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。1234567upstream bakend&#123; #定义负载均衡设备的Ip及设备状态 ip_hash; server 10.0.0.11:9090 down; server 10.0.0.11:8080 weight=2; server 10.0.0.11:6060; server 10.0.0.11:7070 backup; &#125; 四、测试访问http://192.168.31.154/dreyer-user-api/index.jsp 不断的刷新页面，我们会发现应用会在117和103之间随机切换，这表明通过Nginx来实现Tomcat应用的负载均衡目的达到。 五、附录实例nginx.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445#user www www;user root;worker_processes 4;error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;pid logs/nginx.pid;events &#123; use epoll; worker_connections 40960;&#125;# load modules compiled as Dynamic Shared Object (DSO)##dso &#123;# load ngx_http_fastcgi_module.so;# load ngx_http_rewrite_module.so;#&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; #tcp_nopush on; tcp_nodelay on; types_hash_max_size 2048; server_names_hash_max_size 512; server_names_hash_bucket_size 128; #keepalive_timeout 0; keepalive_timeout 280; gzip on; gzip_comp_level 3; gzip_types text/plain text/css text/javascript application/x-javascript application/javascript application/xml application/json; client_header_buffer_size 128k; large_client_header_buffers 4 256k; proxy_headers_hash_max_size 51200; proxy_headers_hash_bucket_size 6400; ## 配置包含的文件信息 include /opt/nginx/conf/tools/*.conf; include /opt/nginx/sites-enabled/*.conf;&#125; 在包含的文件信息中摘取几段12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152## dev-pay-api（单节点）server &#123; listen 80; server_name xxxx.cn; location / &#123; proxy_pass http://172.16.xx.xx:8089/; add_header &apos;Access-Control-Allow-Origin&apos; &apos;*&apos;; add_header &apos;Access-Control-Allow-Credentials&apos; &apos;true&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;POST, GET, OPTIONS,PUT,DELETE&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;*,token&apos;; proxy_set_header Host $http_host; proxy_set_header Cookie $http_cookie; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; client_max_body_size 100m; &#125;&#125;## qa-web-pay-api（双机负载均衡）upstream qa_web_pay_api &#123; server 172.16.xx.xx:8089 weight=3; server 172.16.xx.xx:8089 weight=3;&#125;server &#123; listen 80; server_name xxxx.com; location / &#123; proxy_pass http://qa_web_pay_api/; add_header &apos;Access-Control-Allow-Origin&apos; &apos;*&apos;; add_header &apos;Access-Control-Allow-Credentials&apos; &apos;true&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;POST, GET, OPTIONS,PUT,DELETE&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;*,token&apos;; proxy_set_header Host $http_host; proxy_set_header Cookie $http_cookie; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; client_max_body_size 100m; &#125;&#125;## test-web-b（前端代码，前后端分离的模式）server &#123; listen 80; server_name xxxx.com; rewrite ^(.*)$ https://$host$1 permanent; location / &#123; root /usr/local/www/test-web-b; try_files $uri $uri/ /index.html =404; &#125;&#125;]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx的安装]]></title>
    <url>%2F2017%2F11%2F24%2FNginx%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[一、安装环境 Linux版本：CentOS6.5 Nginx版本：1.6.2 服务器IP：192.168.31.154 安装用户：root 二、具体安装步骤1、安装编译Nginx所需要的依赖包1# yum install gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl openssl-devel 2、上传Nginx（nginx-1.6.2.tar）到/usr/local/src目录3、编译安装Nginx进入Nginx的上传目录：1# cd /usr/local/src/ 解压安装包：1# tar -zxvf nginx-1.6.2.tar.gz 进入解压后的文件夹：1# cd nginx-1.6.2 指定编译位置：1# ./configure --prefix=/usr/local/nginx 编译：1# make &amp;&amp; make install 4、配置Nginx通过修改/conf/nginx.conf文件来修改Nginx的配置信息1# vi /usr/local/nginx/conf/nginx.conf 5、系统防火墙中打开对应的端口80（默认80）1# vi /etc/sysconfig/iptables 添加：12## nginx-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT 添加后重启防火墙：1# service iptables restart 6、检测Nginx的配置文件是否正确。建议每次修改/conf/nginx.conf文件之后，都手动执行该命令以检测此次修改的正确性1# /usr/local/nginx/sbin/nginx -t 出现以下信息，代表成功12nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful 7、启动Nginx1# /usr/local/nginx/sbin/nginx 通过http://ip可以访问nginx的默认页面 查看Nginx进程1# ps -ef|grep nginx 停止Nginx1# /usr/local/nginx/sbin/nginx -s stop 重启Nginx1# /usr/local/nginx/sbin/nginx -s reload 8、设置Nginx开机启动编辑/etc下的rc.local文件1# vi /etc/rc.local 添加：1/usr/local/nginx/sbin/nginx]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RSA非对称加密]]></title>
    <url>%2F2017%2F11%2F21%2FRSA%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%2F</url>
    <content type="text"><![CDATA[一、对称加密与非对称加密 对称加密：加密和解密使用的是同一个密钥，加解密双方必须使用同一个密钥才能进行正常的沟通。 非对称加密：需要两个密钥来进行加密和解密，公开密钥（public key，简称公钥）和私有密钥（private key，简称私钥） ，公钥加密的信息只有私钥才能解开，私钥加密的信息只有公钥才能解开。 需要注意的一点，这个公钥和私钥必须是一对的，如果用公钥对数据进行加密，那么只有使用对应的私钥才能解密，所以只要私钥不泄露，那么我们的数据就是安全的。 常用的加密算法： 对称加密：DES、3DES、TDEA、Blowfish、RC2、RC4、RC5、IDEA、SKIPJACK、AES。 非对称加密：RSA、ECC（椭圆曲线加密算法）、Diffie-Hellman、El Gamal、DSA（数字签名用） Hash 算法：MD2、MD4、MD5、HAVAL、SHA-1、SHA256、SHA512、RipeMD、WHIRLPOOL、SHA3、HMAC 二、非对称加密工作过程甲乙双方使用非对称加密算法的方式进行数据传输 乙方生成一对密钥（公钥与私钥），并将公钥向甲方公开 甲方获取到公钥后，将需要传输的数据用公钥进行加密发送给乙方 乙方获取到甲方加密数据后，用私钥进行解密 在数据传输过程中，即使数据被攻击者截取并获取了公钥，攻击者也无法破解密文，因为只有乙方的私钥才能解密 三、非对称加密中，究竟是公钥加密还是私钥加密？ 对于加密：公钥加密，私钥加密。毕竟公钥可以公开，但是私钥只有你自已知道，你也同样希望只有你自已才能解密 对于签名：私钥加密，公钥解密。好比你的签名只有你自已签的才是真的，别人签的都是假的。 四、RSA非对称加密代码示例RSA 算法基于一个十分简单的数论事实：将两个大素数相乘十分容易，但是想要对其乘积进行因式分解却极其困难，因此可以将乘积公开作为加密密钥。 1、添加jar包 12345&lt;dependency&gt; &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt; &lt;artifactId&gt;bcpkix-jdk15on&lt;/artifactId&gt; &lt;version&gt;1.56&lt;/version&gt;&lt;/dependency&gt; 2、RSAUtil.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259package com.kimeng.weipan.utils;import org.bouncycastle.jce.provider.BouncyCastleProvider;import org.bouncycastle.util.encoders.Base64;import java.security.Key;import java.security.KeyFactory;import java.security.KeyPair;import java.security.KeyPairGenerator;import java.security.NoSuchAlgorithmException;import java.security.PrivateKey;import java.security.PublicKey;import java.security.Security;import java.security.Signature;import java.security.interfaces.RSAPrivateKey;import java.security.interfaces.RSAPublicKey;import java.security.spec.PKCS8EncodedKeySpec;import java.security.spec.X509EncodedKeySpec;import java.util.HashMap;import java.util.Map;import javax.crypto.Cipher;/** * @author: 会跳舞的机器人 * @date: 2017/9/18 15:00 * @description: RSA工具类 */public class RSAUtil &#123; /** * 定义加密方式 */ private final static String KEY_RSA = &quot;RSA&quot;; /** * 定义签名算法 */ private final static String KEY_RSA_SIGNATURE = &quot;MD5withRSA&quot;; /** * 定义公钥算法 */ private final static String KEY_RSA_PUBLICKEY = &quot;RSAPublicKey&quot;; /** * 定义私钥算法 */ private final static String KEY_RSA_PRIVATEKEY = &quot;RSAPrivateKey&quot;; static &#123; Security.addProvider(new BouncyCastleProvider()); &#125; /** * 初始化密钥 */ public static Map&lt;String, Object&gt; init() &#123; Map&lt;String, Object&gt; map = null; try &#123; KeyPairGenerator generator = KeyPairGenerator.getInstance(KEY_RSA); generator.initialize(2048); KeyPair keyPair = generator.generateKeyPair(); // 公钥 RSAPublicKey publicKey = (RSAPublicKey) keyPair.getPublic(); // 私钥 RSAPrivateKey privateKey = (RSAPrivateKey) keyPair.getPrivate(); // 将密钥封装为map map = new HashMap&lt;&gt;(); map.put(KEY_RSA_PUBLICKEY, publicKey); map.put(KEY_RSA_PRIVATEKEY, privateKey); &#125; catch (NoSuchAlgorithmException e) &#123; e.printStackTrace(); &#125; return map; &#125; /** * 公钥加密 * * @param data 待加密数据 * @param key 公钥 */ public static byte[] encryptByPublicKey(String data, String key) &#123; byte[] result = null; try &#123; byte[] bytes = decryptBase64(key); // 取得公钥 X509EncodedKeySpec keySpec = new X509EncodedKeySpec(bytes); KeyFactory factory = KeyFactory.getInstance(KEY_RSA); PublicKey publicKey = factory.generatePublic(keySpec); // 对数据加密 Cipher cipher = Cipher.getInstance(&quot;RSA/None/PKCS1Padding&quot;, &quot;BC&quot;); cipher.init(Cipher.ENCRYPT_MODE, publicKey); byte[] encode = cipher.doFinal(data.getBytes()); // 再进行Base64加密 result = Base64.encode(encode); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return result; &#125; /** * 私钥解密 * * @param data 加密数据 * @param key 私钥 */ public static String decryptByPrivateKey(byte[] data, String key) &#123; String result = null; try &#123; // 对私钥解密 byte[] bytes = decryptBase64(key); // 取得私钥 PKCS8EncodedKeySpec keySpec = new PKCS8EncodedKeySpec(bytes); KeyFactory factory = KeyFactory.getInstance(KEY_RSA); PrivateKey privateKey = factory.generatePrivate(keySpec); // 对数据解密 Cipher cipher = Cipher.getInstance(&quot;RSA/None/PKCS1Padding&quot;, &quot;BC&quot;); cipher.init(Cipher.DECRYPT_MODE, privateKey); // 先Base64解密 byte[] decoded = Base64.decode(data); result = new String(cipher.doFinal(decoded)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return result; &#125; /** * 获取公钥 */ public static String getPublicKey(Map&lt;String, Object&gt; map) &#123; String str = &quot;&quot;; try &#123; Key key = (Key) map.get(KEY_RSA_PUBLICKEY); str = encryptBase64(key.getEncoded()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return str; &#125; /** * 获取私钥 */ public static String getPrivateKey(Map&lt;String, Object&gt; map) &#123; String str = &quot;&quot;; try &#123; Key key = (Key) map.get(KEY_RSA_PRIVATEKEY); str = encryptBase64(key.getEncoded()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return str; &#125; /** * 用私钥对信息生成数字签名 * * @param data 加密数据 * @param privateKey 私钥 */ public static String sign(byte[] data, String privateKey) &#123; String str = &quot;&quot;; try &#123; // 解密由base64编码的私钥 byte[] bytes = decryptBase64(privateKey); // 构造PKCS8EncodedKeySpec对象 PKCS8EncodedKeySpec pkcs = new PKCS8EncodedKeySpec(bytes); // 指定的加密算法 KeyFactory factory = KeyFactory.getInstance(KEY_RSA); // 取私钥对象 PrivateKey key = factory.generatePrivate(pkcs); // 用私钥对信息生成数字签名 Signature signature = Signature.getInstance(KEY_RSA_SIGNATURE); signature.initSign(key); signature.update(data); str = encryptBase64(signature.sign()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return str; &#125; /** * 校验数字签名 * * @param data 加密数据 * @param publicKey 公钥 * @param sign 数字签名 * @return 校验成功返回true，失败返回false */ public static boolean verify(byte[] data, String publicKey, String sign) &#123; boolean flag = false; try &#123; // 解密由base64编码的公钥 byte[] bytes = decryptBase64(publicKey); // 构造X509EncodedKeySpec对象 X509EncodedKeySpec keySpec = new X509EncodedKeySpec(bytes); // 指定的加密算法 KeyFactory factory = KeyFactory.getInstance(KEY_RSA); // 取公钥对象 PublicKey key = factory.generatePublic(keySpec); // 用公钥验证数字签名 Signature signature = Signature.getInstance(KEY_RSA_SIGNATURE); signature.initVerify(key); signature.update(data); flag = signature.verify(decryptBase64(sign)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return flag; &#125; /** * BASE64 解密 * * @param key 需要解密的字符串 * @return 字节数组 */ public static byte[] decryptBase64(String key) throws Exception &#123; return Base64.decode(key); &#125; /** * BASE64 加密 * * @param key 需要加密的字节数组 * @return 字符串 */ public static String encryptBase64(byte[] key) throws Exception &#123; return new String(Base64.encode(key)); &#125; public static void main(String[] args) throws Exception &#123; String publicKey = &quot;&quot;; String privateKey = &quot;&quot;; Map&lt;String, Object&gt; keyMap = RSAUtil.init(); publicKey = RSAUtil.getPublicKey(keyMap); privateKey = RSAUtil.getPrivateKey(keyMap); System.out.println(&quot;公钥：\n\r&quot; + publicKey); System.out.println(&quot;私钥：\n\r&quot; + privateKey); System.out.println(&quot;公钥加密======私钥解密&quot;); String str = &quot;会跳舞的机器人&quot;; byte[] enStr = RSAUtil.encryptByPublicKey(str, publicKey); String decStr = RSAUtil.decryptByPrivateKey(enStr, privateKey); System.out.println(&quot;加密前：&quot; + str + &quot;\n\r解密后：&quot; + decStr); System.out.println(&quot;\n\r&quot;); System.out.println(&quot;私钥签名======公钥验证&quot;); String sign = RSAUtil.sign(str.getBytes(), privateKey); System.out.println(&quot;签名：\n\r&quot; + sign); boolean flag = RSAUtil.verify(str.getBytes(), publicKey, sign); System.out.println(&quot;验签结果：\n\r&quot; + flag); &#125;&#125; 输出1234567891011121314公钥：MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAkmIH1vvK8A+EDak84WOuOrN2IgzlarfDu6vp4i/vv/4Edezr1J5kWGf9WpGx2lfpZPS80bN03nxeA1utRoktJvqu++oXkoU3oDLlm/MciTV2lpSiDf8BiZfZ298FKQsG7CKI1Tj9ii9MlHWCsIDHfEJBsQTYONgDSjM6yecfRu0Xg0ZCNklNNeDki60oFa20hiUdLthSopuCWmxAGQL7uuOwlj07xzBXGEJkh8ixGF9v+CDMQLLU6ezk8ZWnPsOwS7OiJZkndyw13NKx8PMmaHX+cOYL5b6Vi7UQ+yizpGejV36XuX0Zpxb2AZee/IZCRGt/cOg0QKmEcN5KeKG6qQIDAQAB私钥：MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCSYgfW+8rwD4QNqTzhY646s3YiDOVqt8O7q+niL++//gR17OvUnmRYZ/1akbHaV+lk9LzRs3TefF4DW61GiS0m+q776heShTegMuWb8xyJNXaWlKIN/wGJl9nb3wUpCwbsIojVOP2KL0yUdYKwgMd8QkGxBNg42ANKMzrJ5x9G7ReDRkI2SU014OSLrSgVrbSGJR0u2FKim4JabEAZAvu647CWPTvHMFcYQmSHyLEYX2/4IMxAstTp7OTxlac+w7BLs6IlmSd3LDXc0rHw8yZodf5w5gvlvpWLtRD7KLOkZ6NXfpe5fRmnFvYBl578hkJEa39w6DRAqYRw3kp4obqpAgMBAAECggEAcZ0onJGddylzwu6h1AX8Co+TluYPgf7TKmxKAUZXfNp5N9YFTGcLVxqPJ6aBNgiZm8PgcZopkS1SAqU7Hc4Gf4R+IAQW+5/uBqa6U4ojkdMvEbyW8uoDlXmInADDDpICc3ByZ5vuHTyM4YU7RCcPrb/3IJ+z+pqeIw8UB/Uc73yUkao+KwL1alpKY8YTkHGMvIwgQDzhRSrfw7Nc5ZLZlkulWfgPN6B51peAqi8ZHGlyECtheEaS/Ws5O/Ju3y7V50WIs/rKT9DFpUZ8nueUPyn1+m+arMFPP3fKd2+9lUotQIIFgkyhLCdX6j7fxhDy2mQk7F68RhZPcGehg86SSQKBgQDM4uK6LCJ8BNURaaOospuOw0nmxeRmDxGMtGDIuTiOLccxwYJPQDQadZ76bfQpn7WmWyQrJWeKmDuvPZ+Mi+RHSL1W0mJnY9/0nLkgsu7fFFq5TDDBnEfNgqfyrozHZa6Eg89exjfH7WQTDbBI2/JXiNHA1iRwRsLIe+RJM/QsYwKBgQC25tBsLoDNUdyaXL9tf2kCG6vepUfdOdfSpby1pU4zpdONJXMenZglh3wIAKWoLVlhchRn/us1ClNasDqJX2O6WgXYh1OL88wFY8B04VnpV3o73vSJhxibZnm6KOzdonHiDAaP5+p/2jVRKewOoEmMFCITzDgSLqcKhA35wxosgwKBgAPUZdqHAqoAyR7HM7juhbvYaKQ4pLlHpNNVd82osKbvsh56+H2UvKSV+D/EGGFCy/ltELMBwvqzN8Jhy36sCrtOX8OksRQvqLsAxvEWhyKCVePKycqEqk7sF0mQ+66qduWhNRoXaGmDRXCZu+bQvannM8x/9DRpDjEDJ9Q6dWDzAoGBAK5GdCYAkX4SGG+FHGnLU1VM5JE7T6R97yWqAovaPQ99XHxLSMvNQiHQXOCLLU14GIh/WO5WuetKMW5iKQSoPbBdAuD04SijXq1sBP/ZkgCC99eAc+VvMoUwOaCmSjxNAtJuvvnz8z9rvg3eMi0lka7FqErQ9kRs64Fbnq1zt8bdAoGABbMMrAgeDvfkQId/1GsFP/xdgUicnXrLhD/cn6NlAqcU9lAubsRDewIQtfw377OsutK6gzbFdrLnaigRDgwf+y0XACdhldE8vTVOPtBaUIqSjTSjuRlp4ZBN1vx2or6QBG7Om061EuaL625YguAjeHpSxTiAbk5tejBnICXPFhY=公钥加密======私钥解密加密前：会跳舞的机器人解密后：会跳舞的机器人私钥签名======公钥验证签名：DqSegeOz0lIESAd7J7tlkYHsm/81vITAS9RWEmTtOtz5+NVtcD1RGgau2hMVDhNsIr7F0qP2SJSCu0SlAJxvN0pYYrRo5Cy1aLUCfsv/BEoLi32IcNN8cDClCxLa/uIDzyRd3+q8yMDuAhvX2eojiiu6cT3jwrajK4mtxncIbXvNFJ9/po36q8NYujvOhI0ujEFlDC5o5GwskXsJgk/gKf9raiGS9O7Pm8XejsSyDPITre86tuntWQppqWHIku/uN8Cf2n7AlD2HVBryQrma922iIxE3ykfNGoxF4wwof3AGXG4P12nC0rDB/V/7twFZvKmYBWAejni7bDJaV3+pUg==验签结果：true 五、RSA非对称加密的缺点加解密速度较慢，不太适用于高并发的业务场景，一般用它做少量的数据加密。]]></content>
      <categories>
        <category>Java开发</category>
      </categories>
      <tags>
        <tag>RSA</tag>
        <tag>加密</tag>
      </tags>
  </entry>
</search>
